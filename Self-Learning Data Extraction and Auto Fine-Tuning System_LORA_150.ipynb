{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6a0788",
   "metadata": {},
   "source": [
    "## Self-Learning Data Extraction and Auto Fine-Tuning System for Structured and Unstructured Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f61df0",
   "metadata": {},
   "source": [
    "**1. Unified Data Handling**\n",
    "* Develop techniques to process and normalize structured (e.g., CSV, SQL, PDFs), unstructured (e.g., text, images), and streaming data from diverse sources.\n",
    "* Enable seamless integration with offline files, legacy datasets, and live data pipelines (e.g., via data federation or APIs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be5f127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (45.0.7)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
      "Requirement already satisfied: xlrd in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: torch in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: Pillow in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: seaborn in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from seaborn) (2.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: shap in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.48.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (2.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (0.61.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-learn->shap) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: lime in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.2.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (3.10.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (1.16.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (1.7.1)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (0.25.2)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (3.5)\n",
      "Requirement already satisfied: pillow>=10.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (11.1.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (2025.8.28)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-learn>=0.18->lime) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from tqdm->lime) (0.4.6)\n",
      "Requirement already satisfied: peft in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (4.56.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=1.13.0->peft) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from transformers->peft) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from transformers->peft) (0.22.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.47.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from bitsandbytes) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from bitsandbytes) (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=2.0.0->accelerate) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "# all installs\n",
    "\n",
    "!pip install pdfplumber\n",
    "!pip install xlrd \n",
    "!pip install transformers>=4.21.0\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install pytesseract Pillow \n",
    "!pip install matplotlib seaborn\n",
    "!pip install shap\n",
    "!pip install lime\n",
    "\n",
    "\n",
    "\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5dfae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necesary libraries\n",
    "\n",
    "# Fix protobufs:\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\" \n",
    "os.environ[\"TRANSFORMERS_NO_SLOW_TOKENIZER\"] = \"1\"\n",
    "\n",
    "# Tesseract\n",
    "try:\n",
    "    import pytesseract\n",
    "    pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "except ImportError:\n",
    "    print(\"pytesseract not installed yet\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import sqlite3 \n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import torch, numpy, spacy\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from difflib import SequenceMatcher \n",
    "\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e38b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAFE_RUN = True | INTERACTIVE = False\n"
     ]
    }
   ],
   "source": [
    "# Safety switches for Run All\n",
    "\n",
    "SAFE_RUN = True       # Skip heavy/long steps (fine-tune) on first full run\n",
    "INTERACTIVE = False   # Avoid input() prompts during Run All\n",
    "print(\"SAFE_RUN =\", SAFE_RUN, \"| INTERACTIVE =\", INTERACTIVE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75315e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Simple type detection function \n",
    "\n",
    "# This function determines the file type based on the file extension\n",
    "\n",
    "def detect_file_type(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        return \"csv\"\n",
    "    elif ext == \".pdf\":\n",
    "        return \"pdf\"\n",
    "    elif ext == \".txt\":\n",
    "        return \"text\"\n",
    "    elif ext in [\".xlsx\", \".xls\"]:  \n",
    "        return \"excel\"\n",
    "    elif ext == \".db\": \n",
    "        return \"sqlite\"\n",
    "    elif ext in [\".png\", \".jpg\", \".jpeg\", \".tiff\", \".bmp\"]: \n",
    "        return \"image\"\n",
    "    else:\n",
    "        return \"unsupported\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1895bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File loading functions for different data types \n",
    "# These functions handle the actual data extraction from supported file formats\n",
    "# Each loader returns data in a standardized format for downstream processing\n",
    "\n",
    "def load_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.to_dict(orient=\"records\")  # returns list of row dictionaries\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    data = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                data.append(text)\n",
    "    return {\"pages\": data}\n",
    "\n",
    "def load_excel(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    return df.to_dict(orient=\"records\")\n",
    "\n",
    "def load_sqlite(file_path, table_name):\n",
    "    conn = sqlite3.connect(file_path)\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    return df.to_dict(orient=\"records\")\n",
    "\n",
    "def load_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return {\"text\": f.read()}\n",
    "    \n",
    "def load_image_with_ocr(file_path):\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        import pytesseract\n",
    "        \n",
    "        # Set Tesseract path for Windows\n",
    "        pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "        \n",
    "        image = Image.open(file_path)\n",
    "        # Enhanced OCR \n",
    "        text = pytesseract.image_to_string(\n",
    "            image, \n",
    "            config='--oem 3 --psm 6 -l eng+deu'  # Multi-language support\n",
    "        )\n",
    "        return {\"extracted_text\": text.strip(), \"source\": \"ocr\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"OCR failed: {str(e)}\", \"extracted_text\": \"\"}\n",
    "\n",
    "\n",
    "def load_file(file_path):\n",
    "    file_type = detect_file_type(file_path)\n",
    "    if file_type == \"csv\":\n",
    "        return load_csv(file_path)\n",
    "    elif file_type == \"pdf\":\n",
    "        return load_pdf(file_path)\n",
    "    elif file_type == \"text\":\n",
    "        return load_text(file_path)\n",
    "    elif file_type == \"excel\":  \n",
    "        return load_excel(file_path)  \n",
    "    elif file_type == \"sqlite\": \n",
    "        return load_sqlite(file_path) \n",
    "    elif file_type == \"image\":\n",
    "        return load_image_with_ocr(file_path)\n",
    "    else: \n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad26fb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '# Predicting Train Delays Using Deutsche Bahn (DB) Data\\n\\n## Overview\\n\\nThis project tackles the problem of predicting train arrival delays using \\nhistorical data from Deutsche Bahn (DB), Germany\\'s national railway. We approach the problem from two perspectives:\\n\\n- **Classification**: Will a train be delayed by more than 6 minutes?\\n- **Regression**: How many minutes will a train be delayed?\\n\\nThe goal is to support better scheduling decisions, optimize passenger information systems, and demonstrate the effectiveness\\nof supervised machine learning for real-world transportation challenges.\\n\\n\\n## Project Structure\\n\\nPredicting-Train-Delays/\\n├── README.md # Main project README (this file)\\n├── requirements.txt # List of required Python packages\\n├── DBtrainrides.csv # Dataset (to be downloaded)\\n\\n├── Classification/ # Classification task\\n│ ├── ML_Final_Classification.ipynb\\n│ └── README.md\\n\\n└── Regression/ # Regression task\\n├── ML_Final_Regression.ipynb\\n└── README.md\\n\\n## Dataset\\n\\n- Over **2 million rows** of DB train ride data\\n\\n- Columns include:\\n  - Scheduled vs. actual arrival/departure times\\n  - Delay flags\\n  - Station metadata\\n  - Geolocation, time, and date info\\n\\n- **Target variables:**\\n  - \"is_late\" (Classification): whether the train is >6 minutes late\\n  - \"delay_minutes\" (Regression): number of minutes delayed\\n\\nThe dataset is not stored in this repo due to GitHub size limitations.\\n\\nDownload it here: https://www.kaggle.com/datasets/nokkyu/deutsche-bahn-db-delays\\n\\n\\n## ML Tasks & Models\\n\\n### Classification Task\\n\\n> [See detailed Classification README →](Classification/README.md)\\n\\n**Goal:** Predict whether a train will be late (delay > 6 minutes)\\n\\n- Preprocessing:\\n  - Feature engineering (hour, weekday, month, etc.)\\n  - SMOTE for class imbalance\\n  - Encoding: frequency + one-hot\\n  - Scaling and missing value imputation\\n\\n- Models:\\n  - Logistic Regression\\n  - Random Forest\\n  - MLP (Neural Network)\\n  - XGBoost\\n\\n- Best Performance:\\n  - **XGBoost** showed highest ROC-AUC\\n  - **MLP** had best recall (which is important for detecting delays)\\n\\n### Regression Task\\n\\n> [See detailed Regression README →](Regression/README.md)\\n\\n**Goal:** Predict the number of minutes a train will be delayed\\n\\n- Steps:\\n  - Full EDA with heatmaps, charts\\n  - Data cleaning, datetime handling\\n  - New feature creation\\n  - Encoding + scaling\\n\\n- Models:\\n  - Linear Regression\\n  - Random Forest Regressor\\n  - XGBoost Regressor\\n  - MLP Regressor\\n\\n- Best Performance:\\n  - **MLP Regressor** achieved **R² = 0.93**\\n  - Demonstrated strong predictive power for delay estimation\\n\\n## Setup & Dependencies\\n\\n### Requirements\\n\\n```bash\\npip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn xgboost tensorflow\\nCompatible with Python 3.8+\\n\\nYou can also find all dependencies in requirements.txt.\\n\\n**How to Run**\\n\\nClone the repo and download the dataset (DBtrainrides.csv)\\n\\nUpdate the file path in both notebooks\\n\\nOpen notebooks in Jupyter or VS Code\\n\\nRun cells in order to reproduce analysis and results\\n\\n\\n**Notes**\\n\\nThe dataset is highly imbalanced; classification models prioritized recall\\n\\nRegression models included leakage prevention, model saving, and runtime tracking\\n\\nAll notebooks are fully annotated for readability and reproducibility\\n\\n**Authors**\\n\\nAna Bernal — ana.bernal@ue-germany.de\\n\\nAslican Alacal — aslican.alacal@ue-germany.de'}\n"
     ]
    }
   ],
   "source": [
    "# Testing text file\n",
    "\n",
    "file_path = \"example.txt\"\n",
    "data = load_file(r\"C:\\Users\\aslia\\OneDrive\\Desktop\\github\\Predicting-Train-Delays\\README.txt\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d267886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_live_data(api_url, headers=None, timeout=10):\n",
    "    try:\n",
    "        response = requests.get(api_url, headers=headers, timeout=timeout)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Handle different content types\n",
    "            content_type = response.headers.get('content-type', '')\n",
    "            \n",
    "            if 'json' in content_type:\n",
    "                data = response.json()\n",
    "            else:\n",
    "                data = response.text\n",
    "                \n",
    "            return [{\n",
    "                \"file_name\": f\"api_data_{int(time.time())}\",  # Unique timestamp\n",
    "                \"file_type\": \"json\" if 'json' in content_type else \"text\",\n",
    "                \"source\": \"live_api\",\n",
    "                \"content\": [{\n",
    "                    \"section_id\": 0,\n",
    "                    \"text\": str(data),\n",
    "                    \"metadata\": {\n",
    "                        \"source\": api_url,\n",
    "                        \"timestamp\": time.time(),\n",
    "                        \"status_code\": response.status_code\n",
    "                    }\n",
    "                }]\n",
    "            }]\n",
    "        else:\n",
    "            raise Exception(f\"API returned status code: {response.status_code}\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to fetch live data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0687f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Extracted Content\n",
    "\n",
    "# These functions convert extracted data into a standardized format for consistent processing\n",
    "# All functions return a list of dictionaries with 'section_id', 'text', and 'metadata' fields\n",
    "\"\"\"Detects file type cals the appropriate normalization function and\n",
    "Returns unified document structure\"\"\"\n",
    "\n",
    "def normalize_csv(file_path):  # Normalizes CSV data by converting each row into a standardized content block.\n",
    "    df = pd.read_csv(file_path)\n",
    "    content = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content.append({ \"section_id\":idx, \n",
    "                        \"text\":str(row.to_dict()), \"metadata\": {\"row\": idx} # Convert row data to string representation\n",
    "                        }\n",
    "                       )\n",
    "    return content\n",
    "\n",
    "def normalize_pdf(file_path): # All functions return a list of dictionaries with 'section_id', 'text', and 'metadata' fields\n",
    "    content = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:   # Only include pages with extractable text\n",
    "                content.append({\n",
    "                    \"section_id\": i,\n",
    "                    \"text\": text,\n",
    "                    \"metadata\": {\"page\": i + 1}\n",
    "                })\n",
    "    return content\n",
    "\n",
    "def normalize_text(file_path): # Normalizes text files by treating each non-empty line as a separate content section.\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    content = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():  # skip blank lines\n",
    "            content.append({\n",
    "                \"section_id\": i,\n",
    "                \"text\": line.strip(),\n",
    "                \"metadata\": {\"line\": i + 1}\n",
    "            })\n",
    "    return content\n",
    "\n",
    "def normalize_excel(file_path): # Excel loader legacy dataset\n",
    "    df = pd.read_excel(file_path)\n",
    "    content = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content.append({\n",
    "            \"section_id\": idx,\n",
    "            \"text\": str(row.to_dict()),\n",
    "            \"metadata\": {\"row\": idx}\n",
    "        })\n",
    "    return content\n",
    "\n",
    "\n",
    "# SQLite support  dataset\n",
    "\n",
    "def normalize_sqlite(db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    content = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content.append({\n",
    "            \"section_id\": idx,\n",
    "            \"text\": str(row.to_dict()),\n",
    "            \"metadata\": {\"row\": idx}\n",
    "        })\n",
    "    return content\n",
    "\n",
    "\n",
    "# image normalization function\n",
    "\n",
    "def normalize_image(file_path):\n",
    "    ocr_result = load_image_with_ocr(file_path)\n",
    "    \n",
    "    if ocr_result.get(\"error\"):\n",
    "        return [{\"section_id\": 0, \"text\": \"\", \"metadata\": {\"error\": ocr_result[\"error\"]}}]\n",
    "    \n",
    "    text = ocr_result[\"extracted_text\"]\n",
    "    if not text:\n",
    "        return [{\"section_id\": 0, \"text\": \"\", \"metadata\": {\"error\": \"No text found\"}}]\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    content = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():\n",
    "            content.append({\n",
    "                \"section_id\": i,\n",
    "                \"text\": line.strip(),\n",
    "                \"metadata\": {\"line\": i + 1, \"source\": \"ocr\"}\n",
    "            })\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b953d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Master function that loads and normalizes any supported file type.\n",
    "\n",
    "# Creates a unified data structure regardles of input file format.\n",
    "    \n",
    "def load_and_normalize(file_path, table_name=None):\n",
    "    \n",
    "    try:  # Check if file exists\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError (f\"File not found: {file_path}\")\n",
    "    \n",
    "        file_type = detect_file_type(file_path)\n",
    "        file_name = os.path.basename(file_path) # Extract filename without path\n",
    "        print(f\"Processing {file_type} file: {file_name}\")\n",
    "        \n",
    "# appropriatee normalization function based on file type\n",
    "\n",
    "        if file_type == 'csv':\n",
    "            content = normalize_csv(file_path)\n",
    "        elif file_type == 'pdf':\n",
    "            content = normalize_pdf(file_path)\n",
    "        elif file_type == 'text':\n",
    "            content = normalize_text(file_path)\n",
    "        elif file_type == 'excel':\n",
    "            content = normalize_excel(file_path)\n",
    "        elif file_type == 'sqlite':\n",
    "            if not table_name:\n",
    "                raise ValueError(\"sqlite files required a table_name\")\n",
    "            content = normalize_sqlite(file_path, table_name)\n",
    "        elif file_type == 'image': \n",
    "            content = normalize_image(file_path)\n",
    "    \n",
    "        elif file_type == 'unsupported':\n",
    "            raise ValueError(f\"Unsupported file type for: {file_name}\")\n",
    "    \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type:{file_type}\")\n",
    "    \n",
    "# Return standardized document structure                        \n",
    "        return { \n",
    "                \"file_name\": file_name,\n",
    "                \"file_type\": file_type,\n",
    "                \"source\": \"offline\",\n",
    "                \"content\": content,\n",
    "                \"processed_at\": time.time() # Adds timestamp\n",
    "            }\n",
    "    except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a01d097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pdf file: Rechnung.pdf\n",
      "{'file_name': 'Rechnung.pdf', 'file_type': 'pdf', 'source': 'offline', 'content': [{'section_id': 0, 'text': 'DEUBA GmbH & Co. KG | Zum Wiesenhof 84 | 66663 Merzig Rechnung\\nNordson\\nAhmed Ebada Datum 06.02.2020\\nKapellenstr 12 Rechnungsnummer 1407606058\\nKundenreferenz 1226475257\\n85622 Feldkirchen\\nKundennummer 21144538789\\nLieferdatum 05.02.2020\\nSeite 1 von 1\\nPosBeschreibung Menge Preis Betrag\\nEUR EUR\\n1 191474 1 13,95 13,95\\nMülleimer mit Schiebedeckel Kunststoff Silber 50 Liter\\nVersandart: DPD\\nGesamtnettowert (EUR) 11,72\\nMehrwertsteuer 19,00% 2,23\\nGesamtbruttowert (EUR) 13,95\\nRechnungsdatum = Lieferdatum\\nDiese Rechnung ist Bestandteil Ihres Auftrags vom 05.02.2020\\nNur die in diesem Dokument als solche erkennbaren Produkte sind FSC zertifiziert.\\nFSC Zertifizierungsnummer TSUD-COC-000791\\nWir haben Ihre Zahlung am 05.02.2020 erhalten.\\nZahlungsbedingung: ManoMano\\nEs gelten unsere Ihnen bekannten Allgemeinen Geschäftsbedingungen.\\nwww.deubaservice.de\\nRechnungsnummer:\\n1407606058\\nWir sind 24 Stunden am Tag für Sie da!\\nBearbeiten Sie Reklamationen ganz bequem Postleitzahl:\\nvon zu Hause aus - ohne Öffnungszeiten 85622\\nDEUBA GmbH & Co. KG HRA 9472 Kontakt: Bank:\\nZum Wiesenhof 84 e-Mail: kontakt@deuba.info Bank1Saar\\n66663 Merzig , Germany USt-ID: DE815720919 Fon: +49 6861 9010000 IBAN: DE54591900000117582000\\nGeschäftsführer: Steuernummer: 040/152/07650 Fax: +49 6861 9010099 BIC: SABADE5SXXX\\nHenning Valentin, Thorsten Schneider, Sitz Gesellschaft: Merzig Internet: www.deuba.info Konto: 117582000\\nMarius Friedrich Registergericht: Saarbrücken Shop: www.deuba24online.de BLZ: 591 900 00', 'metadata': {'page': 1}}], 'processed_at': 1757933008.4308496}\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "data = load_and_normalize(r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\Rechnung.pdf\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9c8be",
   "metadata": {},
   "source": [
    "**2. Intelligent Information Extraction**\n",
    "* Design or adapt machine learning and NLP models to automatically extract key entities, values, and patterns from heterogeneous data.\n",
    "* Support document parsing, table recognition, entity linking, and contextual extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d86690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding missing imports\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20a55320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QABasedExtractor\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "class QABasedExtractor:\n",
    "    def __init__(self, model_name=\"deepset/xlm-roberta-large-squad2\", local_dir=\"./lora_fine_tuned_model\"):\n",
    "        \n",
    "        # Always define to avoid AttributeError\n",
    "        self.ner_pipeline = None\n",
    "        \n",
    "        # Check for LoRA fine-tuned model\n",
    "        if (os.path.isdir(local_dir) and \n",
    "            os.path.exists(os.path.join(local_dir, \"adapter_config.json\"))):\n",
    "            \n",
    "            print(f\"Loading LoRA fine-tuned model from {local_dir}\")\n",
    "            \n",
    "            # Load base model and tokenizer separately\n",
    "            base_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)  # Use base model tokenizer\n",
    "            \n",
    "            # Load LoRA adapter\n",
    "            model = PeftModel.from_pretrained(base_model, local_dir)\n",
    "            \n",
    "            # Create pipeline with both model and tokenizer\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,  # Explicitly provide tokenizer\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            \n",
    "        elif (os.path.isdir(local_dir) and \n",
    "              os.path.exists(os.path.join(local_dir, \"config.json\"))):\n",
    "            \n",
    "            print(f\"Loading full fine-tuned model from {local_dir}\")\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=local_dir,\n",
    "                tokenizer=local_dir,\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Loading base model: {model_name}\")\n",
    "    \n",
    "            # Load model and tokenizer explicitly\n",
    "            base_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    \n",
    "            # Create pipeline with both model and tokenizer\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=base_model,\n",
    "                tokenizer=tokenizer,  # Explicitly provide tokenizer\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "        \n",
    "        self.model_name = local_dir if os.path.isdir(local_dir) else model_name\n",
    "\n",
    "        # Updated question templates with better targeting\n",
    "        \n",
    "        self.extraction_templates = {\n",
    "            \"invoice\": [\n",
    "                \"What number appears after 'Invoice Number' or 'Rechnungsnummer'?\",\n",
    "                \"What amount appears after 'Total' or 'Gesamtbetrag' or 'Endbetrag'?\",\n",
    "                \"What company name appears at the top of the document?\",\n",
    "                \"What date appears after 'Invoice Date' or 'Rechnungsdatum'?\",\n",
    "                \"What is the largest monetary amount mentioned?\",\n",
    "                \"What customer name appears on the invoice?\",\n",
    "                \"What tax amount is mentioned?\",\n",
    "                \"What is the net amount before tax?\"\n",
    "            ],\n",
    "            \"german_invoice\": [\n",
    "                \"Welche Nummer steht nach 'Rechnungsnummer'?\",\n",
    "                \"Welcher Betrag steht nach 'Gesamtbetrag' oder 'Endbetrag'?\",\n",
    "                \"Wie heißt die Firma auf der Rechnung?\",\n",
    "                \"Welches Datum steht nach 'Rechnungsdatum'?\",\n",
    "                \"What number appears after 'Rechnungsnummer'?\",\n",
    "                \"What amount appears after 'Gesamtbetrag' or 'Total'?\",\n",
    "                \"What company name is mentioned?\",\n",
    "                \"What date appears after 'Rechnungsdatum'?\",\n",
    "                \"What is the highest amount in Euro mentioned?\",\n",
    "                \"What is the MwSt or USt amount?\",\n",
    "                \"What services or products are listed?\",\n",
    "                \"What is the invoice number?\",\n",
    "                \"What is the Rechnungsnummer?\",\n",
    "                \"What is the total amount?\",\n",
    "                \"What is the Gesamtbetrag?\", \n",
    "                \"What is the Endbetrag?\",\n",
    "                \"What is the vendor name?\",\n",
    "                \"What is the company name?\",\n",
    "                \"What is the Firmenname?\",\n",
    "                \"What is the invoice date?\",\n",
    "                \"What is the Rechnungsdatum?\",\n",
    "                \"What is the billing date?\",\n",
    "                \"Who is the customer?\",\n",
    "                \"What is the Kunde?\",\n",
    "                \"What services were provided?\",\n",
    "                \"What is the Leistung?\",\n",
    "                \"What is the tax amount?\",\n",
    "                \"What is the Mehrwertsteuer?\",\n",
    "                \"What is the USt?\",\n",
    "                \"What is the net amount?\",\n",
    "                \"What is the Nettobetrag?\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"What are the most important numbers in this document?\",\n",
    "                \"What companies or organizations are mentioned?\",\n",
    "                \"What dates are mentioned?\",\n",
    "                \"What monetary amounts are mentioned?\",\n",
    "                \"What are the key facts in this document?\",\n",
    "                \"What is the main topic or subject of this document?\",\n",
    "                \"What names of people are mentioned?\",\n",
    "                \"What locations or addresses are mentioned?\",\n",
    "                \"What email addresses or contact information is provided?\",\n",
    "                \"What phone numbers are listed?\",\n",
    "                \"What percentages or statistics are mentioned?\",\n",
    "                \"What products or services are described?\",\n",
    "                \"What deadlines or time periods are mentioned?\",\n",
    "                \"What requirements or specifications are listed?\",\n",
    "                \"What actions or tasks are described?\",\n",
    "                \"What problems or issues are identified?\",\n",
    "                \"What solutions or recommendations are provided?\",\n",
    "                \"What project names or codes are mentioned?\",\n",
    "                \"What versions or releases are referenced?\",\n",
    "                \"What technologies or tools are discussed?\",\n",
    "                \"What departments or teams are mentioned?\",\n",
    "                \"What metrics or measurements are provided?\",\n",
    "                \"What goals or objectives are stated?\",\n",
    "                \"What risks or concerns are identified?\",\n",
    "                \"What benefits or advantages are highlighted?\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Enhanced preprocessing for invoices\"\"\"\n",
    "        # basic cleaning\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        # invoice-specific preprocessing\n",
    "        # context markers to help AI understand\n",
    "        text = re.sub(r'(Rechnungsnummer|Invoice Number)[\\s:]*([A-Z0-9\\-]+)', \n",
    "                      r'The invoice number is \\2. Rechnungsnummer: \\2', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        text = re.sub(r'(Gesamtbetrag|Total|Endbetrag)[\\s:]*([€$]?\\s*[\\d,\\.]+)', \n",
    "                      r'The total amount is \\2. Gesamtbetrag: \\2', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # limit length\n",
    "        if len(text) > 2000:\n",
    "            text = text[:2000] + \"...\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_entities_with_ner(self, text: str) -> Dict:\n",
    "        \"\"\"Enhanced NER extraction\"\"\"\n",
    "        ner = getattr(self, \"ner_pipeline\", None)\n",
    "        if not ner:\n",
    "            return {\n",
    "                'organizations': [],\n",
    "                'money': [],\n",
    "                'dates': [],\n",
    "                'persons': []\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            entities = ner(text)\n",
    "            processed = {'organizations': [], 'money': [], 'dates': [], 'persons': []}\n",
    "            for entity in entities:\n",
    "                entity_type = entity.get('entity_group', '')\n",
    "                word = entity.get('word', '').strip()\n",
    "                if entity_type == 'ORG' and len(word) > 2:\n",
    "                    processed['organizations'].append(word)\n",
    "                elif entity_type == 'PER' and len(word) > 2:\n",
    "                    processed['persons'].append(word)\n",
    "            return processed\n",
    "        except Exception:\n",
    "            return {}\n",
    "        \n",
    "    def extract_with_questions(self, text: str, questions: List[str], \n",
    "                             confidence_threshold: float = 0.05) -> Dict:  # Lower threshold\n",
    "        text = self.preprocess_text(text)\n",
    "        results = {}\n",
    "        \n",
    "        for question in questions:\n",
    "            try:\n",
    "                qa_result = self.qa_pipeline(\n",
    "                    question=question,\n",
    "                    context=text,\n",
    "                    max_answer_len=150  # Longer answers\n",
    "                )\n",
    "                \n",
    "                results[question] = {\n",
    "                    'answer': qa_result['answer'] if qa_result['score'] >= confidence_threshold else None,\n",
    "                    'confidence': qa_result['score'],\n",
    "                    'start_pos': qa_result.get('start', 0),\n",
    "                    'end_pos': qa_result.get('end', 0),\n",
    "                    'extracted': qa_result['score'] >= confidence_threshold\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[question] = {\n",
    "                    'answer': None,\n",
    "                    'confidence': 0.0,\n",
    "                    'error': str(e),\n",
    "                    'extracted': False\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def auto_detect_document_type(self, text: str) -> str:\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Better German detection\n",
    "        german_keywords = ['rechnung', 'rechnungsnummer', 'mehrwertsteuer', 'ust', 'gesamtbetrag', 'firmenname']\n",
    "        if any(keyword in text_lower for keyword in german_keywords):\n",
    "            return 'german_invoice'\n",
    "        \n",
    "        # English invoice detection\n",
    "        if any(word in text_lower for word in ['invoice', 'bill to', 'invoice number']):\n",
    "            return 'invoice'\n",
    "        \n",
    "        return 'general'\n",
    "    \n",
    "    def extract_information(self, text: str, document_type: Optional[str] = None, \n",
    "                          custom_questions: Optional[List[str]] = None) -> Dict:\n",
    "        \n",
    "        # Autodetect if not provided\n",
    "        if document_type is None:\n",
    "            document_type = self.auto_detect_document_type(text)\n",
    "        \n",
    "        # Choose questions\n",
    "        if custom_questions:\n",
    "            questions = custom_questions\n",
    "        else:\n",
    "            questions = self.extraction_templates.get(document_type, self.extraction_templates['general'])\n",
    "        \n",
    "        # Extract with QA\n",
    "        qa_results = self.extract_with_questions(text, questions)\n",
    "        \n",
    "        # Extract entities with NER if available\n",
    "        ner_results = self.extract_entities_with_ner(text)\n",
    "        \n",
    "        # Combine results\n",
    "        successful = sum(1 for r in qa_results.values() if r.get('extracted'))\n",
    "        \n",
    "        return {\n",
    "            'document_type': document_type,\n",
    "            'extraction_timestamp': datetime.now().isoformat(),\n",
    "            'model_used': self.model_name,\n",
    "            'total_questions': len(questions),\n",
    "            'successful_extractions': successful,\n",
    "            'success_rate': successful / len(questions) if questions else 0,\n",
    "            'extractions': qa_results,\n",
    "            'entities': ner_results\n",
    "        }\n",
    "        \n",
    "    # Only uses high-confidence predictions for further training\n",
    "    # This prevents error propagation\n",
    "    def get_high_confidence_extractions(self, results: Dict, min_confidence: float = 0.3) -> Dict:\n",
    "        \"\"\"Lower confidence threshold for invoice extraction\"\"\"\n",
    "        high_conf = {}\n",
    "        \n",
    "        for question, result in results['extractions'].items():\n",
    "            if result.get('confidence', 0) >= min_confidence and result.get('answer'):\n",
    "                high_conf[question] = result\n",
    "        \n",
    "        return {\n",
    "            'document_type': results['document_type'],\n",
    "            'high_confidence_extractions': high_conf,\n",
    "            'total_high_confidence': len(high_conf)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7785e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data processing pipeline with QA extraction\n",
    "\n",
    "def process_document_with_qa(document_data: Dict, custom_questions: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a normalized document using QA-based extraction.\n",
    "    \"\"\"\n",
    "    extractor = QABasedExtractor()\n",
    "    \n",
    "    # Combine all text content from the document\n",
    "    all_text = \"\"\n",
    "    for content_item in document_data['content']:\n",
    "        all_text += content_item['text'] + \" \"\n",
    "    \n",
    "    \"\"\"Takes your normalized document (which has text split by pages/sections)\n",
    "Combines everything into one big text string\n",
    "This gives the AI the full context to answer questions\"\"\"\n",
    "\n",
    "    # Extract information using QA\n",
    "    extraction_results = extractor.extract_information(\n",
    "        text=all_text,\n",
    "        custom_questions=custom_questions\n",
    "    )\n",
    "    \"\"\"Passes the combined text to your AI extractor\n",
    "Uses custom questions if provided (like your German questions)\n",
    "Gets back structured answers with confidence scores\"\"\"\n",
    "\n",
    "    # extraction results to document data\n",
    "    \n",
    "    document_data['qa_extraction'] = extraction_results\n",
    "    document_data['high_confidence_extractions'] = extractor.get_high_confidence_extractions(\n",
    "        extraction_results\n",
    "    )\n",
    "    \"\"\"Adds AI results to  original document structure\n",
    "Creates two versions: all results + high-confidence only\n",
    "Preserves original data while adding AI insights\"\"\"\n",
    "    \n",
    "    return document_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee48d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration with existing pipeline\n",
    "def enhanced_load_and_normalize_with_qa(file_path: str, table_name: Optional[str] = None, \n",
    "                                       custom_questions: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced version of load_and_normalize that includes QA extraction.\n",
    "    \"\"\"\n",
    "    # Use existing normalization function\n",
    "    document_data = load_and_normalize(file_path, table_name)\n",
    "    \n",
    "    # QA-based extraction\n",
    "    enhanced_data = process_document_with_qa(document_data, custom_questions)\n",
    "    \"\"\"Takes the normalized document\n",
    "Runs AI question-answering\n",
    "Adds intelligent extraction results\n",
    "Returns enhanced document with AI insights\"\"\"\n",
    "    return enhanced_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11a80b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAFE_RUN=True: skipping fine-tune auto-trigger\n"
     ]
    }
   ],
   "source": [
    "# Auto-trigger fine-tuning when enough new feedback exists\n",
    "try:\n",
    "    if not SAFE_RUN:\n",
    "        print(\"Checking if fine-tuning should run from feedback...\")\n",
    "        maybe_finetune(threshold_new_rows=5)\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"SAFE_RUN=True: skipping fine-tune auto-trigger\")\n",
    "except NameError:\n",
    "    print(\"Skipping auto-trigger: maybe_finetune not defined yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1328736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pdf file: Rechnung.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Document: Rechnung.pdf\n",
      "✅ Type: german_invoice\n",
      "✅ Success Rate: 64.5%\n",
      "✅ Extractions: 20/31\n",
      "\n",
      "=== High Confidence Results ===\n",
      "Q: Wie heißt die Firma auf der Rechnung?\n",
      "A:  Nordson Ahmed Ebada (confidence: 0.907)\n",
      "----------------------------------------\n",
      "Q: Welches Datum steht nach 'Rechnungsdatum'?\n",
      "A:  Lieferdatum (confidence: 0.934)\n",
      "----------------------------------------\n",
      "Q: What date appears after 'Rechnungsdatum'?\n",
      "A:  Lieferdatum (confidence: 0.883)\n",
      "----------------------------------------\n",
      "Q: What is the invoice number?\n",
      "A:  1407606058. (confidence: 0.551)\n",
      "----------------------------------------\n",
      "Q: What is the Rechnungsnummer?\n",
      "A:  1407606058. (confidence: 0.320)\n",
      "----------------------------------------\n",
      "Q: What is the total amount?\n",
      "A:  Gesamtbruttowert (EUR) 13,95 (confidence: 0.395)\n",
      "----------------------------------------\n",
      "Q: What is the invoice date?\n",
      "A:  06.02.2020 (confidence: 0.325)\n",
      "----------------------------------------\n",
      "Q: What is the Rechnungsdatum?\n",
      "A:  Lieferdatum (confidence: 0.653)\n",
      "----------------------------------------\n",
      "Q: Who is the customer?\n",
      "A:  Nordson Ahmed Ebada (confidence: 0.322)\n",
      "----------------------------------------\n",
      "Q: What is the Mehrwertsteuer?\n",
      "A:  19,00% (confidence: 0.826)\n",
      "----------------------------------------\n",
      "Q: What is the net amount?\n",
      "A:  (EUR) 11,72 (confidence: 0.331)\n",
      "----------------------------------------\n",
      "\n",
      "=== Named Entities Found ===\n"
     ]
    }
   ],
   "source": [
    "# Testing with German PDF\n",
    "german_path = r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\Rechnung.pdf\"\n",
    "\n",
    "# Load and process\n",
    "result = enhanced_load_and_normalize_with_qa(german_path)\n",
    "\n",
    "# Show improved results\n",
    "print(f\"✅ Document: {result['file_name']}\")\n",
    "print(f\"✅ Type: {result['qa_extraction']['document_type']}\")\n",
    "print(f\"✅ Success Rate: {result['qa_extraction']['success_rate']:.1%}\")\n",
    "print(f\"✅ Extractions: {result['qa_extraction']['successful_extractions']}/{result['qa_extraction']['total_questions']}\")\n",
    "\n",
    "print(\"\\n=== High Confidence Results ===\")\n",
    "for question, answer in result['high_confidence_extractions']['high_confidence_extractions'].items():\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer['answer']} (confidence: {answer['confidence']:.3f})\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Show NER entities if found\n",
    "if result['qa_extraction'].get('entities'):\n",
    "    print(\"\\n=== Named Entities Found ===\")\n",
    "    for entity_type, entities in result['qa_extraction']['entities'].items():\n",
    "        if entities:\n",
    "            print(f\"{entity_type}: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63cb69de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Multi-Format Document Extraction\n",
      "\n",
      " Testing: README.txt\n",
      "Processing text file: README.txt\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Type: general\n",
      "✅ Success Rate: 32.0%\n",
      "   Q: What companies or organizations are mentioned?...\n",
      "   A:  Deutsche Bahn\n",
      "   Q: What requirements or specifications are listed?...\n",
      "   A:  Python packages\n",
      "   Q: What actions or tasks are described?...\n",
      "   A:  Classification task\n"
     ]
    }
   ],
   "source": [
    "# Quick test of  multi-format system (extra .txt file)\n",
    "\n",
    "print(\"Testing Multi-Format Document Extraction\")\n",
    "\n",
    "test_files = [\n",
    "    r\"C:\\Users\\aslia\\OneDrive\\Desktop\\github\\Predicting-Train-Delays\\README.txt\"]\n",
    "\n",
    "\n",
    "for file_path in test_files:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"\\n Testing: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            result = enhanced_load_and_normalize_with_qa(file_path)\n",
    "            print(f\"✅ Type: {result['qa_extraction']['document_type']}\")\n",
    "            print(f\"✅ Success Rate: {result['qa_extraction']['success_rate']:.1%}\")\n",
    "            \n",
    "            # Show top 3 extractions\n",
    "            count = 0\n",
    "            for question, answer in result['qa_extraction']['extractions'].items():\n",
    "                if answer.get('answer') and count < 3:\n",
    "                    print(f\"   Q: {question[:50]}...\")\n",
    "                    print(f\"   A: {answer['answer']}\")\n",
    "                    count += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "    else:\n",
    "        print(f\" File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4182610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing OCR Integration with YOUR System ===\n",
      "Processing image file: invoice_sample.png\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OCR + QA Success!\n",
      "✅ Document: invoice_sample.png\n",
      "✅ Type: invoice\n",
      "✅ AI Extractions: 2\n",
      "Q: What customer name appears on the invoice?\n",
      "A:  Richard Sanchez (confidence: 0.983)\n",
      "----------------------------------------\n",
      "Q: What tax amount is mentioned?\n",
      "A:  10% (confidence: 0.320)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing with OCR + QA system\n",
    "\n",
    "print(\"=== Testing OCR Integration with YOUR System ===\")\n",
    "\n",
    "# Test image path\n",
    "image_path = r\"C:\\Users\\aslia\\Downloads\\invoice_sample.png\"\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    \n",
    "    # Use  existing pipeline\n",
    "    result = enhanced_load_and_normalize_with_qa(image_path)\n",
    "    \n",
    "    print(f\"✅ OCR + QA Success!\")\n",
    "    print(f\"✅ Document: {result['file_name']}\")\n",
    "    print(f\"✅ Type: {result['qa_extraction']['document_type']}\")\n",
    "    print(f\"✅ AI Extractions: {result['qa_extraction']['successful_extractions']}\")\n",
    "    \n",
    "    # Show results using system\n",
    "    for question, answer in result['high_confidence_extractions']['high_confidence_extractions'].items():\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {answer['answer']} (confidence: {answer['confidence']:.3f})\")\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"Add your image path to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca90862",
   "metadata": {},
   "source": [
    "**3. Self-Learning and Auto Fine-Tuning**\n",
    "* Build a feedback loop that captures user corrections, validation mismatches, or annotation logs.\n",
    "* Implement automatic model fine-tuning or retraining using this feedback without manual intervention.\n",
    "* Ensure continuous performance improvement while avoiding overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed5f4f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERACTIVE=False: auto-confirming predicted answer\n",
      "Feedback logged.\n"
     ]
    }
   ],
   "source": [
    "# Phase 3A: Feedback Collection\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Path to store feedback logs\n",
    "feedback_file = Path(\"feedback_log.csv\")\n",
    "\n",
    "# Create file with headers if it doesn't exist\n",
    "\n",
    "if not feedback_file.exists():\n",
    "    with open(feedback_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"timestamp\", \"question\", \"context\", \"predicted_answer\", \"correct_answer\"])\n",
    "\n",
    "# Captures human feedback for reinforcement\n",
    "def log_feedback(question, context, predicted_answer, correct_answer=None):\n",
    "    \"\"\"\n",
    "    Logs model output and user-provided corrections to a CSV file.\n",
    "    \"\"\"\n",
    "    with open(feedback_file, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            datetime.now().isoformat(),\n",
    "            question,\n",
    "            context,\n",
    "            predicted_answer,\n",
    "            correct_answer if correct_answer else \"\"\n",
    "        ])\n",
    "\n",
    "# Example usage after prediction step:\n",
    "\n",
    "question = \"What is the capital of Germany?\"\n",
    "context = \"Berlin is the capital and largest city of Germany.\"\n",
    "predicted_answer = \"Berlin\"\n",
    "\n",
    "# Suppose user confirms or corrects:\n",
    "\n",
    "if INTERACTIVE:\n",
    "    correct_answer = input(f\"Predicted answer: {predicted_answer}\\nIf wrong, type correct answer (or press Enter to confirm): \")\n",
    "else:\n",
    "    print(\"INTERACTIVE=False: auto-confirming predicted answer\")\n",
    "    correct_answer = predicted_answer\n",
    "\n",
    "print(\"Feedback logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd6f35",
   "metadata": {},
   "source": [
    "🔹feedback_log.csv is created the first time you run it. After each prediction, the system:\n",
    "   Shows the predicted answer.\n",
    "   Lets the user correct it or press Enter to confirm.\n",
    "\n",
    "Appends a new row with timestamp, Q, context, prediction, and correction.\n",
    "\n",
    "🔹This creates your personal dataset for fine-tuning.\n",
    " It ensures the model improves based on your domain-specific corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "309852af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback_dataset.json created!\n"
     ]
    }
   ],
   "source": [
    "# Phase 3B: Prepare Feedback for Fine-Tuning\n",
    "\n",
    "# Load feedback\n",
    "feedback_file = \"feedback_log.csv\"\n",
    "\n",
    "# Prepare SQuAD-style dataset(current model deepset/xlm-roberta-large-squad2 uses this format)\n",
    "data = {\n",
    "    \"version\": \"v2.0\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"feedback_data\",\n",
    "            \"paragraphs\": []\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "import hashlib \n",
    "with open(feedback_file, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        context = row[\"context\"]\n",
    "        question = row[\"question\"]\n",
    "        answer_text = row[\"correct_answer\"] or row[\"predicted_answer\"]\n",
    "\n",
    "        # Find start index of answer in context (required for SQuAD format)\n",
    "        start_index = context.find(answer_text)\n",
    "        if start_index == -1:\n",
    "            continue  # skip if answer not found in context\n",
    "\n",
    "        # Force string types and stable ID\n",
    "        safe_q = str(question)\n",
    "        safe_ctx = str(context)\n",
    "        qid = hashlib.sha1((safe_q + \"||\" + safe_ctx).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        paragraph = {\n",
    "            \"context\": safe_ctx,\n",
    "            \"qas\": [\n",
    "                {\n",
    "                    \"id\": qid,\n",
    "                    \"question\": safe_q,\n",
    "                    \"answers\": [\n",
    "                        {\n",
    "                            \"text\": answer_text,\n",
    "                            \"answer_start\": start_index\n",
    "                        }\n",
    "                    ],\n",
    "                    \"is_impossible\": False\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        data[\"data\"][0][\"paragraphs\"].append(paragraph)\n",
    "\n",
    "# Save dataset\n",
    "with open(\"feedback_dataset.json\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(data, out_file, ensure_ascii=False, indent=2)\n",
    "print(\"feedback_dataset.json created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b74aa",
   "metadata": {},
   "source": [
    "* Read feedback_log.csv ( manually collected corrections).\n",
    "\n",
    "* Convert it to SQuAD-style JSON : required for most extractive QA fine-tuning (current model deepset/xlm-roberta-large-squad2 uses this format).\n",
    "\n",
    "* Save it as feedback_dataset.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be651aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback_dataset.json loaded: 7 paragraphs\n"
     ]
    }
   ],
   "source": [
    "# Phase 3C:load the feedback_dataset.json into a Hugging Face Dataset object.\n",
    "\n",
    "if os.path.exists(\"feedback_dataset.json\"):\n",
    "    with open(\"feedback_dataset.json\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"feedback_dataset.json loaded: {len(data['data'][0]['paragraphs'])} paragraphs\")\n",
    "else:\n",
    "    print(\"feedback_dataset.json not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc3ad8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,574,914 || all params: 560,417,796 || trainable%: 0.2810\n",
      " Starting LoRA fine-tuning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA fine-tuning complete! Saved to './lora_fine_tuned_model'\n"
     ]
    }
   ],
   "source": [
    "# Phase 3D: Fine-tuning with LoRA\n",
    "\n",
    "import os, json, torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    TrainingArguments, Trainer, DefaultDataCollator\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load your SQuAD-style feedback JSON\n",
    "\n",
    "with open(\"feedback_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "examples = []\n",
    "for article in data[\"data\"]:\n",
    "    for para in article[\"paragraphs\"]:\n",
    "        ctx = para[\"context\"]\n",
    "        for qa in para[\"qas\"]:\n",
    "            ans = qa[\"answers\"][0] if qa.get(\"answers\") else {\"text\": \"\", \"answer_start\": 0}\n",
    "            examples.append({\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"context\": ctx,\n",
    "                \"answer_text\": ans.get(\"text\", \"\"),\n",
    "                \"answer_start\": ans.get(\"answer_start\", 0),\n",
    "            })\n",
    "\n",
    "def make_features(ex):\n",
    "    tok = tokenizer(\n",
    "        ex[\"question\"],\n",
    "        ex[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    offsets = tok.pop(\"offset_mapping\")\n",
    "    seq_ids = tok.sequence_ids()\n",
    "\n",
    "    # Context token span\n",
    "    start_ctx = 0\n",
    "    while start_ctx < len(seq_ids) and seq_ids[start_ctx] != 1:\n",
    "        start_ctx += 1\n",
    "    end_ctx = len(seq_ids) - 1\n",
    "    while end_ctx >= 0 and seq_ids[end_ctx] != 1:\n",
    "        end_ctx -= 1\n",
    "\n",
    "    start_char = ex[\"answer_start\"]\n",
    "    end_char = start_char + len(ex[\"answer_text\"])\n",
    "\n",
    "    if ex[\"answer_text\"] == \"\":\n",
    "        start_pos = end_pos = start_ctx\n",
    "    elif not (offsets[start_ctx][0] <= start_char and offsets[end_ctx][1] >= end_char):\n",
    "        start_pos = end_pos = start_ctx\n",
    "    else:\n",
    "        s = start_ctx\n",
    "        while s < len(offsets) and offsets[s][0] <= start_char:\n",
    "            s += 1\n",
    "        start_pos = s - 1\n",
    "        e = end_ctx\n",
    "        while e > 0 and offsets[e][1] >= end_char:\n",
    "            e -= 1\n",
    "        end_pos = e + 1\n",
    "\n",
    "    tok[\"start_positions\"] = start_pos\n",
    "    tok[\"end_positions\"] = end_pos\n",
    "    return tok\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.features = [make_features(ex) for ex in examples]\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, i):\n",
    "        return {k: torch.tensor(v) for k, v in self.features[i].items()}\n",
    "\n",
    "train_dataset = QADataset(examples)\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "\n",
    "#base model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# LoRA Configuration \n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    "    inference_mode=False,\n",
    "    r=16,  # Rank - higher = more parameters but slower\n",
    "    lora_alpha=32,  # Scaling parameter\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    target_modules=[\"query\", \"value\"]  # Which layers to adapt\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  \n",
    "\n",
    "# training arguments for LoRA\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./lora_fine_tuned_model\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=3e-4, \n",
    "    per_device_train_batch_size=4,  # Can use larger batch\n",
    "    num_train_epochs=2,  # Fewer epochs needed\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    "    report_to=None,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_strategy=\"epoch\",\n",
    "    warmup_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DefaultDataCollator(),\n",
    ")\n",
    "\n",
    "print(\" Starting LoRA fine-tuning\")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./lora_fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./lora_fine_tuned_model\")\n",
    "print(\"✅ LoRA fine-tuning complete! Saved to './lora_fine_tuned_model'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d387ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3E:Automatic fine-tuning \n",
    "\n",
    "# This is the \"self-learning\" part that makes this system continuously improve without human intervention\n",
    "# heart of the self-learning system\n",
    "\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    TrainingArguments, Trainer, DefaultDataCollator\n",
    ")\n",
    "\n",
    "#1.  File Management Setup\n",
    "\"\"\"Defines paths for state tracking, feedback storage, and model output\n",
    "STATE_FILE: Tracks training history (.self_tune_state.json)\n",
    "FEEDBACK_CSV: User corrections (feedback_log.csv)\n",
    "FEEDBACK_JSON: Training-ready format (feedback_dataset.json)\"\"\"\n",
    "\n",
    "STATE_FILE = Path(\".self_tune_state.json\") # Tracks training history\n",
    "FEEDBACK_CSV = Path(\"feedback_log.csv\")  # User corrections\n",
    "FEEDBACK_JSON = Path(\"feedback_dataset.json\") # Training-ready format\n",
    "OUTPUT_DIR = Path(\"./lora_fine_tuned_model\") # Where improved model is saved\n",
    "\n",
    "BASE_MODEL = \"deepset/xlm-roberta-large-squad2\"\n",
    "\n",
    "\n",
    "#2. State Tracking Functions\n",
    "\"\"\"_load_state() and _save_state(): Remember how many feedback rows were used for training\n",
    "_feedback_count(): Count new user corrections since last training\n",
    "Prevents retraining on the same data repeatedly\"\"\"\n",
    "\n",
    "def _load_state(): # + save?state Remember how many feedback rows were used for training\n",
    "    if STATE_FILE.exists():\n",
    "        return json.loads(STATE_FILE.read_text(encoding=\"utf-8\"))\n",
    "    return {\"last_trained_count\": 0, \"last_trained_at\": 0}\n",
    "\n",
    "def _save_state(state):\n",
    "    STATE_FILE.write_text(json.dumps(state, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _feedback_count():   # Count new user corrections since last training\n",
    "    if not FEEDBACK_CSV.exists():\n",
    "        return 0\n",
    "    with FEEDBACK_CSV.open(newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = list(reader)\n",
    "        return max(0, len(rows) - 1)  # minus header\n",
    "\n",
    "#3. Data Conversion Pipeline\n",
    "\"\"\"_build_squad_from_csv(): Converts user feedback CSV into SQuAD format (the training format AI models expect)\n",
    "_load_examples(): Loads and prepares training examples\n",
    "_features_maker(): Tokenizes questions/contexts and maps answer positions\"\"\"\n",
    "\n",
    "def _build_squad_from_csv(csv_path=FEEDBACK_CSV, out_json=FEEDBACK_JSON): #cnverts feedbacks CSV into SQuAD format (the training format AI models expect)\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(\"feedback_log.csv not found\")\n",
    "    import hashlib \n",
    "    data = {\"version\": \"v2.0\", \"data\": [{\"title\": \"feedback_data\", \"paragraphs\": []}]}\n",
    "    with csv_path.open(newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            ctx = row[\"context\"]\n",
    "            q = row[\"question\"]\n",
    "            ans_text = row[\"correct_answer\"] or row[\"predicted_answer\"]\n",
    "            start = ctx.find(ans_text)\n",
    "            if start == -1:\n",
    "                continue\n",
    "            safe_q = str(q)\n",
    "            safe_ctx = str(ctx)\n",
    "            qid = hashlib.sha1((safe_q + \"||\" + safe_ctx).encode(\"utf-8\")).hexdigest()\n",
    "            data[\"data\"][0][\"paragraphs\"].append({\n",
    "                \"context\": safe_ctx,\n",
    "                \"qas\": [{\n",
    "                    \"id\": qid,\n",
    "                    \"question\": safe_q,\n",
    "                    \"answers\": [{\"text\": ans_text, \"answer_start\": start}],\n",
    "                    \"is_impossible\": False\n",
    "                }]\n",
    "            })\n",
    "    out_json.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return out_json\n",
    "\n",
    "\n",
    "def _load_examples(feedback_json=FEEDBACK_JSON):\n",
    "    d = json.loads(Path(feedback_json).read_text(encoding=\"utf-8\"))\n",
    "    examples = []\n",
    "    for article in d[\"data\"]:\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            ctx = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                ans = qa[\"answers\"][0] if qa.get(\"answers\") else {\"text\": \"\", \"answer_start\": 0}\n",
    "                examples.append({\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"context\": ctx,\n",
    "                    \"answer_text\": ans.get(\"text\", \"\"),\n",
    "                    \"answer_start\": ans.get(\"answer_start\", 0),\n",
    "                })\n",
    "    return examples\n",
    "\n",
    "\n",
    "\n",
    "def _features_maker(tokenizer): \n",
    "    def make_features(ex):\n",
    "        tok = tokenizer(\n",
    "            ex[\"question\"], ex[\"context\"],\n",
    "            truncation=\"only_second\", max_length=384, stride=128,\n",
    "            return_offsets_mapping=True, padding=\"max_length\",\n",
    "        )\n",
    "        offsets = tok.pop(\"offset_mapping\")\n",
    "        seq_ids = tok.sequence_ids()\n",
    "        start_ctx = 0\n",
    "        while start_ctx < len(seq_ids) and seq_ids[start_ctx] != 1:\n",
    "            start_ctx += 1\n",
    "        end_ctx = len(seq_ids) - 1\n",
    "        while end_ctx >= 0 and seq_ids[end_ctx] != 1:\n",
    "            end_ctx -= 1\n",
    "        start_char = ex[\"answer_start\"]\n",
    "        end_char = start_char + len(ex[\"answer_text\"])\n",
    "        if ex[\"answer_text\"] == \"\":\n",
    "            start_pos = end_pos = start_ctx\n",
    "        elif not (offsets[start_ctx][0] <= start_char and offsets[end_ctx][1] >= end_char):\n",
    "            start_pos = end_pos = start_ctx\n",
    "        else:\n",
    "            s = start_ctx\n",
    "            while s < len(offsets) and offsets[s][0] <= start_char:\n",
    "                s += 1\n",
    "            start_pos = s - 1\n",
    "            e = end_ctx\n",
    "            while e > 0 and offsets[e][1] >= end_char:\n",
    "                e -= 1\n",
    "            end_pos = e + 1\n",
    "        tok[\"start_positions\"] = start_pos\n",
    "        tok[\"end_positions\"] = end_pos\n",
    "        return tok\n",
    "    return make_features\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer):\n",
    "        maker = _features_maker(tokenizer)\n",
    "        self.features = [maker(ex) for ex in examples]\n",
    "    def __len__(self): return len(self.features)\n",
    "    def __getitem__(self, i): return {k: torch.tensor(v) for k, v in self.features[i].items()}\n",
    "\n",
    "\n",
    "\n",
    "#4. LoRA Fine-Tuning Functions\n",
    "# train_from_feedback function\n",
    "\n",
    "\"\"\"train_from_feedback_lora(): The actual fast LoRA fine-tuning process\n",
    "QADataset: PyTorch dataset class for training\n",
    "Much faster than full fine-tuning\"\"\"\n",
    "\n",
    "def train_from_feedback_lora(base_model=None, output_dir=\"./lora_fine_tuned_model\", epochs=2, lr=3e-4, batch_size=4):\n",
    "    \"\"\"Fast LoRA fine-tuning from feedback\"\"\"\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    base = base_model or BASE_MODEL\n",
    "    \n",
    "    _build_squad_from_csv(FEEDBACK_CSV, FEEDBACK_JSON)\n",
    "    examples = _load_examples(FEEDBACK_JSON)\n",
    "    \n",
    "    if not examples:\n",
    "        print(\"No examples to train.\")\n",
    "        return False\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base)\n",
    "    dataset = QADataset(examples, tokenizer)\n",
    "    print(f\" LoRA training samples: {len(dataset)} (base={base})\")\n",
    "\n",
    "    # Load base model\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(base)\n",
    "    \n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.QUESTION_ANS,\n",
    "        inference_mode=False,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query\", \"value\"]\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        eval_strategy=\"no\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=5,\n",
    "        save_total_limit=1,\n",
    "        push_to_hub=False,\n",
    "        report_to=None,\n",
    "        dataloader_pin_memory=False,\n",
    "        warmup_steps=10,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=DefaultDataCollator(),\n",
    "    )\n",
    "    \n",
    "    print(\" Starting fast LoRA training...\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(str(output_dir))\n",
    "    tokenizer.save_pretrained(str(output_dir))\n",
    "    print(f\"✅ LoRA model saved to {output_dir}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# 5. Auto-Trigger automatically retrains when enough feedback collected actuaşl \"self-training\" part\n",
    "\n",
    "\n",
    "\"\"\"maybe_finetune_lora(): The \"smart automation\" that checks if enough new feedback exists\n",
    "Default threshold: 5 new corrections trigger automatic retraining\n",
    "Updates state tracking after successful training\"\"\"\n",
    "\n",
    "def maybe_finetune_lora(threshold_new_rows=5):\n",
    "    \"\"\"Auto-trigger LoRA fine-tuning\"\"\"\n",
    "    state = _load_state()\n",
    "    total = _feedback_count()\n",
    "    new = total - state.get(\"last_trained_count\", 0)\n",
    "    print(f\"Feedback rows: {total} (new since last train: {new})\")\n",
    "    if new >= threshold_new_rows:\n",
    "        print(\" Starting fast LoRA fine-tuning...\")\n",
    "        ok = train_from_feedback_lora()  # Much faster!\n",
    "        if ok:\n",
    "            state[\"last_trained_count\"] = total\n",
    "            state[\"last_trained_at\"] = int(time.time())\n",
    "            _save_state(state)\n",
    "            print(\"✅ LoRA fine-tuned and state updated.\")\n",
    "    else:\n",
    "        print(f\"Not enough new feedback yet (need {threshold_new_rows}).\")\n",
    "        \n",
    "        \"\"\"How It Works in Practice:\n",
    "You use the system → AI makes predictions\n",
    "You correct wrong answers → Logged to feedback_log.csv\n",
    "Background check → maybe_finetune() counts new corrections\n",
    "Auto-training → When you have 5+ new corrections, it automatically fine-tunes\n",
    "Improved model → Next time you use the system, it's smarter\n",
    "Why This Is Powerful:\n",
    "Zero manual work: No need to manually retrain\n",
    "Continuous improvement: Gets better with each correction you make\n",
    "Domain-specific: Learns YOUR specific documents and terminology\n",
    "Memory: Never forgets previous corrections\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aea508",
   "metadata": {},
   "source": [
    "**4.\tModel Evaluation and Versioning**\n",
    "* Define evaluation metrics for accuracy, confidence, and error detection.\n",
    "* Track model versions and performance history, and allow safe promotion of improved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "781c2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Versioning System\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, metrics_file=\"model_metrics.json\"):\n",
    "        # Creates a tracker that saves results to a JSON file\n",
    "        self.metrics_file = Path(metrics_file)\n",
    "        self.metrics_history = self._load_metrics() # Loads previous results\n",
    "        \n",
    "    def _load_metrics(self):\n",
    "        if self.metrics_file.exists():\n",
    "            return json.loads(self.metrics_file.read_text())\n",
    "        return {\"evaluations\": []}\n",
    "    \n",
    "    def _save_metrics(self):\n",
    "        self.metrics_file.write_text(json.dumps(self.metrics_history, indent=2))\n",
    "    \n",
    "    def create_ground_truth_template(self, invoice_files): # invoice_files is a LIST of paths\n",
    "        \"\"\"Create template for manual ground truth annotation\"\"\" \n",
    "        template = {}\n",
    "        for file_path in invoice_files:\n",
    "            filename = Path(file_path).name  # Extract just filename (eg \"invoice1.pdf\")\n",
    "            template[filename] = {\n",
    "                \"invoice_number\": \"\",\n",
    "                \"total_amount\": \"\",\n",
    "                \"company_name\": \"\",\n",
    "                \"invoice_date\": \"\",\n",
    "                \"tax_amount\": \"\",\n",
    "                \"net_amount\": \"\",\n",
    "                \"customer_name\": \"\",\n",
    "                \"notes\": \"Manual annotation needed\"\n",
    "            }\n",
    "        \n",
    "        # Save template\n",
    "        with open(\"ground_truth_template.json\", \"w\") as f:\n",
    "            json.dump(template, f, indent=2)\n",
    "        \n",
    "        print(\"Ground truth template created: ground_truth_template.json\")\n",
    "        print(\"Please fill in the correct values manually\")\n",
    "        return template\n",
    "    \n",
    "    def evaluate_extraction_results(self, results_dict, ground_truth=None, document_name=\"\"):\n",
    "        \"\"\"Evaluate extraction quality with multiple metrics\"\"\"\n",
    "        evaluation = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"document_name\": document_name,\n",
    "            \"model_version\": results_dict.get('qa_extraction', {}).get('model_used', 'unknown'),\n",
    "            \"document_type\": results_dict.get('qa_extraction', {}).get('document_type', 'unknown'),\n",
    "            \"total_questions\": results_dict.get('qa_extraction', {}).get('total_questions', 0),\n",
    "            \"successful_extractions\": results_dict.get('qa_extraction', {}).get('successful_extractions', 0),\n",
    "            \"success_rate\": results_dict.get('qa_extraction', {}).get('success_rate', 0),\n",
    "            \"avg_confidence\": self._calculate_avg_confidence(results_dict),\n",
    "            \"high_confidence_count\": len(results_dict.get('high_confidence_extractions', {}).get('high_confidence_extractions', {}))\n",
    "        } \n",
    "        \n",
    "        # Add ground truth comparison if available\n",
    "        if ground_truth:\n",
    "            evaluation.update(self._compare_with_ground_truth(results_dict, ground_truth))\n",
    "        \n",
    "        self.metrics_history[\"evaluations\"].append(evaluation)\n",
    "        self._save_metrics()\n",
    "        return evaluation\n",
    "    \n",
    "    def _calculate_avg_confidence(self, results_dict):\n",
    "        extractions = results_dict.get('qa_extraction', {}).get('extractions', {})\n",
    "        confidences = [result.get('confidence', 0) for result in extractions.values() \n",
    "                      if result.get('confidence') is not None]\n",
    "        return sum(confidences) / len(confidences) if confidences else 0\n",
    "    \n",
    "    def _compare_with_ground_truth(self, results_dict, ground_truth):\n",
    "        \"\"\"Compare extractions with ground truth for accuracy metrics\"\"\"\n",
    "        extractions = results_dict.get('qa_extraction', {}).get('extractions', {})\n",
    "        \n",
    "        # Key extraction mapping\n",
    "        key_mappings = {\n",
    "            \"invoice_number\": [\"rechnungsnummer\", \"invoice number\", \"nummer\"],\n",
    "            \"total_amount\": [\"gesamtbetrag\", \"total\", \"endbetrag\", \"amount\"],\n",
    "            \"company_name\": [\"company\", \"firma\", \"firmenname\", \"vendor\"],\n",
    "            \"invoice_date\": [\"datum\", \"date\", \"rechnungsdatum\"],\n",
    "            \"tax_amount\": [\"mwst\", \"ust\", \"tax\", \"mehrwertsteuer\"],\n",
    "            \"net_amount\": [\"netto\", \"net\", \"nettobetrag\"]\n",
    "        }\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        detailed_results = {}\n",
    "        \n",
    "        for gt_key, gt_value in ground_truth.items():\n",
    "            if not gt_value:  # Skip empty ground truth values\n",
    "                continue\n",
    "                \n",
    "            best_match = None\n",
    "            best_similarity = 0\n",
    "            \n",
    "            # Find best matching extraction\n",
    "            for question, result in extractions.items():\n",
    "                if result.get('answer') and any(keyword in question.lower() for keyword in key_mappings.get(gt_key, [])):\n",
    "                    similarity = self._answers_match_score(result['answer'], gt_value)\n",
    "                    if similarity > best_similarity:\n",
    "                        best_similarity = similarity\n",
    "                        best_match = result['answer']\n",
    "            \n",
    "            total += 1\n",
    "            is_correct = best_similarity >= 0.8\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            \n",
    "            detailed_results[gt_key] = {\n",
    "                \"ground_truth\": gt_value,\n",
    "                \"predicted\": best_match,\n",
    "                \"similarity\": best_similarity,\n",
    "                \"correct\": is_correct\n",
    "            }\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        return {\n",
    "            \"ground_truth_accuracy\": accuracy,\n",
    "            \"correct_answers\": correct,\n",
    "            \"total_compared\": total,\n",
    "            \"detailed_results\": detailed_results\n",
    "        }\n",
    "    \n",
    "    def _answers_match_score(self, predicted, actual):\n",
    "        \"\"\"Calculate similarity score between predicted and actual answers\"\"\"\n",
    "        if not predicted or not actual:\n",
    "            return 0\n",
    "        \n",
    "        # Clean and normalize\n",
    "        pred_clean = str(predicted).lower().strip()\n",
    "        actual_clean = str(actual).lower().strip()\n",
    "        \n",
    "        # Exact match\n",
    "        if pred_clean == actual_clean:\n",
    "            return 1.0\n",
    "        \n",
    "        # Fuzzy matching\n",
    "        similarity = SequenceMatcher(None, pred_clean, actual_clean).ratio()\n",
    "        \n",
    "        # Bonus for number matching (important for invoices)\n",
    "    \n",
    "        pred_numbers = re.findall(r'\\d+[\\.,]?\\d*', pred_clean)\n",
    "        actual_numbers = re.findall(r'\\d+[\\.,]?\\d*', actual_clean)\n",
    "        \n",
    "        if pred_numbers and actual_numbers:\n",
    "            # Normalize numbers for comparison\n",
    "            pred_num = pred_numbers[0].replace(',', '.')\n",
    "            actual_num = actual_numbers[0].replace(',', '.')\n",
    "            try:\n",
    "                if float(pred_num) == float(actual_num):\n",
    "                    similarity = max(similarity, 0.9)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def batch_evaluate_invoices(self, invoice_folder, ground_truth_file=None):\n",
    "        \"\"\" Runs your AI on multiple invoices to generates performance report.\"\"\"\n",
    "        invoice_folder = Path(invoice_folder)\n",
    "        results = []\n",
    "        \n",
    "        # Load ground truth\n",
    "        ground_truth_data = {}\n",
    "        if ground_truth_file and Path(ground_truth_file).exists():\n",
    "            with open(ground_truth_file) as f:\n",
    "                ground_truth_data = json.load(f)\n",
    "        \n",
    "        # Process each file type separately to fix the glob() error\n",
    "        supported_patterns = [\"*.pdf\", \"*.png\", \"*.jpg\", \"*.jpeg\", \"*.tiff\", \"*.bmp\"]\n",
    "        \n",
    "        for pattern in supported_patterns:\n",
    "            for invoice_file in invoice_folder.glob(pattern):  # ONE pattern at a time\n",
    "                print(f\"Processing: {invoice_file.name}\")\n",
    "                \n",
    "                try:\n",
    "                    # Extract using your existing pipeline\n",
    "                    result = enhanced_load_and_normalize_with_qa(str(invoice_file))\n",
    "                    \n",
    "                    # Get ground truth for this file\n",
    "                    gt = ground_truth_data.get(invoice_file.name, {})\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    evaluation = self.evaluate_extraction_results(result, gt, invoice_file.name)\n",
    "                    results.append(evaluation)\n",
    "                    \n",
    "                    print(f\"✅ Success rate: {evaluation['success_rate']:.1%}\")\n",
    "                    if gt:\n",
    "                        print(f\"✅ Accuracy: {evaluation.get('ground_truth_accuracy', 0):.1%}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error processing {invoice_file.name}: {e}\")\n",
    "        \n",
    "        # Generate summary report\n",
    "        if results:\n",
    "            self._generate_evaluation_report(results)\n",
    "        else:\n",
    "            print(\"❌ No files were processed successfully!\")\n",
    "            print(\" Check if files exist in:\", str(invoice_folder.absolute()))\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _generate_evaluation_report(self, results):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        if not results:\n",
    "            print(\"No results to report\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "=== INVOICE EXTRACTION EVALUATION REPORT ===\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "    OVERALL PERFORMANCE:\n",
    "• Total invoices processed: {len(results)}\n",
    "• Average success rate: {df['success_rate'].mean():.1%}\n",
    "• Average confidence: {df['avg_confidence'].mean():.3f}\n",
    "• High confidence extractions: {df['high_confidence_count'].mean():.1f}/question\n",
    "\n",
    "    ACCURACY METRICS:\n",
    "\"\"\"\n",
    "        \n",
    "        if 'ground_truth_accuracy' in df.columns:\n",
    "            accuracy_data = df.dropna(subset=['ground_truth_accuracy'])\n",
    "            if not accuracy_data.empty:\n",
    "                report += f\"\"\"• Ground truth accuracy: {accuracy_data['ground_truth_accuracy'].mean():.1%}\n",
    "• Correct answers: {accuracy_data['correct_answers'].sum()}/{accuracy_data['total_compared'].sum()}\n",
    "• Best performing invoice: {accuracy_data.loc[accuracy_data['ground_truth_accuracy'].idxmax(), 'document_name']}\n",
    "• Worst performing invoice: {accuracy_data.loc[accuracy_data['ground_truth_accuracy'].idxmin(), 'document_name']}\n",
    "\"\"\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    " PERFORMANCE DISTRIBUTION:\n",
    "• Success rate std dev: {df['success_rate'].std():.3f}\n",
    "• Confidence std dev: {df['avg_confidence'].std():.3f}\n",
    "\n",
    "    RECOMMENDATIONS:\n",
    "\"\"\"\n",
    "        \n",
    "        avg_success = df['success_rate'].mean()\n",
    "        if avg_success < 0.7:\n",
    "            report += \"• Consider expanding question templates\\n\"\n",
    "        if avg_success > 0.8:\n",
    "            report += \"• System performing well - ready for production\\n\"\n",
    "        \n",
    "        avg_confidence = df['avg_confidence'].mean()\n",
    "        if avg_confidence < 0.5:\n",
    "            report += \"• Low confidence scores - may need more training data\\n\"\n",
    "        \n",
    "        print(report)\n",
    "        \n",
    "        # Save report\n",
    "        with open(f\"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\", \"w\") as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Model Version Manager :  Tracks different model versions and safely promotes improvements\n",
    "class ModelVersionManager:\n",
    "    def __init__(self, versions_file=\"model_versions.json\"):\n",
    "        self.versions_file = Path(versions_file)\n",
    "        self.versions = self._load_versions()\n",
    "    \n",
    "    def _load_versions(self):\n",
    "        if self.versions_file.exists():\n",
    "            return json.loads(self.versions_file.read_text())\n",
    "        return {\"versions\": [], \"current_version\": None}\n",
    "    \n",
    "    def _save_versions(self):\n",
    "        self.versions_file.write_text(json.dumps(self.versions, indent=2))\n",
    "    \n",
    "    def register_new_version(self, model_path, performance_metrics, description=\"\"):\n",
    "        \"\"\"Registers each new fine-tuned model with its performance scores\"\"\"\n",
    "        version_info = {\n",
    "            \"version_id\": f\"v{len(self.versions['versions']) + 1}\", #v1,v2,v3\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_path\": str(model_path),  # ./lora_fine_tuned_model\n",
    "            \"performance\": performance_metrics,  # Success rates, accuracy\n",
    "            \"description\": description,\n",
    "            \"is_active\": False       # Not active yet\n",
    "        }\n",
    "        \n",
    "        self.versions[\"versions\"].append(version_info)\n",
    "        self._save_versions()\n",
    "        return version_info[\"version_id\"]\n",
    "    \n",
    "    def promote_version(self, version_id, min_success_rate=0.7, min_accuracy=0.6):\n",
    "        \"\"\" Safety checks before making model active,\n",
    "        Only promotes models that perform better than thresholds (prevents regression)\"\"\"\n",
    "        version = self._find_version(version_id)\n",
    "        if not version:\n",
    "            return False, \"Version not found\"\n",
    "        \n",
    "        # Check performance criteria\n",
    "        success_rate = version[\"performance\"].get(\"avg_success_rate\", 0)\n",
    "        accuracy = version[\"performance\"].get(\"avg_accuracy\", 0)\n",
    "        \n",
    "        if success_rate < min_success_rate:\n",
    "            return False, f\"Success rate {success_rate:.2%} below threshold {min_success_rate:.2%}\"\n",
    "        \n",
    "        if accuracy > 0 and accuracy < min_accuracy:\n",
    "            return False, f\"Accuracy {accuracy:.2%} below threshold {min_accuracy:.2%}\"\n",
    "        \n",
    "        # Deactivate current version\n",
    "        for v in self.versions[\"versions\"]:\n",
    "            v[\"is_active\"] = False\n",
    "        \n",
    "        # Activate new version\n",
    "        version[\"is_active\"] = True\n",
    "        self.versions[\"current_version\"] = version_id\n",
    "        self._save_versions()\n",
    "        \n",
    "        return True, f\"Version {version_id} promoted successfully\"\n",
    "    \n",
    "    def _find_version(self, version_id):\n",
    "        for version in self.versions[\"versions\"]:\n",
    "            if version[\"version_id\"] == version_id:\n",
    "                return version\n",
    "        return None\n",
    "    \n",
    "    \"\"\"Research Paper Benefits:\n",
    "Quantitative results showing improvement over time\n",
    "Comparison metrics before/after self-learning\n",
    "Performance graphs demonstrating ReST effectiveness\n",
    "Statistical validation of your approach\"\"\"\n",
    "\n",
    "# Model Evaluation and Versioning System ready after this next 10-20 invoices for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b50f9404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STARTING LARGE-SCALE DATASET SETUP\n",
      "==================================================\n",
      "\n",
      " Scanning: C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\new_invoices_dataset\n",
      "\n",
      " Scanning: C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\n",
      " Processing: new_invoices_dataset\n",
      " Copied 100 files...\n",
      " Copied 150 files from this folder\n",
      " Processing: invoice_dataset\n",
      " Copied 6 files from this folder\n",
      "\n",
      "============================================================\n",
      " DATASET SETUP COMPLETE\n",
      "============================================================\n",
      " Total invoices ready: 156\n",
      " Location: c:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\large_scale_invoice_dataset\n",
      " Errors: 0\n",
      " Only 156 files found - may need more datasets\n",
      "\n",
      "✅ Ready for large-scale evaluation with 156 documents\n",
      " Files location: ./large_scale_invoice_dataset/\n",
      " Next: Run your evaluation system on this dataset\n"
     ]
    }
   ],
   "source": [
    "# Step 4A: Large-Scale Dataset Setup for 1000+ Invoice Evaluation\n",
    "\n",
    "\n",
    "def setup_large_scale_invoice_dataset():\n",
    "    \"\"\"\n",
    "    Dataset setup for large-scale evaluation\n",
    "    Automatically discovers and organizes invoices from multiple source folders\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration for dataset sources\n",
    "    DATASET_SOURCES = [\n",
    "        \n",
    "        # downloaded dataset\n",
    "        r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\new_invoices_dataset\",\n",
    "\n",
    "        # Original test files\n",
    "        r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\",\n",
    "        \n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Supported file extensions\n",
    "    SUPPORTED_EXTENSIONS = ['.pdf', '.png', '.jpg', '.jpeg', '.tiff', '.bmp']\n",
    "\n",
    "    \n",
    "    # Create main evaluation directory\n",
    "    EVAL_DIR = Path(\"./large_scale_invoice_dataset\")\n",
    "    EVAL_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {'total_copied': 0, 'errors': []}\n",
    "    \n",
    "    # Scan all source directories\n",
    "    for source_dir in DATASET_SOURCES:\n",
    "        source_path = Path(source_dir)\n",
    "        \n",
    "        if not source_path.exists():\n",
    "            print(f\" Source not found: {source_dir}\")\n",
    "            stats['errors'].append(f\"Directory not found: {source_dir}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n Scanning: {source_dir}\")\n",
    "        folder_stats = {'pdf': 0, 'image': 0, 'other': 0, 'total': 0}\n",
    "        \n",
    "        # Scan all source directories\n",
    "    for source_dir in DATASET_SOURCES:\n",
    "        source_path = Path(source_dir)\n",
    "        \n",
    "        if not source_path.exists():\n",
    "            print(f\" Source not found: {source_dir}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\" Processing: {source_path.name}\")\n",
    "        folder_count = 0\n",
    "        \n",
    "        # Find and copy all supported files\n",
    "        for ext in SUPPORTED_EXTENSIONS:\n",
    "            pattern = f\"**/*{ext}\"\n",
    "            for file_path in source_path.glob(pattern):\n",
    "                if file_path.is_file():\n",
    "                    # Create unique filename\n",
    "                    counter = 1\n",
    "                    dest_name = file_path.name\n",
    "                    dest_path = EVAL_DIR / dest_name\n",
    "                    \n",
    "                    while dest_path.exists():\n",
    "                        stem = file_path.stem\n",
    "                        suffix = file_path.suffix\n",
    "                        dest_name = f\"{stem}_{counter}{suffix}\"\n",
    "                        dest_path = EVAL_DIR / dest_name\n",
    "                        counter += 1\n",
    "                    \n",
    "                    try:\n",
    "                        shutil.copy2(file_path, dest_path)\n",
    "                        stats['total_copied'] += 1\n",
    "                        folder_count += 1\n",
    "                        \n",
    "                        # Progress indicator\n",
    "                        if stats['total_copied'] % 100 == 0:\n",
    "                            print(f\" Copied {stats['total_copied']} files...\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        stats['errors'].append(f\"Failed: {file_path.name}\")\n",
    "        \n",
    "        print(f\" Copied {folder_count} files from this folder\")\n",
    "        \n",
    "# Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\" DATASET SETUP COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\" Total invoices ready: {stats['total_copied']}\")\n",
    "    print(f\" Location: {EVAL_DIR.absolute()}\")\n",
    "    print(f\" Errors: {len(stats['errors'])}\")\n",
    "    if stats['total_copied'] >= 1000:\n",
    "        print(\"✅ 1000+ REQUIREMENT MET - READY FOR LARGE-SCALE TESTING!\")\n",
    "    else:\n",
    "        print(f\" Only {stats['total_copied']} files found - may need more datasets\")\n",
    "        \n",
    "    # Save simple report\n",
    "    with open(\"dataset_summary.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            'total_files': stats['total_copied'],\n",
    "            'setup_completed': datetime.now().isoformat(),\n",
    "            'dataset_location': str(EVAL_DIR.absolute()),\n",
    "            'ready_for_evaluation': stats['total_copied'] >= 1000\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Execute the setup\n",
    "print(\" STARTING LARGE-SCALE DATASET SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "setup_stats = setup_large_scale_invoice_dataset()\n",
    "\n",
    "if setup_stats['total_copied'] > 0:\n",
    "    # Create evaluator for large-scale testing\n",
    "    evaluator = ModelEvaluator()\n",
    "    print(f\"\\n✅ Ready for large-scale evaluation with {setup_stats['total_copied']} documents\")\n",
    "    print(\" Files location: ./large_scale_invoice_dataset/\")\n",
    "    print(\" Next: Run your evaluation system on this dataset\")\n",
    "else:\n",
    "    print(\"\\n No files copied - please check your source paths\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a8e08aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running SMART AUTOMATED evaluation on 156 files...\n",
      "    PDFs: 109\n",
      "   Images: 47\n",
      " STARTING FULLY AUTOMATED EVALUATION PIPELINE\n",
      "============================================================\n",
      "\n",
      " Step 1: Creating smart ground truth...\n",
      "Processing 1/160: invoice_10697.pdf\n",
      "Processing pdf file: invoice_10697.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      "Processing 2/160: invoice_10698.pdf\n",
      "Processing pdf file: invoice_10698.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.93)\n",
      "Processing 3/160: invoice_10699.pdf\n",
      "Processing pdf file: invoice_10699.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.92)\n",
      "Processing 4/160: invoice_10700.pdf\n",
      "Processing pdf file: invoice_10700.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      "Processing 5/160: invoice_10701.pdf\n",
      "Processing pdf file: invoice_10701.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.86)\n",
      "Processing 6/160: invoice_10702.pdf\n",
      "Processing pdf file: invoice_10702.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 7/160: invoice_10703.pdf\n",
      "Processing pdf file: invoice_10703.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      "Processing 8/160: invoice_10704.pdf\n",
      "Processing pdf file: invoice_10704.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 9/160: invoice_10705.pdf\n",
      "Processing pdf file: invoice_10705.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 10/160: invoice_10706.pdf\n",
      "Processing pdf file: invoice_10706.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.89)\n",
      "Processing 11/160: invoice_10707.pdf\n",
      "Processing pdf file: invoice_10707.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      "Processing 12/160: invoice_10708.pdf\n",
      "Processing pdf file: invoice_10708.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.88)\n",
      "Processing 13/160: invoice_10709.pdf\n",
      "Processing pdf file: invoice_10709.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      "Processing 14/160: invoice_10710.pdf\n",
      "Processing pdf file: invoice_10710.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      "Processing 15/160: invoice_10711.pdf\n",
      "Processing pdf file: invoice_10711.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.89)\n",
      "Processing 16/160: invoice_10712.pdf\n",
      "Processing pdf file: invoice_10712.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 17/160: invoice_10713.pdf\n",
      "Processing pdf file: invoice_10713.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      "Processing 18/160: invoice_10714.pdf\n",
      "Processing pdf file: invoice_10714.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      "Processing 19/160: invoice_10715.pdf\n",
      "Processing pdf file: invoice_10715.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.80)\n",
      "Processing 20/160: invoice_10716.pdf\n",
      "Processing pdf file: invoice_10716.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.90)\n",
      "Processing 21/160: invoice_10717.pdf\n",
      "Processing pdf file: invoice_10717.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.97)\n",
      "Processing 22/160: invoice_10718.pdf\n",
      "Processing pdf file: invoice_10718.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.78)\n",
      "Processing 23/160: invoice_10719.pdf\n",
      "Processing pdf file: invoice_10719.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.92)\n",
      "Processing 24/160: invoice_10720.pdf\n",
      "Processing pdf file: invoice_10720.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      "Processing 25/160: invoice_10721.pdf\n",
      "Processing pdf file: invoice_10721.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.88)\n",
      "Processing 26/160: invoice_10722.pdf\n",
      "Processing pdf file: invoice_10722.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 27/160: invoice_10723.pdf\n",
      "Processing pdf file: invoice_10723.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.96)\n",
      "Processing 28/160: invoice_10724.pdf\n",
      "Processing pdf file: invoice_10724.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      "Processing 29/160: invoice_10725.pdf\n",
      "Processing pdf file: invoice_10725.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      "Processing 30/160: invoice_10726.pdf\n",
      "Processing pdf file: invoice_10726.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 31/160: invoice_10727.pdf\n",
      "Processing pdf file: invoice_10727.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 32/160: invoice_10728.pdf\n",
      "Processing pdf file: invoice_10728.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.92)\n",
      "Processing 33/160: invoice_10729.pdf\n",
      "Processing pdf file: invoice_10729.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      "Processing 34/160: invoice_10730.pdf\n",
      "Processing pdf file: invoice_10730.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.92)\n",
      "Processing 35/160: invoice_10731.pdf\n",
      "Processing pdf file: invoice_10731.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 36/160: invoice_10732.pdf\n",
      "Processing pdf file: invoice_10732.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.92)\n",
      "Processing 37/160: invoice_10733.pdf\n",
      "Processing pdf file: invoice_10733.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      "Processing 38/160: invoice_10734.pdf\n",
      "Processing pdf file: invoice_10734.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.85)\n",
      "Processing 39/160: invoice_10735.pdf\n",
      "Processing pdf file: invoice_10735.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.87)\n",
      "Processing 40/160: invoice_10736.pdf\n",
      "Processing pdf file: invoice_10736.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.88)\n",
      "Processing 41/160: invoice_10737.pdf\n",
      "Processing pdf file: invoice_10737.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.82)\n",
      "Processing 42/160: invoice_10738.pdf\n",
      "Processing pdf file: invoice_10738.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.93)\n",
      "Processing 43/160: invoice_10739.pdf\n",
      "Processing pdf file: invoice_10739.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 44/160: invoice_10740.pdf\n",
      "Processing pdf file: invoice_10740.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      "Processing 45/160: invoice_10741.pdf\n",
      "Processing pdf file: invoice_10741.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.92)\n",
      "Processing 46/160: invoice_10742.pdf\n",
      "Processing pdf file: invoice_10742.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 47/160: invoice_10743.pdf\n",
      "Processing pdf file: invoice_10743.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 48/160: invoice_10744.pdf\n",
      "Processing pdf file: invoice_10744.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.85)\n",
      "Processing 49/160: invoice_10745.pdf\n",
      "Processing pdf file: invoice_10745.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.76)\n",
      "Processing 50/160: invoice_10746.pdf\n",
      "Processing pdf file: invoice_10746.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      "Processing 51/160: invoice_10747.pdf\n",
      "Processing pdf file: invoice_10747.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.89)\n",
      "Processing 52/160: invoice_10748.pdf\n",
      "Processing pdf file: invoice_10748.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.85)\n",
      "Processing 53/160: invoice_10749.pdf\n",
      "Processing pdf file: invoice_10749.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 54/160: invoice_10750.pdf\n",
      "Processing pdf file: invoice_10750.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.92)\n",
      "Processing 55/160: invoice_10751.pdf\n",
      "Processing pdf file: invoice_10751.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.91)\n",
      "Processing 56/160: invoice_10752.pdf\n",
      "Processing pdf file: invoice_10752.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      "Processing 57/160: invoice_10753.pdf\n",
      "Processing pdf file: invoice_10753.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      "Processing 58/160: invoice_10754.pdf\n",
      "Processing pdf file: invoice_10754.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.93)\n",
      "Processing 59/160: invoice_10755.pdf\n",
      "Processing pdf file: invoice_10755.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.81)\n",
      "Processing 60/160: invoice_10756.pdf\n",
      "Processing pdf file: invoice_10756.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 61/160: invoice_10757.pdf\n",
      "Processing pdf file: invoice_10757.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 62/160: invoice_10758.pdf\n",
      "Processing pdf file: invoice_10758.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      "Processing 63/160: invoice_10759.pdf\n",
      "Processing pdf file: invoice_10759.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 64/160: invoice_10760.pdf\n",
      "Processing pdf file: invoice_10760.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      "Processing 65/160: invoice_10761.pdf\n",
      "Processing pdf file: invoice_10761.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      "Processing 66/160: invoice_10762.pdf\n",
      "Processing pdf file: invoice_10762.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 67/160: invoice_10763.pdf\n",
      "Processing pdf file: invoice_10763.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 68/160: invoice_10764.pdf\n",
      "Processing pdf file: invoice_10764.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.86)\n",
      "Processing 69/160: invoice_10765.pdf\n",
      "Processing pdf file: invoice_10765.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      "Processing 70/160: invoice_10766.pdf\n",
      "Processing pdf file: invoice_10766.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      "Processing 71/160: invoice_10767.pdf\n",
      "Processing pdf file: invoice_10767.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.85)\n",
      "Processing 72/160: invoice_10768.pdf\n",
      "Processing pdf file: invoice_10768.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      "Processing 73/160: invoice_10769.pdf\n",
      "Processing pdf file: invoice_10769.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.82)\n",
      "Processing 74/160: invoice_10770.pdf\n",
      "Processing pdf file: invoice_10770.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.88)\n",
      "Processing 75/160: invoice_10771.pdf\n",
      "Processing pdf file: invoice_10771.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      "Processing 76/160: invoice_10772.pdf\n",
      "Processing pdf file: invoice_10772.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 1.00)\n",
      "Processing 77/160: invoice_10773.pdf\n",
      "Processing pdf file: invoice_10773.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.91)\n",
      "Processing 78/160: invoice_10774.pdf\n",
      "Processing pdf file: invoice_10774.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.79)\n",
      "Processing 79/160: invoice_10775.pdf\n",
      "Processing pdf file: invoice_10775.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.89)\n",
      "Processing 80/160: invoice_10776.pdf\n",
      "Processing pdf file: invoice_10776.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      "Processing 81/160: invoice_10777.pdf\n",
      "Processing pdf file: invoice_10777.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.92)\n",
      "Processing 82/160: invoice_10778.pdf\n",
      "Processing pdf file: invoice_10778.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.94)\n",
      "Processing 83/160: invoice_10779.pdf\n",
      "Processing pdf file: invoice_10779.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.82)\n",
      "Processing 84/160: invoice_10780.pdf\n",
      "Processing pdf file: invoice_10780.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 85/160: invoice_10781.pdf\n",
      "Processing pdf file: invoice_10781.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.87)\n",
      "Processing 86/160: invoice_10782.pdf\n",
      "Processing pdf file: invoice_10782.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.88)\n",
      "Processing 87/160: invoice_10783.pdf\n",
      "Processing pdf file: invoice_10783.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 88/160: invoice_10784.pdf\n",
      "Processing pdf file: invoice_10784.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.85)\n",
      "Processing 89/160: invoice_10785.pdf\n",
      "Processing pdf file: invoice_10785.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 90/160: invoice_10786.pdf\n",
      "Processing pdf file: invoice_10786.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.89)\n",
      "Processing 91/160: invoice_10787.pdf\n",
      "Processing pdf file: invoice_10787.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      "Processing 92/160: invoice_10788.pdf\n",
      "Processing pdf file: invoice_10788.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.87)\n",
      "Processing 93/160: invoice_10789.pdf\n",
      "Processing pdf file: invoice_10789.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.87)\n",
      "Processing 94/160: invoice_10790.pdf\n",
      "Processing pdf file: invoice_10790.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      "Processing 95/160: invoice_10791.pdf\n",
      "Processing pdf file: invoice_10791.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.87)\n",
      "Processing 96/160: invoice_10792.pdf\n",
      "Processing pdf file: invoice_10792.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.96)\n",
      "Processing 97/160: invoice_10793.pdf\n",
      "Processing pdf file: invoice_10793.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.86)\n",
      "Processing 98/160: invoice_10794.pdf\n",
      "Processing pdf file: invoice_10794.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.90)\n",
      "Processing 99/160: invoice_10795.pdf\n",
      "Processing pdf file: invoice_10795.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.87)\n",
      "Processing 100/160: invoice_10796.pdf\n",
      "Processing pdf file: invoice_10796.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.93)\n",
      "Processing 101/160: invoice_10797.pdf\n",
      "Processing pdf file: invoice_10797.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.93)\n",
      "Processing 102/160: invoice_10798.pdf\n",
      "Processing pdf file: invoice_10798.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "Processing 103/160: invoice_10799.pdf\n",
      "Processing pdf file: invoice_10799.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      "Processing 104/160: invoice_10800.pdf\n",
      "Processing pdf file: invoice_10800.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.89)\n",
      "Processing 105/160: Invoice_252-27878353-TI-1.pdf\n",
      "Processing pdf file: Invoice_252-27878353-TI-1.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P2' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.94)\n",
      "Processing 106/160: Rechnung 412955.pdf\n",
      "Processing pdf file: Rechnung 412955.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.90)\n",
      "Processing 107/160: Rechnung_1407606058.pdf\n",
      "Processing pdf file: Rechnung_1407606058.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      "Processing 108/160: Strom Rechnung_October.pdf\n",
      "Processing pdf file: Strom Rechnung_October.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      "Processing 109/160: SWME_Rechnung_07122020.pdf\n",
      "Processing pdf file: SWME_Rechnung_07122020.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.85)\n",
      "Processing 110/160: batch1-0016.jpg\n",
      "Processing image file: batch1-0016.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.92)\n",
      "Processing 111/160: batch1-0017.jpg\n",
      "Processing image file: batch1-0017.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 112/160: batch1-0018.jpg\n",
      "Processing image file: batch1-0018.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 113/160: batch1-0019.jpg\n",
      "Processing image file: batch1-0019.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 1.00)\n",
      "Processing 114/160: batch1-0020.jpg\n",
      "Processing image file: batch1-0020.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 115/160: batch1-0021.jpg\n",
      "Processing image file: batch1-0021.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 116/160: batch1-0022.jpg\n",
      "Processing image file: batch1-0022.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.60)\n",
      "Processing 117/160: batch1-0023.jpg\n",
      "Processing image file: batch1-0023.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 118/160: batch1-0024.jpg\n",
      "Processing image file: batch1-0024.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 119/160: batch1-0025.jpg\n",
      "Processing image file: batch1-0025.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 120/160: batch1-0026.jpg\n",
      "Processing image file: batch1-0026.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 121/160: batch1-0027.jpg\n",
      "Processing image file: batch1-0027.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 122/160: batch1-0028.jpg\n",
      "Processing image file: batch1-0028.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.89)\n",
      "Processing 123/160: batch1-0029.jpg\n",
      "Processing image file: batch1-0029.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 124/160: batch1-0030.jpg\n",
      "Processing image file: batch1-0030.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.92)\n",
      "Processing 125/160: batch1-0031.jpg\n",
      "Processing image file: batch1-0031.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 126/160: batch1-0032.jpg\n",
      "Processing image file: batch1-0032.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 127/160: batch1-0033.jpg\n",
      "Processing image file: batch1-0033.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 128/160: batch1-0034.jpg\n",
      "Processing image file: batch1-0034.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 129/160: batch1-0035.jpg\n",
      "Processing image file: batch1-0035.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 130/160: batch1-0036.jpg\n",
      "Processing image file: batch1-0036.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 131/160: batch1-0037.jpg\n",
      "Processing image file: batch1-0037.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 132/160: batch1-0038.jpg\n",
      "Processing image file: batch1-0038.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 133/160: batch1-0039.jpg\n",
      "Processing image file: batch1-0039.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 134/160: batch1-0040.jpg\n",
      "Processing image file: batch1-0040.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 135/160: batch1-0041.jpg\n",
      "Processing image file: batch1-0041.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 136/160: batch1-0042.jpg\n",
      "Processing image file: batch1-0042.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 137/160: batch1-0043.jpg\n",
      "Processing image file: batch1-0043.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 138/160: batch1-0044.jpg\n",
      "Processing image file: batch1-0044.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 139/160: batch1-0045.jpg\n",
      "Processing image file: batch1-0045.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 140/160: batch1-0046.jpg\n",
      "Processing image file: batch1-0046.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 141/160: batch1-0047.jpg\n",
      "Processing image file: batch1-0047.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 142/160: batch1-0048.jpg\n",
      "Processing image file: batch1-0048.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 143/160: batch1-0049.jpg\n",
      "Processing image file: batch1-0049.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 144/160: batch1-0050.jpg\n",
      "Processing image file: batch1-0050.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 145/160: batch1-0051.jpg\n",
      "Processing image file: batch1-0051.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 146/160: batch1-0052.jpg\n",
      "Processing image file: batch1-0052.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 1.00)\n",
      "Processing 147/160: batch1-0053.jpg\n",
      "Processing image file: batch1-0053.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 148/160: batch1-0054.jpg\n",
      "Processing image file: batch1-0054.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 149/160: batch1-0055.jpg\n",
      "Processing image file: batch1-0055.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 150/160: batch1-0056.jpg\n",
      "Processing image file: batch1-0056.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 151/160: batch1-0057.jpg\n",
      "Processing image file: batch1-0057.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 152/160: batch1-0058.jpg\n",
      "Processing image file: batch1-0058.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "Processing 153/160: batch1-0059.jpg\n",
      "Processing image file: batch1-0059.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 154/160: batch1-0060.jpg\n",
      "Processing image file: batch1-0060.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 155/160: batch1-0447.jpg\n",
      "Processing image file: batch1-0447.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌Insufficient validation - skipped\n",
      "Processing 156/160: Rechnung 412955_page-0001.jpg\n",
      "Processing image file: Rechnung 412955_page-0001.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      "\n",
      " Smart ground truth created for 130 files\n",
      "\n",
      " Step 2: Running automated evaluation...\n",
      "Processing: invoice_10697.pdf\n",
      "Processing pdf file: invoice_10697.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10698.pdf\n",
      "Processing pdf file: invoice_10698.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10699.pdf\n",
      "Processing pdf file: invoice_10699.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.8%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10700.pdf\n",
      "Processing pdf file: invoice_10700.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10701.pdf\n",
      "Processing pdf file: invoice_10701.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10702.pdf\n",
      "Processing pdf file: invoice_10702.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10703.pdf\n",
      "Processing pdf file: invoice_10703.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10704.pdf\n",
      "Processing pdf file: invoice_10704.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10705.pdf\n",
      "Processing pdf file: invoice_10705.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10706.pdf\n",
      "Processing pdf file: invoice_10706.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10707.pdf\n",
      "Processing pdf file: invoice_10707.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10708.pdf\n",
      "Processing pdf file: invoice_10708.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10709.pdf\n",
      "Processing pdf file: invoice_10709.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10710.pdf\n",
      "Processing pdf file: invoice_10710.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.8%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10711.pdf\n",
      "Processing pdf file: invoice_10711.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10712.pdf\n",
      "Processing pdf file: invoice_10712.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10713.pdf\n",
      "Processing pdf file: invoice_10713.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10714.pdf\n",
      "Processing pdf file: invoice_10714.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10715.pdf\n",
      "Processing pdf file: invoice_10715.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.8%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10716.pdf\n",
      "Processing pdf file: invoice_10716.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10717.pdf\n",
      "Processing pdf file: invoice_10717.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10718.pdf\n",
      "Processing pdf file: invoice_10718.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 41.9%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10719.pdf\n",
      "Processing pdf file: invoice_10719.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10720.pdf\n",
      "Processing pdf file: invoice_10720.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10721.pdf\n",
      "Processing pdf file: invoice_10721.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10722.pdf\n",
      "Processing pdf file: invoice_10722.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10723.pdf\n",
      "Processing pdf file: invoice_10723.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10724.pdf\n",
      "Processing pdf file: invoice_10724.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10725.pdf\n",
      "Processing pdf file: invoice_10725.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 38.7%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10726.pdf\n",
      "Processing pdf file: invoice_10726.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10727.pdf\n",
      "Processing pdf file: invoice_10727.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10728.pdf\n",
      "Processing pdf file: invoice_10728.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10729.pdf\n",
      "Processing pdf file: invoice_10729.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10730.pdf\n",
      "Processing pdf file: invoice_10730.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10731.pdf\n",
      "Processing pdf file: invoice_10731.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10732.pdf\n",
      "Processing pdf file: invoice_10732.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 19.4%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10733.pdf\n",
      "Processing pdf file: invoice_10733.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 38.7%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10734.pdf\n",
      "Processing pdf file: invoice_10734.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 38.7%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10735.pdf\n",
      "Processing pdf file: invoice_10735.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10736.pdf\n",
      "Processing pdf file: invoice_10736.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10737.pdf\n",
      "Processing pdf file: invoice_10737.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.8%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10738.pdf\n",
      "Processing pdf file: invoice_10738.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10739.pdf\n",
      "Processing pdf file: invoice_10739.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10740.pdf\n",
      "Processing pdf file: invoice_10740.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10741.pdf\n",
      "Processing pdf file: invoice_10741.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10742.pdf\n",
      "Processing pdf file: invoice_10742.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10743.pdf\n",
      "Processing pdf file: invoice_10743.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10744.pdf\n",
      "Processing pdf file: invoice_10744.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.8%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10745.pdf\n",
      "Processing pdf file: invoice_10745.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10746.pdf\n",
      "Processing pdf file: invoice_10746.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10747.pdf\n",
      "Processing pdf file: invoice_10747.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10748.pdf\n",
      "Processing pdf file: invoice_10748.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10749.pdf\n",
      "Processing pdf file: invoice_10749.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10750.pdf\n",
      "Processing pdf file: invoice_10750.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 41.9%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10751.pdf\n",
      "Processing pdf file: invoice_10751.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 38.7%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10752.pdf\n",
      "Processing pdf file: invoice_10752.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10753.pdf\n",
      "Processing pdf file: invoice_10753.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10754.pdf\n",
      "Processing pdf file: invoice_10754.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10755.pdf\n",
      "Processing pdf file: invoice_10755.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10756.pdf\n",
      "Processing pdf file: invoice_10756.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10757.pdf\n",
      "Processing pdf file: invoice_10757.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10758.pdf\n",
      "Processing pdf file: invoice_10758.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10759.pdf\n",
      "Processing pdf file: invoice_10759.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10760.pdf\n",
      "Processing pdf file: invoice_10760.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10761.pdf\n",
      "Processing pdf file: invoice_10761.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10762.pdf\n",
      "Processing pdf file: invoice_10762.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10763.pdf\n",
      "Processing pdf file: invoice_10763.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10764.pdf\n",
      "Processing pdf file: invoice_10764.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10765.pdf\n",
      "Processing pdf file: invoice_10765.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10766.pdf\n",
      "Processing pdf file: invoice_10766.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10767.pdf\n",
      "Processing pdf file: invoice_10767.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 19.4%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10768.pdf\n",
      "Processing pdf file: invoice_10768.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10769.pdf\n",
      "Processing pdf file: invoice_10769.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10770.pdf\n",
      "Processing pdf file: invoice_10770.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10771.pdf\n",
      "Processing pdf file: invoice_10771.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10772.pdf\n",
      "Processing pdf file: invoice_10772.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10773.pdf\n",
      "Processing pdf file: invoice_10773.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 38.7%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10774.pdf\n",
      "Processing pdf file: invoice_10774.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10775.pdf\n",
      "Processing pdf file: invoice_10775.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10776.pdf\n",
      "Processing pdf file: invoice_10776.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10777.pdf\n",
      "Processing pdf file: invoice_10777.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 19.4%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10778.pdf\n",
      "Processing pdf file: invoice_10778.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.8%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10779.pdf\n",
      "Processing pdf file: invoice_10779.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10780.pdf\n",
      "Processing pdf file: invoice_10780.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10781.pdf\n",
      "Processing pdf file: invoice_10781.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 45.2%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10782.pdf\n",
      "Processing pdf file: invoice_10782.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.8%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10783.pdf\n",
      "Processing pdf file: invoice_10783.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10784.pdf\n",
      "Processing pdf file: invoice_10784.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10785.pdf\n",
      "Processing pdf file: invoice_10785.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10786.pdf\n",
      "Processing pdf file: invoice_10786.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10787.pdf\n",
      "Processing pdf file: invoice_10787.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.8%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10788.pdf\n",
      "Processing pdf file: invoice_10788.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.8%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10789.pdf\n",
      "Processing pdf file: invoice_10789.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10790.pdf\n",
      "Processing pdf file: invoice_10790.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 38.7%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10791.pdf\n",
      "Processing pdf file: invoice_10791.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 29.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10792.pdf\n",
      "Processing pdf file: invoice_10792.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10793.pdf\n",
      "Processing pdf file: invoice_10793.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.8%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10794.pdf\n",
      "Processing pdf file: invoice_10794.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10795.pdf\n",
      "Processing pdf file: invoice_10795.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10796.pdf\n",
      "Processing pdf file: invoice_10796.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10797.pdf\n",
      "Processing pdf file: invoice_10797.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 38.7%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10798.pdf\n",
      "Processing pdf file: invoice_10798.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10799.pdf\n",
      "Processing pdf file: invoice_10799.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: invoice_10800.pdf\n",
      "Processing pdf file: invoice_10800.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 32.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: Invoice_252-27878353-TI-1.pdf\n",
      "Processing pdf file: Invoice_252-27878353-TI-1.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P2' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 77.4%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: Rechnung 412955.pdf\n",
      "Processing pdf file: Rechnung 412955.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 74.2%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: Rechnung_1407606058.pdf\n",
      "Processing pdf file: Rechnung_1407606058.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 64.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: Strom Rechnung_October.pdf\n",
      "Processing pdf file: Strom Rechnung_October.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 58.1%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: SWME_Rechnung_07122020.pdf\n",
      "Processing pdf file: SWME_Rechnung_07122020.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 61.3%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0016.jpg\n",
      "Processing image file: batch1-0016.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 45.2%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0017.jpg\n",
      "Processing image file: batch1-0017.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0018.jpg\n",
      "Processing image file: batch1-0018.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 37.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0019.jpg\n",
      "Processing image file: batch1-0019.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 37.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0020.jpg\n",
      "Processing image file: batch1-0020.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "Processing: batch1-0021.jpg\n",
      "Processing image file: batch1-0021.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "Processing: batch1-0022.jpg\n",
      "Processing image file: batch1-0022.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0023.jpg\n",
      "Processing image file: batch1-0023.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 37.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0024.jpg\n",
      "Processing image file: batch1-0024.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 12.5%\n",
      "Processing: batch1-0025.jpg\n",
      "Processing image file: batch1-0025.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 12.5%\n",
      "Processing: batch1-0026.jpg\n",
      "Processing image file: batch1-0026.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 37.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0027.jpg\n",
      "Processing image file: batch1-0027.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "Processing: batch1-0028.jpg\n",
      "Processing image file: batch1-0028.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 35.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0029.jpg\n",
      "Processing image file: batch1-0029.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 12.5%\n",
      "Processing: batch1-0030.jpg\n",
      "Processing image file: batch1-0030.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 41.9%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0031.jpg\n",
      "Processing image file: batch1-0031.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 12.5%\n",
      "Processing: batch1-0032.jpg\n",
      "Processing image file: batch1-0032.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0033.jpg\n",
      "Processing image file: batch1-0033.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 12.5%\n",
      "Processing: batch1-0034.jpg\n",
      "Processing image file: batch1-0034.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 12.5%\n",
      "Processing: batch1-0035.jpg\n",
      "Processing image file: batch1-0035.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "Processing: batch1-0036.jpg\n",
      "Processing image file: batch1-0036.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 37.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0037.jpg\n",
      "Processing image file: batch1-0037.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0038.jpg\n",
      "Processing image file: batch1-0038.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 37.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0039.jpg\n",
      "Processing image file: batch1-0039.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 12.5%\n",
      "Processing: batch1-0040.jpg\n",
      "Processing image file: batch1-0040.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "Processing: batch1-0041.jpg\n",
      "Processing image file: batch1-0041.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "Processing: batch1-0042.jpg\n",
      "Processing image file: batch1-0042.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 12.5%\n",
      "Processing: batch1-0043.jpg\n",
      "Processing image file: batch1-0043.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0044.jpg\n",
      "Processing image file: batch1-0044.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 12.5%\n",
      "Processing: batch1-0045.jpg\n",
      "Processing image file: batch1-0045.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 37.5%\n",
      "✅ Accuracy: 100.0%\n",
      "Processing: batch1-0046.jpg\n",
      "Processing image file: batch1-0046.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "Processing: batch1-0047.jpg\n",
      "Processing image file: batch1-0047.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success rate: 25.0%\n",
      "Processing: batch1-0048.jpg\n",
      "Processing image file: batch1-0048.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 208\u001b[39m\n\u001b[32m    205\u001b[39m smart_evaluator = SmartAutomatedEvaluator()\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# ONE-LINE FULLY AUTOMATED EVALUATION\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m automation_report, ground_truth, evaluation_results = \u001b[43msmart_evaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_fully_automated_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ground_truth:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mZERO-INTERVENTION EVALUATION COMPLETE!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mSmartAutomatedEvaluator.run_fully_automated_evaluation\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Step 3: Run evaluation with smart ground truth\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Step 2: Running automated evaluation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_evaluate_invoices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./large_scale_invoice_dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mground_truth_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msmart_auto_ground_truth.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    165\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Step 4: Generate comprehensive report\u001b[39;00m\n\u001b[32m    168\u001b[39m automation_report = {\n\u001b[32m    169\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpipeline_completed_at\u001b[39m\u001b[33m\"\u001b[39m: datetime.now().isoformat(),\n\u001b[32m    170\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msmart_ground_truth\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m    179\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mautomation_level\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfull_zero_intervention\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 175\u001b[39m, in \u001b[36mModelEvaluator.batch_evaluate_invoices\u001b[39m\u001b[34m(self, invoice_folder, ground_truth_file)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvoice_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# Extract using your existing pipeline\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     result = \u001b[43menhanced_load_and_normalize_with_qa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minvoice_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     \u001b[38;5;66;03m# Get ground truth for this file\u001b[39;00m\n\u001b[32m    178\u001b[39m     gt = ground_truth_data.get(invoice_file.name, {})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36menhanced_load_and_normalize_with_qa\u001b[39m\u001b[34m(file_path, table_name, custom_questions)\u001b[39m\n\u001b[32m      8\u001b[39m     document_data = load_and_normalize(file_path, table_name)\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# QA-based extraction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     enhanced_data = \u001b[43mprocess_document_with_qa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_questions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Takes the normalized document\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03mRuns AI question-answering\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03mAdds intelligent extraction results\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03mReturns enhanced document with AI insights\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m enhanced_data\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mprocess_document_with_qa\u001b[39m\u001b[34m(document_data, custom_questions)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Takes your normalized document (which has text split by pages/sections)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03mCombines everything into one big text string\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03mThis gives the AI the full context to answer questions\"\"\"\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Extract information using QA\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     extraction_results = \u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_information\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_questions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_questions\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Passes the combined text to your AI extractor\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03mUses custom questions if provided (like your German questions)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03mGets back structured answers with confidence scores\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# extraction results to document data\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 240\u001b[39m, in \u001b[36mQABasedExtractor.extract_information\u001b[39m\u001b[34m(self, text, document_type, custom_questions)\u001b[39m\n\u001b[32m    237\u001b[39m     questions = \u001b[38;5;28mself\u001b[39m.extraction_templates.get(document_type, \u001b[38;5;28mself\u001b[39m.extraction_templates[\u001b[33m'\u001b[39m\u001b[33mgeneral\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# Extract with QA\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m qa_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_with_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Extract entities with NER if available\u001b[39;00m\n\u001b[32m    243\u001b[39m ner_results = \u001b[38;5;28mself\u001b[39m.extract_entities_with_ner(text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 188\u001b[39m, in \u001b[36mQABasedExtractor.extract_with_questions\u001b[39m\u001b[34m(self, text, questions, confidence_threshold)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m         qa_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqa_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_answer_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Longer answers\u001b[39;49;00m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m         results[question] = {\n\u001b[32m    195\u001b[39m             \u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m: qa_result[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m qa_result[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m] >= confidence_threshold \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    196\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m'\u001b[39m: qa_result[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mextracted\u001b[39m\u001b[33m'\u001b[39m: qa_result[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m] >= confidence_threshold\n\u001b[32m    200\u001b[39m         }\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:402\u001b[39m, in \u001b[36mQuestionAnsweringPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    400\u001b[39m examples = \u001b[38;5;28mself\u001b[39m._args_parser(*args, **kwargs)\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(examples, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\pipelines\\base.py:1459\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1457\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[32m-> \u001b[39m\u001b[32m1459\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:126\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:271\u001b[39m, in \u001b[36mPipelinePackIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    268\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    273\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:529\u001b[39m, in \u001b[36mQuestionAnsweringPipeline._forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(model_forward).parameters:\n\u001b[32m    528\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m529\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m\"\u001b[39m: output[\u001b[33m\"\u001b[39m\u001b[33mstart_logits\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m: output[\u001b[33m\"\u001b[39m\u001b[33mend_logits\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mexample\u001b[39m\u001b[33m\"\u001b[39m: example, **inputs}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\peft\\peft_model.py:2716\u001b[39m, in \u001b[36mPeftModelForQuestionAnswering.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   2714\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   2715\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m2716\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2717\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2718\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2719\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstart_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m            \u001b[49m\u001b[43mend_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2728\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   2729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2730\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:222\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:1486\u001b[39m, in \u001b[36mXLMRobertaForQuestionAnswering.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1473\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1474\u001b[39m \u001b[33;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1475\u001b[39m \u001b[33;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1482\u001b[39m \u001b[33;03m    [What are token type IDs?](../glossary#token-type-ids)\u001b[39;00m\n\u001b[32m   1483\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1484\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1498\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1500\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.qa_outputs(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:852\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    850\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m852\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    866\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:606\u001b[39m, in \u001b[36mXLMRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    602\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    604\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:543\u001b[39m, in \u001b[36mXLMRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    540\u001b[39m     attention_output = cross_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    541\u001b[39m     outputs = outputs + cross_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\pytorch_utils.py:257\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:552\u001b[39m, in \u001b[36mXLMRobertaLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    551\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:479\u001b[39m, in \u001b[36mXLMRobertaOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    481\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aslia\\anaconda3\\envs\\selflearn\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Step 4B: Smart Automated Batch Evaluation\n",
    "\n",
    "class SmartAutomatedEvaluator:\n",
    "    \"\"\"Combines smart validation with full automation for zero human intervention\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluator = ModelEvaluator()\n",
    "        self.confidence_patterns = {\n",
    "            \"invoice_number\": [r\"[A-Z0-9\\-]{5,15}\", r\"\\d{6,12}\"],\n",
    "            \"total_amount\": [r\"[\\d,\\.]+\\s*€\", r\"€\\s*[\\d,\\.]+\", r\"\\d+[,\\.]\\d{2}\"],\n",
    "            \"company_name\": [r\"[A-Z][a-zA-Z\\s&,\\.]{3,30}\"],\n",
    "            \"invoice_date\": [r\"\\d{1,2}[./\\-]\\d{1,2}[./\\-]\\d{2,4}\"]\n",
    "        }\n",
    "    \n",
    "    def create_smart_ground_truth(self, invoice_folder=\"./large_scale_invoice_dataset\"):\n",
    "        \"\"\"Creates validated ground truth automatically\"\"\"\n",
    "        \n",
    "        smart_gt = {}\n",
    "        validation_scores = {}\n",
    "        files_processed = 0  # Add counter\n",
    "        max_files = 160\n",
    "    \n",
    "        # Process each invoice with multiple validation layers\n",
    "        for pattern in [\"*.pdf\", \"*.png\", \"*.jpg\", \"*.jpeg\"]:\n",
    "            for invoice_file in Path(invoice_folder).glob(pattern):\n",
    "                if files_processed >= max_files:  # Add limit\n",
    "                    print(f\"Reached maximum file limit ({max_files}), stopping...\")\n",
    "                    break\n",
    "     \n",
    "                try:\n",
    "                    print(f\"Processing {files_processed + 1}/{max_files}: {invoice_file.name}\")\n",
    "                    files_processed += 1  # Increment counter\n",
    "    \n",
    "                    # Extract with existing system\n",
    "                    result = enhanced_load_and_normalize_with_qa(str(invoice_file))\n",
    "                    extractions = result['qa_extraction']['extractions']\n",
    "                    \n",
    "                    # Smart validation for each extraction\n",
    "                    validated_data = {}\n",
    "                    file_validation_score = 0\n",
    "                    total_validations = 0\n",
    "                    \n",
    "                    for question, answer_data in extractions.items():\n",
    "                        confidence = answer_data.get('confidence', 0)\n",
    "                        answer = answer_data.get('answer', '')\n",
    "                        \n",
    "                        # null checking\n",
    "                        if answer is None:\n",
    "                            answer = ''\n",
    "                        else:\n",
    "                            answer = str(answer).strip()\n",
    "                        \n",
    "                        if not answer or len(answer) < 2:\n",
    "                            continue\n",
    "                        \n",
    "                        # Multi-criteria validation\n",
    "                        validation_score = 0\n",
    "                        field = self._map_question_to_field(question)\n",
    "                        \n",
    "                        # Confidence threshold\n",
    "                        if confidence >= 0.3:  # Lowered threshold for more data\n",
    "                            validation_score += 0.3\n",
    "                        \n",
    "                        # Pattern matching\n",
    "                        if field and self._validate_pattern(field, answer):\n",
    "                            validation_score += 0.4\n",
    "                        \n",
    "                        # Length and format check\n",
    "                        if 2 <= len(answer) <= 50 and not answer.isspace():\n",
    "                            validation_score += 0.2\n",
    "                        \n",
    "                        # Cross-reference check\n",
    "                        if self._cross_validate_answer(answer, extractions, field):\n",
    "                            validation_score += 0.1\n",
    "                        \n",
    "                        # Accept if validation score >= 0.6\n",
    "                        if validation_score >= 0.6 and field:\n",
    "                            validated_data[field] = answer\n",
    "                            file_validation_score += validation_score\n",
    "                            total_validations += 1\n",
    "                    \n",
    "                    # Only include files with sufficient validated extractions\n",
    "                    if len(validated_data) >= 1:  # Lowered threshold\n",
    "                        smart_gt[invoice_file.name] = validated_data\n",
    "                        validation_scores[invoice_file.name] = file_validation_score / total_validations if total_validations > 0 else 0\n",
    "                        print(f\"   ✅ Validated {len(validated_data)} fields (score: {validation_scores[invoice_file.name]:.2f})\")\n",
    "                    else:\n",
    "                        print(f\"   ❌Insufficient validation - skipped\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Error: {e}\")\n",
    "                    \n",
    "            if files_processed >= max_files:  # Break outer loop too\n",
    "                break\n",
    "        \n",
    "        # Save smart ground truth\n",
    "        with open(\"smart_auto_ground_truth.json\", \"w\") as f:\n",
    "            json.dump(smart_gt, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n Smart ground truth created for {len(smart_gt)} files\")\n",
    "        return smart_gt\n",
    "    \n",
    "    def _validate_pattern(self, field, value):\n",
    "        \"\"\"Validate using regex patterns with null checking\"\"\"\n",
    "        if not value or field not in self.confidence_patterns:\n",
    "            return True\n",
    "        \n",
    "        patterns = self.confidence_patterns[field]\n",
    "        return any(re.search(pattern, str(value), re.IGNORECASE) for pattern in patterns)\n",
    "    \n",
    "    def _cross_validate_answer(self, answer, all_extractions, field):\n",
    "        \"\"\"Cross-validate with other answers\"\"\"\n",
    "        if not answer:\n",
    "            return False\n",
    "            \n",
    "        similar_count = 0\n",
    "        for q, data in all_extractions.items():\n",
    "            other_answer = data.get('answer', '')\n",
    "            if other_answer and self._similarity_score(answer, other_answer) > 0.7:\n",
    "                similar_count += 1\n",
    "        return similar_count >= 1\n",
    "    \n",
    "    def _similarity_score(self, text1, text2):\n",
    "        \"\"\"Calculate similarity between two text strings\"\"\"\n",
    "        if not text1 or not text2:\n",
    "            return 0\n",
    "        return SequenceMatcher(None, str(text1).lower(), str(text2).lower()).ratio()\n",
    "    \n",
    "    def _map_question_to_field(self, question):\n",
    "        \"\"\"Map questions to standard ground truth fields\"\"\"\n",
    "        if not question:\n",
    "            return None\n",
    "            \n",
    "        q_lower = question.lower()\n",
    "        if any(kw in q_lower for kw in [\"rechnungsnummer\", \"invoice number\", \"nummer\"]):\n",
    "            return \"invoice_number\"\n",
    "        elif any(kw in q_lower for kw in [\"gesamtbetrag\", \"total\", \"amount\", \"endbetrag\"]):\n",
    "            return \"total_amount\"\n",
    "        elif any(kw in q_lower for kw in [\"company\", \"firma\", \"vendor\", \"firmenname\"]):\n",
    "            return \"company_name\"\n",
    "        elif any(kw in q_lower for kw in [\"datum\", \"date\", \"rechnungsdatum\"]):\n",
    "            return \"invoice_date\"\n",
    "        return None\n",
    "    \n",
    "    def run_fully_automated_evaluation(self):\n",
    "        \"\"\"Complete automated pipeline with zero human intervention\"\"\"\n",
    "        \n",
    "        print(\" STARTING FULLY AUTOMATED EVALUATION PIPELINE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Create smart ground truth\n",
    "        print(\"\\n Step 1: Creating smart ground truth...\")\n",
    "        smart_gt = self.create_smart_ground_truth()\n",
    "        \n",
    "        # Step 2: Only proceed if we have some validated data\n",
    "        if not smart_gt:\n",
    "            print(\"❌ No valid ground truth data created - check your invoice files\")\n",
    "            return {}, {}, []\n",
    "        \n",
    "        # Step 3: Run evaluation with smart ground truth\n",
    "        print(\"\\n Step 2: Running automated evaluation...\")\n",
    "        results = self.evaluator.batch_evaluate_invoices(\n",
    "            \"./large_scale_invoice_dataset\",\n",
    "            ground_truth_file=\"smart_auto_ground_truth.json\"\n",
    "        )\n",
    "        \n",
    "        # Step 4: Generate comprehensive report\n",
    "        automation_report = {\n",
    "            \"pipeline_completed_at\": datetime.now().isoformat(),\n",
    "            \"smart_ground_truth\": {\n",
    "                \"files_validated\": len(smart_gt),\n",
    "                \"validation_method\": \"multi_criteria_smart_validation\"\n",
    "            },\n",
    "            \"evaluation_results\": {\n",
    "                \"total_processed\": len(results),\n",
    "                \"avg_success_rate\": sum(r['success_rate'] for r in results) / len(results) if results else 0,\n",
    "                \"avg_confidence\": sum(r['avg_confidence'] for r in results) / len(results) if results else 0\n",
    "            },\n",
    "            \"automation_level\": \"full_zero_intervention\"\n",
    "        }\n",
    "        \n",
    "        with open(\"automation_report.json\", \"w\") as f:\n",
    "            json.dump(automation_report, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n FULLY AUTOMATED EVALUATION COMPLETE!\")\n",
    "        print(f\"   Ground truth files: {len(smart_gt)}\")\n",
    "        print(f\"   Evaluation results: {len(results)} invoices\")\n",
    "        if results:\n",
    "            print(f\" Average success rate: {automation_report['evaluation_results']['avg_success_rate']:.1%}\")\n",
    "        \n",
    "        return automation_report, smart_gt, results\n",
    "\n",
    "# Initialize and run the smart automated evaluator with error handling\n",
    "if os.path.exists(\"./large_scale_invoice_dataset\"):\n",
    "    pdf_files = list(Path(\"./large_scale_invoice_dataset\").glob(\"*.pdf\"))\n",
    "    image_files = list(Path(\"./large_scale_invoice_dataset\").glob(\"*.png\")) + list(Path(\"./large_scale_invoice_dataset\").glob(\"*.jpg\"))\n",
    "    \n",
    "    total_files = len(pdf_files) + len(image_files)\n",
    "    \n",
    "    if total_files > 0:\n",
    "        print(f\" Running SMART AUTOMATED evaluation on {total_files} files...\")\n",
    "        print(f\"    PDFs: {len(pdf_files)}\")\n",
    "        print(f\"   Images: {len(image_files)}\")\n",
    "        \n",
    "        smart_evaluator = SmartAutomatedEvaluator()\n",
    "        \n",
    "        # ONE-LINE FULLY AUTOMATED EVALUATION\n",
    "        automation_report, ground_truth, evaluation_results = smart_evaluator.run_fully_automated_evaluation()\n",
    "        \n",
    "        if ground_truth:\n",
    "            print(\"ZERO-INTERVENTION EVALUATION COMPLETE!\")\n",
    "        else:\n",
    "            print(\" Evaluation completed but no valid ground truth generated\")\n",
    "    else:\n",
    "        print(\"❌ No supported files (PDF/PNG/JPG) found in ./large_scale_invoice_dataset\")\n",
    "else:\n",
    "    print(\"❌ ./large_scale_invoice_dataset folder not found\")\n",
    "    print(\"   Please ensure Step 4A copied the files correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9148c",
   "metadata": {},
   "source": [
    "**IMPORTANT** 100% accuracy ABOVE is artificial because the system created its own \"answer key\" and then compared against it. but the success rates (67-100%) are the real performance indicators showing how many questions the AI could confidently answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10994fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Selected 4 files for feedback:\n",
      "   • batch1-0024.jpg\n",
      "   • batch1-0025.jpg\n",
      "   • batch1-0029.jpg\n",
      "   • batch1-0031.jpg\n"
     ]
    }
   ],
   "source": [
    "# AUTO-SELECT files with lowest performance from recent evaluation\n",
    "def get_low_performance_files(min_files=3, max_files=5):\n",
    "    \"\"\"Automatically select files that need feedback based on performance\"\"\"\n",
    "    \n",
    "    # Check if we have evaluation results\n",
    "    if os.path.exists(\"model_metrics.json\"):\n",
    "        with open(\"model_metrics.json\", 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        # Get recent evaluations and sort by success rate\n",
    "        evaluations = metrics.get(\"evaluations\", [])\n",
    "        if evaluations:\n",
    "            # Sort by success rate lowest first\n",
    "            sorted_evals = sorted(evaluations, key=lambda x: x.get('success_rate', 0))\n",
    "            \n",
    "            # Get lowest performing files\n",
    "            low_perf_files = []\n",
    "            for eval_data in sorted_evals[:max_files]:\n",
    "                filename = eval_data.get('document_name', '')\n",
    "                if filename and os.path.exists(f\"./large_scale_invoice_dataset/{filename}\"):\n",
    "                    low_perf_files.append(filename)\n",
    "            \n",
    "            if len(low_perf_files) >= min_files:\n",
    "                return low_perf_files[:max_files]\n",
    "    \n",
    "    # Fallback: get any available files\n",
    "    available_files = []\n",
    "    if os.path.exists(\"./large_scale_invoice_dataset\"):\n",
    "        for file_ext in [\"*.pdf\", \"*.png\", \"*.jpg\", \"*.jpeg\"]:\n",
    "            available_files.extend([f.name for f in Path(\"./large_scale_invoice_dataset\").glob(file_ext)])\n",
    "    \n",
    "    return available_files[:max_files] if available_files else []\n",
    "\n",
    "# Get files dynamically\n",
    "low_performance_files = get_low_performance_files(min_files=2, max_files=4)\n",
    "\n",
    "if not low_performance_files:\n",
    "    print(\"❌ No files found for feedback collection!\")\n",
    "    print(\" Make sure you have files in ./large_scale_invoice_dataset/ or run evaluation first\")\n",
    "else:\n",
    "    print(f\" Selected {len(low_performance_files)} files for feedback:\")\n",
    "    for filename in low_performance_files:\n",
    "        print(f\"   • {filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda7c7b",
   "metadata": {},
   "source": [
    "**5. Scalability and Adaptability**\n",
    "* Ensure the system scales across multiple domains (finance, healthcare, legal, etc.) and data types.\n",
    "* Support plug-and-play modularity to integrate new extraction modules or data sources easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8034dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Domain Template Manager initialized with 6 domains and 15+ document types\n"
     ]
    }
   ],
   "source": [
    "# Phase 5A: Domain-Specific Question Templates and Extraction Modules\n",
    "\n",
    "class DomainTemplateManager:\n",
    "    \"\"\"Manages extraction templates for different industries and document types\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.domain_templates = {\n",
    "            \"finance\": {\n",
    "                \"invoice\": [\n",
    "                    \"What is the invoice number?\",\n",
    "                    \"What is the total amount?\",\n",
    "                    \"What is the due date?\",\n",
    "                    \"Who is the vendor?\",\n",
    "                    \"What is the tax amount?\",\n",
    "                    \"What is the net amount?\",\n",
    "                    \"What payment terms are specified?\",\n",
    "                    \"What is the purchase order number?\"\n",
    "                ],\n",
    "                \"financial_statement\": [\n",
    "                    \"What is the total revenue?\",\n",
    "                    \"What is the net income?\",\n",
    "                    \"What is the reporting period?\",\n",
    "                    \"What are the total assets?\",\n",
    "                    \"What are the total liabilities?\",\n",
    "                    \"What is the cash flow from operations?\",\n",
    "                    \"What is the debt-to-equity ratio?\",\n",
    "                    \"What is the earnings per share?\"\n",
    "                ],\n",
    "                \"contract\": [\n",
    "                    \"What is the contract value?\",\n",
    "                    \"What is the contract duration?\",\n",
    "                    \"Who are the contracting parties?\",\n",
    "                    \"What is the effective date?\",\n",
    "                    \"What is the termination date?\",\n",
    "                    \"What are the payment terms?\",\n",
    "                    \"What penalties are specified?\",\n",
    "                    \"What deliverables are mentioned?\"\n",
    "                ]\n",
    "            },\n",
    "            \"healthcare\": {\n",
    "                \"medical_record\": [\n",
    "                    \"What is the patient name?\",\n",
    "                    \"What is the patient ID?\",\n",
    "                    \"What is the diagnosis?\",\n",
    "                    \"What medications are prescribed?\",\n",
    "                    \"What is the treatment plan?\",\n",
    "                    \"What are the vital signs?\",\n",
    "                    \"What allergies are documented?\",\n",
    "                    \"What is the next appointment date?\"\n",
    "                ],\n",
    "                \"lab_report\": [\n",
    "                    \"What tests were performed?\",\n",
    "                    \"What are the test results?\",\n",
    "                    \"What is the reference range?\",\n",
    "                    \"What is the specimen type?\",\n",
    "                    \"When was the sample collected?\",\n",
    "                    \"Who is the ordering physician?\",\n",
    "                    \"Are any results abnormal?\",\n",
    "                    \"What follow-up is recommended?\"\n",
    "                ],\n",
    "                \"prescription\": [\n",
    "                    \"What medication is prescribed?\",\n",
    "                    \"What is the dosage?\",\n",
    "                    \"What is the frequency?\",\n",
    "                    \"How long is the treatment duration?\",\n",
    "                    \"Who is the prescribing doctor?\",\n",
    "                    \"What is the patient name?\",\n",
    "                    \"Are there any warnings?\",\n",
    "                    \"How many refills are allowed?\"\n",
    "                ]\n",
    "            },\n",
    "            \"legal\": {\n",
    "                \"contract\":[\n",
    "                    \"What is the contract value?\",\n",
    "                    \"What is the contract duration?\", \n",
    "                    \"Who are the contracting parties?\",\n",
    "                    \"What is the effective date?\",\n",
    "                    \"What is the termination date?\",\n",
    "                    \"What are the payment terms?\",\n",
    "                    \"What penalties are specified?\",\n",
    "                    \"What deliverables are mentioned?\"\n",
    "                ],\n",
    "                \n",
    "                \"court_document\": [\n",
    "                    \"What is the case number?\",\n",
    "                    \"Who are the plaintiff and defendant?\",\n",
    "                    \"What court is handling the case?\",\n",
    "                    \"What is the filing date?\",\n",
    "                    \"What relief is sought?\",\n",
    "                    \"What are the key facts?\",\n",
    "                    \"What laws are cited?\",\n",
    "                    \"What is the next hearing date?\"\n",
    "                ],\n",
    "                \"legal_notice\": [\n",
    "                    \"Who is the sender?\",\n",
    "                    \"Who is the recipient?\",\n",
    "                    \"What is the subject matter?\",\n",
    "                    \"What action is demanded?\",\n",
    "                    \"What is the deadline for response?\",\n",
    "                    \"What legal basis is cited?\",\n",
    "                    \"What consequences are threatened?\",\n",
    "                    \"Is legal representation mentioned?\"\n",
    "                ]\n",
    "            },\n",
    "            \"hr\": {\n",
    "                \"resume\": [\n",
    "                    \"What is the full name of the candidate?\",\n",
    "                    \"What email address is provided for contact?\",\n",
    "                    \"What phone number is listed?\",\n",
    "                    \"What is the most recent job title?\",\n",
    "                    \"What company does the candidate currently work for?\",\n",
    "                    \"How many years of total experience are mentioned?\",\n",
    "                    \"What degree or education is mentioned?\",\n",
    "                    \"What programming languages are listed?\",\n",
    "                    \"What technical skills are mentioned?\",\n",
    "                    \"What university or school is mentioned?\",\n",
    "                    \"What certifications are listed?\",\n",
    "                    \"What projects are described?\",\n",
    "                    \"What achievements are highlighted?\",\n",
    "                    \"What software tools are mentioned?\",\n",
    "                    \"What languages does the candidate speak?\"\n",
    "                    ],\n",
    "                \"employee_record\": [\n",
    "                    \"What is the employee ID?\",\n",
    "                    \"What is the employee name?\",\n",
    "                    \"What is their department?\",\n",
    "                    \"What is their position?\",\n",
    "                    \"What is their salary?\",\n",
    "                    \"When was their hire date?\",\n",
    "                    \"Who is their manager?\",\n",
    "                    \"What benefits are they enrolled in?\"\n",
    "                ],\n",
    "                \"performance_review\": [\n",
    "                    \"What is the review period?\",\n",
    "                    \"What is the overall rating?\",\n",
    "                    \"What are the key achievements?\",\n",
    "                    \"What areas need improvement?\",\n",
    "                    \"What goals are set for next period?\",\n",
    "                    \"Is a promotion recommended?\",\n",
    "                    \"What training is suggested?\",\n",
    "                    \"What is the salary recommendation?\"\n",
    "                ]\n",
    "            },\n",
    "            \"education\": {\n",
    "                \"transcript\": [\n",
    "                    \"What is the student name?\",\n",
    "                    \"What is the student ID?\",\n",
    "                    \"What degree program?\",\n",
    "                    \"What is the GPA?\",\n",
    "                    \"What courses were completed?\",\n",
    "                    \"What grades were received?\",\n",
    "                    \"What is the graduation date?\",\n",
    "                    \"Are there any honors or distinctions?\"\n",
    "                ],\n",
    "                \"research_paper\": [\n",
    "                    \"What is the title?\",\n",
    "                    \"Who are the authors?\",\n",
    "                    \"What is the abstract?\",\n",
    "                    \"What methodology is used?\",\n",
    "                    \"What are the key findings?\",\n",
    "                    \"What conclusions are drawn?\",\n",
    "                    \"What future work is suggested?\",\n",
    "                    \"What references are cited?\"\n",
    "                ]\n",
    "            },\n",
    "            \"retail\": {\n",
    "                \"receipt\": [\n",
    "                    \"What store issued this receipt?\",\n",
    "                    \"What is the transaction date?\",\n",
    "                    \"What items were purchased?\",\n",
    "                    \"What are the item prices?\",\n",
    "                    \"What is the subtotal?\",\n",
    "                    \"What taxes were applied?\",\n",
    "                    \"What is the total amount?\",\n",
    "                    \"What payment method was used?\"\n",
    "                ],\n",
    "                \"inventory_report\": [\n",
    "                    \"What products are listed?\",\n",
    "                    \"What are the current stock levels?\",\n",
    "                    \"What is the reorder point?\",\n",
    "                    \"What is the unit cost?\",\n",
    "                    \"What is the total inventory value?\",\n",
    "                    \"Which items are low in stock?\",\n",
    "                    \"What is the turnover rate?\",\n",
    "                    \"When was the last inventory count?\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_domain_templates(self, domain: str, document_type: str = None) -> List[str]:\n",
    "        \"\"\"Get extraction templates for specific domain and document type\"\"\"\n",
    "        if domain not in self.domain_templates:\n",
    "            return self.domain_templates.get(\"finance\", {}).get(\"invoice\", [])  # Fallback\n",
    "        \n",
    "        domain_data = self.domain_templates[domain]\n",
    "        \n",
    "        if document_type and document_type in domain_data:\n",
    "            return domain_data[document_type]\n",
    "        \n",
    "        # Return all questions for the domain if no specific document type\n",
    "        all_questions = []\n",
    "        for doc_type, questions in domain_data.items():\n",
    "            all_questions.extend(questions)\n",
    "        \n",
    "        return all_questions\n",
    "    \n",
    "    def add_custom_domain(self, domain_name: str, templates: Dict[str, List[str]]):\n",
    "        \"\"\"Add new domain with custom templates\"\"\"\n",
    "        self.domain_templates[domain_name] = templates\n",
    "        print(f\"✅ Added custom domain: {domain_name}\")\n",
    "    \n",
    "    def detect_domain_and_type(self, text: str) -> tuple:\n",
    "        \"\"\"Auto-detect domain and document type from text content\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Healthcare keywords\n",
    "        if any(word in text_lower for word in ['patient', 'diagnosis', 'prescription', 'medical', 'doctor', 'hospital']):\n",
    "            if any(word in text_lower for word in ['prescription', 'medication', 'dosage']):\n",
    "                return 'healthcare', 'prescription'\n",
    "            elif any(word in text_lower for word in ['lab', 'test', 'result', 'specimen']):\n",
    "                return 'healthcare', 'lab_report'\n",
    "            else:\n",
    "                return 'healthcare', 'medical_record'\n",
    "        \n",
    "        # Legal keywords\n",
    "        elif any(word in text_lower for word in ['court', 'plaintiff', 'defendant', 'lawsuit', 'legal']):\n",
    "            if any(word in text_lower for word in ['case number', 'filing', 'court']):\n",
    "                return 'legal', 'court_document'\n",
    "            elif any(word in text_lower for word in ['notice', 'demand', 'cease']):\n",
    "                return 'legal', 'legal_notice'\n",
    "            else:\n",
    "                return 'legal', 'contract'\n",
    "        \n",
    "        # HR keywords\n",
    "        elif any(word in text_lower for word in ['employee', 'resume', 'candidate', 'performance']):\n",
    "            if any(word in text_lower for word in ['resume', 'cv', 'experience', 'education']):\n",
    "                return 'hr', 'resume'\n",
    "            elif any(word in text_lower for word in ['performance', 'review', 'rating']):\n",
    "                return 'hr', 'performance_review'\n",
    "            else:\n",
    "                return 'hr', 'employee_record'\n",
    "        \n",
    "        # Education keywords\n",
    "        elif any(word in text_lower for word in ['student', 'grade', 'transcript', 'university', 'research']):\n",
    "            if any(word in text_lower for word in ['transcript', 'gpa', 'courses']):\n",
    "                return 'education', 'transcript'\n",
    "            else:\n",
    "                return 'education', 'research_paper'\n",
    "        \n",
    "        # Retail keywords\n",
    "        elif any(word in text_lower for word in ['receipt', 'purchase', 'inventory', 'store', 'items']):\n",
    "            if any(word in text_lower for word in ['inventory', 'stock', 'reorder']):\n",
    "                return 'retail', 'inventory_report'\n",
    "            else:\n",
    "                return 'retail', 'receipt'\n",
    "        \n",
    "        # Finance keywords (including existing invoice detection)\n",
    "        elif any(word in text_lower for word in ['invoice', 'bill', 'payment', 'financial', 'revenue']):\n",
    "            if any(word in text_lower for word in ['revenue', 'income', 'assets', 'liabilities']):\n",
    "                return 'finance', 'financial_statement'\n",
    "            elif any(word in text_lower for word in ['contract', 'agreement', 'terms']):\n",
    "                return 'finance', 'contract'\n",
    "            else:\n",
    "                return 'finance', 'invoice'\n",
    "        \n",
    "        # Default fallback\n",
    "        return 'finance', 'invoice'\n",
    "\n",
    "# Initialize domain manager\n",
    "domain_manager = DomainTemplateManager()\n",
    "print(\"✅ Domain Template Manager initialized with 6 domains and 15+ document types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8df5e9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Domain Extractor ready for 6 industries\n"
     ]
    }
   ],
   "source": [
    "# Phase 5B: Multi-Domain Extractor\n",
    "\n",
    "class MultiDomainExtractor(QABasedExtractor):\n",
    "    \"\"\"Extended QA extractor with domain-specific capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"deepset/xlm-roberta-large-squad2\", local_dir=\"./lora_fine_tuned_model\"):\n",
    "        super().__init__(model_name, local_dir)\n",
    "        self.domain_manager = DomainTemplateManager()\n",
    "        \n",
    "        # Add domain-specific preprocessing patterns\n",
    "        self.domain_patterns = {\n",
    "            'finance': {\n",
    "                'currency': r'[\\$€£¥][\\d,\\.]+',\n",
    "                'dates': r'\\d{1,2}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{2,4}',\n",
    "                'invoice_numbers': r'(?:INV|inv|Invoice|INVOICE)[#\\-\\s]*([A-Z0-9\\-]+)'\n",
    "            },\n",
    "            'healthcare': {\n",
    "                'medications': r'(?:mg|ml|tablets?|capsules?)\\s*\\d+',\n",
    "                'vital_signs': r'(?:BP|Blood Pressure)[:\\s]*\\d+\\/\\d+',\n",
    "                'patient_ids': r'(?:Patient ID|ID)[:\\s]*([A-Z0-9\\-]+)'\n",
    "            },\n",
    "            'legal': {\n",
    "                'case_numbers': r'(?:Case|No\\.)[:\\s]*([A-Z0-9\\-\\/]+)',\n",
    "                'dates': r'\\d{1,2}(?:st|nd|rd|th)?\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}',\n",
    "                'parties': r'(?:Plaintiff|Defendant)[:\\s]*([A-Za-z\\s,\\.]+)'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def preprocess_domain_text(self, text: str, domain: str) -> str:\n",
    "        \"\"\"Enhanced preprocessing based on detected domain\"\"\"\n",
    "        text = super().preprocess_text(text)\n",
    "        \n",
    "        if domain in self.domain_patterns:\n",
    "            patterns = self.domain_patterns[domain]\n",
    "            \n",
    "            # Add domain-specific context markers\n",
    "            for pattern_type, pattern in patterns.items():\n",
    "                matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    if isinstance(match, tuple):\n",
    "                        match = match[0] if match else \"\"\n",
    "                    text = text.replace(str(match), f\"IMPORTANT_{pattern_type.upper()}: {match}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_multi_domain(self, text: str, domain: str = None, document_type: str = None) -> Dict:\n",
    "        \"\"\"Extract information using domain-specific templates\"\"\"\n",
    "        \n",
    "        # Auto-detect domain if not provided\n",
    "        if not domain:\n",
    "            domain, document_type = self.domain_manager.detect_domain_and_type(text)\n",
    "            print(f\" Auto-detected: {domain}/{document_type}\")\n",
    "        \n",
    "        # Get domain-specific questions\n",
    "        questions = self.domain_manager.get_domain_templates(domain, document_type)\n",
    "        \n",
    "        # Use domain-specific preprocessing\n",
    "        processed_text = self.preprocess_domain_text(text, domain)\n",
    "        \n",
    "        # Extract with domain-specific questions\n",
    "        qa_results = self.extract_with_questions(processed_text, questions)\n",
    "        \n",
    "        # Get entities\n",
    "        ner_results = self.extract_entities_with_ner(processed_text)\n",
    "        \n",
    "        successful = sum(1 for r in qa_results.values() if r.get('extracted'))\n",
    "        \n",
    "        return {\n",
    "            'domain': domain,\n",
    "            'document_type': document_type,\n",
    "            'extraction_timestamp': datetime.now().isoformat(),\n",
    "            'model_used': self.model_name,\n",
    "            'total_questions': len(questions),\n",
    "            'successful_extractions': successful,\n",
    "            'success_rate': successful / len(questions) if questions else 0,\n",
    "            'extractions': qa_results,\n",
    "            'entities': ner_results,\n",
    "            'confidence_distribution': self._analyze_confidence_distribution(qa_results)\n",
    "        }\n",
    "    \n",
    "    def _analyze_confidence_distribution(self, qa_results: Dict) -> Dict:\n",
    "        \"\"\"Analyze confidence score distribution for quality assessment\"\"\"\n",
    "        confidences = [r.get('confidence', 0) for r in qa_results.values()]\n",
    "        \n",
    "        if not confidences:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'mean_confidence': sum(confidences) / len(confidences),\n",
    "            'high_confidence_count': sum(1 for c in confidences if c >= 0.7),\n",
    "            'medium_confidence_count': sum(1 for c in confidences if 0.3 <= c < 0.7),\n",
    "            'low_confidence_count': sum(1 for c in confidences if c < 0.3),\n",
    "            'confidence_std': np.std(confidences) if len(confidences) > 1 else 0\n",
    "        }\n",
    "\n",
    "# Create multi-domain extractor instance\n",
    "multi_extractor = MultiDomainExtractor()\n",
    "print(\"✅ Multi-Domain Extractor ready for 6 industries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df052528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registered module: table_extractor\n",
      "✅ Registered module: email_extractor\n",
      "✅ Registered module: datetime_extractor\n",
      "✅ Modular Extraction System initialized with 3 default modules\n"
     ]
    }
   ],
   "source": [
    "# Phase 5C: Plug-and-Play Module System\n",
    "\n",
    "class ExtractorModule:\n",
    "    \"\"\"Base class for extraction modules\"\"\"\n",
    "    \n",
    "    def __init__(self, module_name: str, supported_formats: List[str]):\n",
    "        self.module_name = module_name\n",
    "        self.supported_formats = supported_formats\n",
    "        self.is_active = True\n",
    "    \n",
    "    def can_process(self, file_path: str, content: str) -> bool:\n",
    "        \"\"\"Check if this module can process the given file\"\"\"\n",
    "        file_ext = os.path.splitext(file_path)[1].lower()\n",
    "        return file_ext in self.supported_formats\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        \"\"\"Override this method in subclasses\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement extract method\")\n",
    "\n",
    "class TableExtractionModule(ExtractorModule):\n",
    "    \"\"\"Specialized module for table extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"table_extractor\", [\".pdf\", \".xlsx\", \".csv\"])\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        # Simulate table extraction\n",
    "        tables = []\n",
    "        \n",
    "        # Look for table-like patterns\n",
    "        lines = content.split('\\n')\n",
    "        potential_tables = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            # lines with multiple numbers/currencies\n",
    "            if len(re.findall(r'\\d+[,\\.]?\\d*', line)) >= 3:\n",
    "                potential_tables.append({\n",
    "                    'line_number': i + 1,\n",
    "                    'content': line.strip(),\n",
    "                    'confidence': 0.8\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'module': self.module_name,\n",
    "            'tables_found': len(potential_tables),\n",
    "            'tables': potential_tables[:5],  # Limit to first 5\n",
    "            'extraction_type': 'tabular_data'\n",
    "        }\n",
    "\n",
    "class EmailExtractionModule(ExtractorModule):\n",
    "    \"\"\"Specialized module for email extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"email_extractor\", [\".eml\", \".txt\", \".pdf\"])\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        phone_pattern = r'(?:\\+\\d{1,3}[\\s-]?)?\\(?[0-9]{3}\\)?[\\s-]?[0-9]{3}[\\s-]?[0-9]{4}'\n",
    "        \n",
    "        emails = re.findall(email_pattern, content)\n",
    "        phones = re.findall(phone_pattern, content)\n",
    "        \n",
    "        return {\n",
    "            'module': self.module_name,\n",
    "            'emails_found': emails,\n",
    "            'phones_found': phones,\n",
    "            'contact_count': len(emails) + len(phones),\n",
    "            'extraction_type': 'contact_information'\n",
    "        }\n",
    "\n",
    "class DateTimeExtractionModule(ExtractorModule):\n",
    "    \"\"\"Specialized module for date/time extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"datetime_extractor\", [\".pdf\", \".txt\", \".docx\"])\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        # Various date patterns\n",
    "        date_patterns = [\n",
    "            r'\\d{1,2}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{2,4}',  # MM/DD/YYYY\n",
    "            r'\\d{4}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{1,2}',    # YYYY/MM/DD\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{4}\\b',\n",
    "            r'\\d{1,2}(?:st|nd|rd|th)?\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}'\n",
    "        ]\n",
    "        \n",
    "        found_dates = []\n",
    "        for pattern in date_patterns:\n",
    "            matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "            found_dates.extend(matches)\n",
    "        \n",
    "        return {\n",
    "            'module': self.module_name,\n",
    "            'dates_found': found_dates,\n",
    "            'date_count': len(found_dates),\n",
    "            'extraction_type': 'temporal_information'\n",
    "        }\n",
    "\n",
    "class ModularExtractionSystem:\n",
    "    \"\"\"Plug-and-play system for managing extraction modules\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.modules = {}\n",
    "        self.processing_order = []\n",
    "        \n",
    "        # Register default modules\n",
    "        self._register_default_modules()\n",
    "    \n",
    "    def _register_default_modules(self):\n",
    "        \"\"\"Register built-in extraction modules\"\"\"\n",
    "        default_modules = [\n",
    "            TableExtractionModule(),\n",
    "            EmailExtractionModule(),\n",
    "            DateTimeExtractionModule()\n",
    "        ]\n",
    "        \n",
    "        for module in default_modules:\n",
    "            self.register_module(module)\n",
    "    \n",
    "    def register_module(self, module: ExtractorModule):\n",
    "        \"\"\"Register a new extraction module\"\"\"\n",
    "        self.modules[module.module_name] = module\n",
    "        if module.module_name not in self.processing_order:\n",
    "            self.processing_order.append(module.module_name)\n",
    "        print(f\"✅ Registered module: {module.module_name}\")\n",
    "    \n",
    "    def unregister_module(self, module_name: str):\n",
    "        \"\"\"Remove an extraction module\"\"\"\n",
    "        if module_name in self.modules:\n",
    "            del self.modules[module_name]\n",
    "            if module_name in self.processing_order:\n",
    "                self.processing_order.remove(module_name)\n",
    "            print(f\"❌ Unregistered module: {module_name}\")\n",
    "    \n",
    "    def get_compatible_modules(self, file_path: str, content: str) -> List[ExtractorModule]:\n",
    "        \"\"\"Get modules that can process the given file\"\"\"\n",
    "        compatible = []\n",
    "        for module in self.modules.values():\n",
    "            if module.is_active and module.can_process(file_path, content):\n",
    "                compatible.append(module)\n",
    "        return compatible\n",
    "    \n",
    "    def extract_with_modules(self, file_path: str, content: str) -> Dict:\n",
    "        \"\"\"Run all compatible modules on the content\"\"\"\n",
    "        compatible_modules = self.get_compatible_modules(file_path, content)\n",
    "        \n",
    "        results = {\n",
    "            'file_path': file_path,\n",
    "            'modules_used': [m.module_name for m in compatible_modules],\n",
    "            'module_results': {},\n",
    "            'total_modules': len(compatible_modules),\n",
    "            'extraction_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        for module in compatible_modules:\n",
    "            try:\n",
    "                module_result = module.extract(content)\n",
    "                results['module_results'][module.module_name] = module_result\n",
    "                print(f\"✅ {module.module_name}: {module_result.get('extraction_type', 'extracted')}\")\n",
    "            except Exception as e:\n",
    "                results['module_results'][module.module_name] = {\n",
    "                    'error': str(e),\n",
    "                    'extraction_type': 'failed'\n",
    "                }\n",
    "                print(f\" {module.module_name}: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize modular system\n",
    "modular_system = ModularExtractionSystem()\n",
    "print(\"✅ Modular Extraction System initialized with 3 default modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e13cc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5D: Unified Scalable Pipeline\n",
    "\n",
    "def enhanced_multi_domain_extraction(file_path: str, domain: str = None, \n",
    "                                   document_type: str = None, \n",
    "                                   use_modules: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Ultimate extraction pipeline combining:\n",
    "    - Multi-domain QA extraction\n",
    "    - Plug-and-play modules\n",
    "    - Original normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Load and normalize (existing system)\n",
    "    document_data = load_and_normalize(file_path)\n",
    "    \n",
    "    # Step 2: Combine text for processing\n",
    "    all_text = \"\"\n",
    "    for content_item in document_data['content']:\n",
    "        all_text += content_item['text'] + \" \"\n",
    "    \n",
    "    # Step 3: Multi-domain QA extraction\n",
    "    qa_results = multi_extractor.extract_multi_domain(all_text, domain, document_type)\n",
    "    \n",
    "    # Step 4: Modular extraction (if enabled)\n",
    "    module_results = {}\n",
    "    if use_modules:\n",
    "        module_results = modular_system.extract_with_modules(file_path, all_text)\n",
    "    \n",
    "    # Step 5: Combine all results\n",
    "    enhanced_document = {\n",
    "        **document_data,  # Original normalized data\n",
    "        'multi_domain_extraction': qa_results,\n",
    "        'modular_extraction': module_results,\n",
    "        'scalability_features': {\n",
    "            'domain_detected': qa_results.get('domain', 'unknown'),\n",
    "            'document_type_detected': qa_results.get('document_type', 'unknown'),\n",
    "            'modules_used': module_results.get('modules_used', []),\n",
    "            'total_extraction_methods': 1 + len(module_results.get('modules_used', [])),\n",
    "            'confidence_analysis': qa_results.get('confidence_distribution', {}),\n",
    "            'processing_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return enhanced_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4221c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING MULTI-DOMAIN SCALABILITY\n",
      "❌ Test file not found: C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\interview\\UNPAID_Internship_Agreement_Aslican_Alacal.pdf\n",
      "\n",
      " Testing: CV\n",
      "Processing pdf file: Aslican_Alacal_CV.pdf\n",
      " Auto-detected: education/research_paper\n",
      "✅ table_extractor: tabular_data\n",
      "✅ email_extractor: contact_information\n",
      "✅ datetime_extractor: temporal_information\n",
      "✅ Domain: education\n",
      "✅ Document Type: research_paper\n",
      "✅ Success Rate: 12.5%\n",
      "✅ Modules Used: 3\n",
      "✅ Mean Confidence: 0.016\n",
      "\n",
      " Testing: invoice\n",
      "Processing pdf file: SWME_Rechnung_07122020.pdf\n",
      " Auto-detected: finance/invoice\n",
      "✅ table_extractor: tabular_data\n",
      "✅ email_extractor: contact_information\n",
      "✅ datetime_extractor: temporal_information\n",
      "✅ Domain: finance\n",
      "✅ Document Type: invoice\n",
      "✅ Success Rate: 50.0%\n",
      "✅ Modules Used: 3\n",
      "✅ Mean Confidence: 0.210\n",
      "   Q: What is the invoice number?...\n",
      "   A:  100260250 (conf: 0.618)\n",
      "   Q: What is the due date?...\n",
      "   A:  21.12.2020 (conf: 0.758)\n"
     ]
    }
   ],
   "source": [
    "# Phase 5D: Test scalability with different domains\n",
    "\n",
    "print(\"TESTING MULTI-DOMAIN SCALABILITY\")\n",
    "\n",
    "# Test files for different domains \n",
    "test_scenarios = [\n",
    "    {\n",
    "        'name': 'Internship_Agreement',\n",
    "        'file': r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\interview\\UNPAID_Internship_Agreement_Aslican_Alacal.pdf\",\n",
    "        'expected_domain': 'hr'\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'name': 'CV',\n",
    "        'file': r\"C:\\Users\\aslia\\Downloads\\Aslican_Alacal_CV.pdf\",\n",
    "        'expected_domain': 'hr'\n",
    "    },\n",
    "      {\n",
    "        'name': 'invoice',\n",
    "        'file': r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\\SWME_Rechnung_07122020.pdf\",\n",
    "        'expected_domain': 'finance'\n",
    "    }\n",
    "    # add more test files \n",
    "]\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    if os.path.exists(scenario['file']):\n",
    "        print(f\"\\n Testing: {scenario['name']}\")\n",
    "        \n",
    "        result = enhanced_multi_domain_extraction(scenario['file'])\n",
    "        \n",
    "        scalability = result['scalability_features']\n",
    "        domain_result = result['multi_domain_extraction']\n",
    "        \n",
    "        print(f\"✅ Domain: {scalability['domain_detected']}\")\n",
    "        print(f\"✅ Document Type: {scalability['document_type_detected']}\")\n",
    "        print(f\"✅ Success Rate: {domain_result['success_rate']:.1%}\")\n",
    "        print(f\"✅ Modules Used: {len(scalability['modules_used'])}\")\n",
    "        print(f\"✅ Mean Confidence: {scalability['confidence_analysis'].get('mean_confidence', 0):.3f}\")\n",
    "        \n",
    "        # Show top extractions\n",
    "        extractions = domain_result['extractions']\n",
    "        high_conf_count = 0\n",
    "        for question, answer in extractions.items():\n",
    "            if answer.get('confidence', 0) > 0.5 and high_conf_count < 3:\n",
    "                print(f\"   Q: {question[:60]}...\")\n",
    "                print(f\"   A: {answer['answer']} (conf: {answer['confidence']:.3f})\")\n",
    "                high_conf_count += 1\n",
    "    else:\n",
    "        print(f\"❌ Test file not found: {scenario['file']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52db25",
   "metadata": {},
   "source": [
    "**6. Explainability and Trust**\n",
    "* Integrate explainable AI (XAI) tools (e.g., SHAP, attention visualization) to make extraction results transparent and interpretable for end users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26283b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6A- Explainable AI Dependencies and Setup\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "from typing import Optional, Union, Tuple\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForQuestionAnswering, \n",
    "    pipeline,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DefaultDataCollator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf3e3945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Attention Visualizer ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 6B - Attention Visualization for Transformer Models\n",
    "\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    \"\"\"Visualize attention patterns in transformer models for explainability\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def get_attention_weights(self, question: str, context: str):\n",
    "        \"\"\"Extract attention weights from the model\"\"\"\n",
    "        try:\n",
    "            # Tokenize inputs\n",
    "            inputs = self.tokenizer(\n",
    "                question, context,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Get model outputs with attention\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, output_attentions=True)\n",
    "            \n",
    "            # Extract attention weights (last layer, first head for simplicity)\n",
    "            attention = outputs.attentions[-1][0, 0].cpu().numpy()\n",
    "            \n",
    "            # Get tokens\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "            \n",
    "            return {\n",
    "                'attention_weights': attention,\n",
    "                'tokens': tokens,\n",
    "                'question_length': len(self.tokenizer.tokenize(question)),\n",
    "                'context_start': len(self.tokenizer.tokenize(question)) + 2  # +2 for special tokens\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Attention extraction failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def visualize_attention_heatmap(self, question: str, context: str, save_path: str = None):\n",
    "        \"\"\"Create attention heatmap visualization\"\"\"\n",
    "        attention_data = self.get_attention_weights(question, context)\n",
    "        \n",
    "        if not attention_data:\n",
    "            return None\n",
    "        \n",
    "        attention = attention_data['attention_weights']\n",
    "        tokens = attention_data['tokens']\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(\n",
    "            attention[:len(tokens), :len(tokens)],\n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cmap='Blues',\n",
    "            cbar=True\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Attention Heatmap\\nQ: {question[:50]}...')\n",
    "        plt.xlabel('Target Tokens')\n",
    "        plt.ylabel('Source Tokens')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\" Attention heatmap saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        return attention_data\n",
    "    \n",
    "    def get_top_attended_tokens(self, question: str, context: str, top_k: int = 5):\n",
    "        \"\"\"Get tokens that received highest attention for answer\"\"\"\n",
    "        attention_data = self.get_attention_weights(question, context)\n",
    "        \n",
    "        if not attention_data:\n",
    "            return []\n",
    "        \n",
    "        attention = attention_data['attention_weights']\n",
    "        tokens = attention_data['tokens']\n",
    "        context_start = attention_data['context_start']\n",
    "        \n",
    "        # Focus on context tokens\n",
    "        context_attention = attention[context_start:, context_start:]\n",
    "        context_tokens = tokens[context_start:]\n",
    "        \n",
    "        # Get average attention received by each context token\n",
    "        avg_attention = context_attention.mean(axis=0)\n",
    "        \n",
    "        # Get top attended tokens\n",
    "        top_indices = avg_attention.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        top_tokens = []\n",
    "        for idx in top_indices:\n",
    "            if idx < len(context_tokens):\n",
    "                top_tokens.append({\n",
    "                    'token': context_tokens[idx],\n",
    "                    'attention_score': avg_attention[idx],\n",
    "                    'position': idx\n",
    "                })\n",
    "        \n",
    "        return top_tokens\n",
    "\n",
    "# Initialize attention visualizer\n",
    "print(\"✅ Attention Visualizer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3b794b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LIME Explainer ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 6C - LIME Text Explainer Integration\n",
    "\n",
    "\n",
    "class LIMEExplainer:\n",
    "    \"\"\"LIME-based explainability for QA predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, qa_pipeline):\n",
    "        self.qa_pipeline = qa_pipeline\n",
    "        self.explainer = LimeTextExplainer(class_names=['answer_confidence'])\n",
    "        \n",
    "    def predict_function(self, texts, question):\n",
    "        \"\"\"Prediction function for LIME\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                result = self.qa_pipeline(question=question, context=text)\n",
    "                # Return confidence score as prediction\n",
    "                predictions.append([1 - result['score'], result['score']])\n",
    "            except:\n",
    "                predictions.append([0.5, 0.5])  # Neutral if failed\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def explain_prediction(self, question: str, context: str, num_features: int = 10):\n",
    "        \"\"\"Generate LIME explanation for QA prediction\"\"\"\n",
    "        \n",
    "        # Create prediction function for this specific question\n",
    "        def pred_fn(texts):\n",
    "            return self.predict_function(texts, question)\n",
    "        \n",
    "        try:\n",
    "            # Generate explanation\n",
    "            explanation = self.explainer.explain_instance(\n",
    "                context,\n",
    "                pred_fn,\n",
    "                num_features=num_features,\n",
    "                num_samples=100\n",
    "            )\n",
    "            \n",
    "            # Extract feature importance\n",
    "            feature_importance = explanation.as_list()\n",
    "            \n",
    "            # Get original prediction\n",
    "            original_result = self.qa_pipeline(question=question, context=context)\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'predicted_answer': original_result['answer'],\n",
    "                'confidence': original_result['score'],\n",
    "                'feature_importance': feature_importance,\n",
    "                'explanation_type': 'LIME',\n",
    "                'important_words': [item[0] for item in feature_importance[:5]]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ LIME explanation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def visualize_explanation(self, explanation_data, save_path: str = None):\n",
    "        \"\"\"Visualize LIME explanation\"\"\"\n",
    "        if not explanation_data:\n",
    "            return\n",
    "        \n",
    "        features = explanation_data['feature_importance']\n",
    "        \n",
    "        # Separate positive and negative contributions\n",
    "        words = [item[0] for item in features]\n",
    "        scores = [item[1] for item in features]\n",
    "        \n",
    "        # Create colors based on positive/negative contribution\n",
    "        colors = ['green' if score > 0 else 'red' for score in scores]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.barh(range(len(words)), scores, color=colors, alpha=0.7)\n",
    "        \n",
    "        plt.yticks(range(len(words)), words)\n",
    "        plt.xlabel('Feature Importance (Positive = Supports Answer)')\n",
    "        plt.title(f'LIME Explanation\\nQ: {explanation_data[\"question\"][:50]}...\\nA: {explanation_data[\"predicted_answer\"]} (conf: {explanation_data[\"confidence\"]:.3f})')\n",
    "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, scores):\n",
    "            plt.text(bar.get_width() + (0.01 if score > 0 else -0.01), \n",
    "                    bar.get_y() + bar.get_height()/2,\n",
    "                    f'{score:.3f}', \n",
    "                    ha='left' if score > 0 else 'right', \n",
    "                    va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\" LIME explanation saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "print(\"✅ LIME Explainer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3fb90d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded fine-tuned model from ./lora_fine_tuned_model\n",
      "✅ Explainable QA System ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 6D - Comprehensive Explainability Pipeline\n",
    "\n",
    "class ExplainableQASystem:\n",
    "    \"\"\"Complete explainable QA system combining multiple XAI techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"./lora_fine_tuned_model\"):\n",
    "        # Load model and tokenizer\n",
    "        if os.path.exists(model_path):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer\n",
    "            )\n",
    "            print(f\"✅ Loaded fine-tuned model from {model_path}\")\n",
    "        else:\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=\"deepset/xlm-roberta-large-squad2\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"deepset/xlm-roberta-large-squad2\")\n",
    "            self.model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/xlm-roberta-large-squad2\")\n",
    "            print(\"✅ Using base model for explainability\")\n",
    "        \n",
    "        # Initialize explainers\n",
    "        self.attention_viz = AttentionVisualizer(self.model, self.tokenizer)\n",
    "        self.lime_explainer = LIMEExplainer(self.qa_pipeline)\n",
    "        \n",
    "    def explain_prediction(self, question: str, context: str, \n",
    "                         use_attention: bool = True, \n",
    "                         use_lime: bool = True,\n",
    "                         save_visualizations: bool = True) -> Dict:\n",
    "        \"\"\"Generate comprehensive explanation for QA prediction\"\"\"\n",
    "        # TRUNCATE CONTEXT TO PREVENT MEMORY ISSUES\n",
    "        if len(context) > 2000:\n",
    "            context = context[:2000] + \"...\"\n",
    "            print(\" Context truncated for memory optimization\")\n",
    "        \n",
    "        print(f\"Generating explanations for:\")\n",
    "        print(f\"Q: {question[:60]}...\")\n",
    "        print(f\"Context: {context[:100]}...\")\n",
    "        \n",
    "        # Get base prediction\n",
    "        prediction = self.qa_pipeline(question=question, context=context)\n",
    "        \n",
    "        explanations = {\n",
    "            'question': question,\n",
    "            'context': context[:200] + \"...\" if len(context) > 200 else context,\n",
    "            'prediction': {\n",
    "                'answer': prediction['answer'],\n",
    "                'confidence': prediction['score'],\n",
    "                'start_pos': prediction['start'],\n",
    "                'end_pos': prediction['end']\n",
    "            },\n",
    "            'explanations': {}\n",
    "        }\n",
    "        \n",
    "        # Generate attention explanation\n",
    "        if use_attention:\n",
    "            print(\"Generating attention visualization...\")\n",
    "            try:\n",
    "                top_tokens = self.attention_viz.get_top_attended_tokens(question, context)\n",
    "                explanations['explanations']['attention'] = {\n",
    "                    'top_attended_tokens': top_tokens,\n",
    "                    'explanation_type': 'attention_weights'\n",
    "                }\n",
    "                \n",
    "                if save_visualizations:\n",
    "                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                    attention_path = f\"attention_explanation_{timestamp}.png\"\n",
    "                    self.attention_viz.visualize_attention_heatmap(\n",
    "                        question, context, attention_path\n",
    "                    )\n",
    "                    explanations['explanations']['attention']['visualization_path'] = attention_path\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Attention explanation failed: {e}\")\n",
    "        \n",
    "        # Generate LIME explanation\n",
    "        if use_lime:\n",
    "            print(\" Generating LIME explanation...\")\n",
    "            try:\n",
    "                lime_result = self.lime_explainer.explain_prediction(question, context)\n",
    "                if lime_result:\n",
    "                    explanations['explanations']['lime'] = lime_result\n",
    "                    \n",
    "                    if save_visualizations:\n",
    "                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                        lime_path = f\"lime_explanation_{timestamp}.png\"\n",
    "                        self.lime_explainer.visualize_explanation(lime_result, lime_path)\n",
    "                        explanations['explanations']['lime']['visualization_path'] = lime_path\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"❌LIME explanation failed: {e}\")\n",
    "        \n",
    "        # Generate confidence interpretation\n",
    "        explanations['trust_indicators'] = self._generate_trust_indicators(prediction, explanations)\n",
    "        \n",
    "        return explanations\n",
    "    \n",
    "    def _generate_trust_indicators(self, prediction: Dict, explanations: Dict) -> Dict:\n",
    "        \"\"\"Generate trust and reliability indicators\"\"\"\n",
    "        confidence = prediction['score']\n",
    "        \n",
    "        # Confidence level categorization\n",
    "        if confidence >= 0.8:\n",
    "            confidence_level = \"High\"\n",
    "            trust_message = \"High confidence - answer is very reliable\"\n",
    "        elif confidence >= 0.5:\n",
    "            confidence_level = \"Medium\"\n",
    "            trust_message = \"Medium confidence - answer is moderately reliable\"\n",
    "        elif confidence >= 0.3:\n",
    "            confidence_level = \"Low\"\n",
    "            trust_message = \"Low confidence - answer may be uncertain\"\n",
    "        else:\n",
    "            confidence_level = \"Very Low\"\n",
    "            trust_message = \"Very low confidence - answer is unreliable\"\n",
    "        \n",
    "        # Check explanation consistency\n",
    "        explanation_consistency = \"Unknown\"\n",
    "        if 'attention' in explanations.get('explanations', {}):\n",
    "            top_tokens = explanations['explanations']['attention'].get('top_attended_tokens', [])\n",
    "            if top_tokens and len(top_tokens) > 0:\n",
    "                # Check if answer appears in top attended tokens\n",
    "                answer_tokens = prediction['answer'].lower().split()\n",
    "                attended_tokens = [t['token'].lower().replace('##', '') for t in top_tokens]\n",
    "                \n",
    "                overlap = any(token in ' '.join(attended_tokens) for token in answer_tokens)\n",
    "                explanation_consistency = \"High\" if overlap else \"Low\"\n",
    "        \n",
    "        return {\n",
    "            'confidence_score': confidence,\n",
    "            'confidence_level': confidence_level,\n",
    "            'trust_message': trust_message,\n",
    "            'explanation_consistency': explanation_consistency,\n",
    "            'reliability_score': min(confidence * 1.2, 1.0) if explanation_consistency == \"High\" else confidence * 0.8\n",
    "        }\n",
    "    \n",
    "    def batch_explain_extractions(self, extractions_dict: Dict, max_explanations: int = 3) -> Dict:\n",
    "        \"\"\"Generate explanations for multiple extractions\"\"\"\n",
    "        print(f\"Generating explanations for top {max_explanations} extractions...\")\n",
    "        \n",
    "        explained_extractions = {}\n",
    "        count = 0\n",
    "        \n",
    "        for question, result in extractions_dict.items():\n",
    "            if count >= max_explanations:\n",
    "                break\n",
    "                \n",
    "            if result.get('answer') and result.get('confidence', 0) > 0.3:\n",
    "                print(f\"\\n Explaining: {question[:50]}...\")\n",
    "                \n",
    "                # Reconstruct context \n",
    "                context = result.get('context', 'Context not available')\n",
    "                \n",
    "                explanation = self.explain_prediction(\n",
    "                    question=question,\n",
    "                    context=context,\n",
    "                    use_attention=True,\n",
    "                    use_lime=True,\n",
    "                    save_visualizations=True\n",
    "                )\n",
    "                \n",
    "                explained_extractions[question] = explanation\n",
    "                count += 1\n",
    "        \n",
    "        return explained_extractions\n",
    "\n",
    "# Initialize explainable QA system\n",
    "explainable_qa = ExplainableQASystem()\n",
    "print(\"✅ Explainable QA System ready!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0e3d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Explainable Multi-Domain Extraction ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Phase 6E: Integration with Multi-Domain System\n",
    "\n",
    "def enhanced_multi_domain_extraction_with_explanations(file_path: str, \n",
    "                                                      domain: str = None, \n",
    "                                                      document_type: str = None,\n",
    "                                                      explain_top_results: bool = True,\n",
    "                                                      max_explanations: int = 2) -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced extraction with built-in explainability\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1: Run your existing multi-domain extraction\n",
    "    result = enhanced_multi_domain_extraction(file_path, domain, document_type)\n",
    "    \n",
    "    # 2: Add explainability for high-confidence results\n",
    "    if explain_top_results:\n",
    "        print(f\"\\n Adding explainability to extraction results...\")\n",
    "        \n",
    "        # Get text content for explanations\n",
    "        all_text = \"\"\n",
    "        for content_item in result['content']:\n",
    "            all_text += content_item['text'] + \" \"\n",
    "        \n",
    "        # Get high-confidence extractions\n",
    "        extractions = result['multi_domain_extraction']['extractions']\n",
    "        high_conf_extractions = {}\n",
    "        \n",
    "        count = 0\n",
    "        for question, answer_data in extractions.items():\n",
    "            if (answer_data.get('confidence', 0) > 0.4 and \n",
    "                answer_data.get('answer') and \n",
    "                count < max_explanations):\n",
    "                \n",
    "                high_conf_extractions[question] = {\n",
    "                    **answer_data,\n",
    "                    'context': all_text  # Add context for explanation\n",
    "                }\n",
    "                count += 1\n",
    "        \n",
    "        # Generate explanations\n",
    "        if high_conf_extractions:\n",
    "            explanations = {}\n",
    "            for question, data in high_conf_extractions.items():\n",
    "                print(f\" Explaining: {question[:40]}...\")\n",
    "                \n",
    "                explanation = explainable_qa.explain_prediction(\n",
    "                    question=question,\n",
    "                    context=data['context'],\n",
    "                    use_attention=True,\n",
    "                    use_lime=True,\n",
    "                    save_visualizations=True\n",
    "                )\n",
    "                \n",
    "                explanations[question] = explanation\n",
    "            \n",
    "            # Add explanations to result\n",
    "            result['explainability'] = {\n",
    "                'explained_extractions': explanations,\n",
    "                'explanation_count': len(explanations),\n",
    "                'explainability_methods': ['attention_visualization', 'lime_text_explanation'],\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            }\n",
    "        else:\n",
    "            result['explainability'] = {\n",
    "                'message': 'No high-confidence extractions found for explanation',\n",
    "                'min_confidence_threshold': 0.4\n",
    "            }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✅ Explainable Multi-Domain Extraction ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5a27ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing explainability with: invoice_10697.pdf\n",
      "Processing pdf file: invoice_10697.pdf\n",
      " Auto-detected: finance/invoice\n",
      "✅ table_extractor: tabular_data\n",
      "✅ email_extractor: contact_information\n",
      "✅ datetime_extractor: temporal_information\n",
      "\n",
      " Adding explainability to extraction results...\n",
      "\n",
      "✅ EXPLAINABLE EXTRACTION COMPLETE!\n",
      "Domain: finance\n",
      "Success Rate: 25.0%\n",
      "❌ Explanation message: No high-confidence extractions found for explanation\n"
     ]
    }
   ],
   "source": [
    "# Phase 6F: Test Explainable System\n",
    "\n",
    "def get_available_test_file():\n",
    "    \"\"\"Get any available test file for explainability\"\"\"\n",
    "    \n",
    "    test_files = []\n",
    "    \n",
    "    # Check large dataset folder\n",
    "    if os.path.exists(\"./large_scale_invoice_dataset\"):\n",
    "        dataset_files = list(Path(\"./large_scale_invoice_dataset\").glob(\"*.pdf\"))\n",
    "        test_files.extend([str(f) for f in dataset_files[:2]])\n",
    "    \n",
    "    # Return first available file\n",
    "    for file_path in test_files:\n",
    "        if os.path.exists(file_path):\n",
    "            return file_path\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def test_explainable_system():\n",
    "    \"\"\"Test explainability with fallback options\"\"\"\n",
    "    \n",
    "    # Get best available test file\n",
    "    test_file = get_available_test_file()\n",
    "    \n",
    "    if not test_file:\n",
    "        print(\"❌ No test files available, check your dataset paths\")\n",
    "        return\n",
    "    \n",
    "    print(f\" Testing explainability with: {os.path.basename(test_file)}\")\n",
    "    \n",
    "    try:\n",
    "        # Run with error handling\n",
    "        explainable_result = enhanced_multi_domain_extraction_with_explanations(\n",
    "            test_file,\n",
    "            explain_top_results=True,\n",
    "            max_explanations=1  # Reduced to prevent timeout\n",
    "        )\n",
    "        \n",
    "        # Display results with error checking\n",
    "        print(f\"\\n✅ EXPLAINABLE EXTRACTION COMPLETE!\")\n",
    "        \n",
    "        scalability = explainable_result.get('scalability_features', {})\n",
    "        multi_domain = explainable_result.get('multi_domain_extraction', {})\n",
    "        \n",
    "        print(f\"Domain: {scalability.get('domain_detected', 'unknown')}\")\n",
    "        print(f\"Success Rate: {multi_domain.get('success_rate', 0):.1%}\")\n",
    "        \n",
    "        # Check if explanations were generated\n",
    "        explainability = explainable_result.get('explainability', {})\n",
    "        if 'explained_extractions' in explainability:\n",
    "            print(f\" Explanations Generated: {explainability.get('explanation_count', 0)}\")\n",
    "            print(\" EXPLAINABLE AI INTEGRATION SUCCESSFUL!\")\n",
    "        else:\n",
    "            print(f\"❌ Explanation message: {explainability.get('message', 'No explanations')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌Explainability test failed: {e}\")\n",
    "        print(\" This might be due to model/memory limitations\")\n",
    "\n",
    "# Run the robust test\n",
    "test_explainable_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a84d37",
   "metadata": {},
   "source": [
    "**7.\tSecurity and Offline Operability**\n",
    "* Ensure the system runs in secure environments without requiring external cloud APIs or persistent internet access.\n",
    "* Design for deployment in edge devices or private infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae458979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keyring\n",
      "  Using cached keyring-25.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pywin32-ctypes>=0.2.0 (from keyring)\n",
      "  Using cached pywin32_ctypes-0.2.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting jaraco.classes (from keyring)\n",
      "  Using cached jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jaraco.functools (from keyring)\n",
      "  Using cached jaraco_functools-4.3.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jaraco.context (from keyring)\n",
      "  Using cached jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting more-itertools (from jaraco.classes->keyring)\n",
      "  Downloading more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\n",
      "Using cached keyring-25.6.0-py3-none-any.whl (39 kB)\n",
      "Using cached pywin32_ctypes-0.2.3-py3-none-any.whl (30 kB)\n",
      "Using cached jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
      "Using cached jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
      "Using cached jaraco_functools-4.3.0-py3-none-any.whl (10 kB)\n",
      "Downloading more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "Installing collected packages: pywin32-ctypes, more-itertools, jaraco.context, jaraco.functools, jaraco.classes, keyring\n",
      "\n",
      "   ------ --------------------------------- 1/6 [more-itertools]\n",
      "   --------------------------------- ------ 5/6 [keyring]\n",
      "   ---------------------------------------- 6/6 [keyring]\n",
      "\n",
      "Successfully installed jaraco.classes-3.4.0 jaraco.context-6.0.1 jaraco.functools-4.3.0 keyring-25.6.0 more-itertools-10.8.0 pywin32-ctypes-0.2.3\n",
      "✅ Secure Data Handler ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 7A: Security Framework and Encryption\n",
    "\n",
    "!pip install keyring\n",
    "import keyring\n",
    "import hashlib\n",
    "import secrets\n",
    "import base64\n",
    "from cryptography.fernet import Fernet\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "\n",
    "class SecureDataHandler:\n",
    "    \"\"\"Handles encryption/decryption of sensitive data and model files\"\"\"\n",
    "    \n",
    "    def __init__(self, password: str = None):\n",
    "        self.password = password or self._get_secure_password()\n",
    "        self.key = self._derive_key(self.password)\n",
    "        self.cipher = Fernet(self.key)\n",
    "        \n",
    "    def _get_secure_password(self):\n",
    "        \"\"\"Get password securely (in production, use proper key management)\"\"\"\n",
    "        try:\n",
    "            # Try to get from system keyring first\n",
    "            password = keyring.get_password(\"qa_extractor\", \"main_key\")\n",
    "            if not password:\n",
    "                password = getpass.getpass(\"Enter encryption password for secure storage: \")\n",
    "                keyring.set_password(\"qa_extractor\", \"main_key\", password)\n",
    "            return password\n",
    "        except:\n",
    "            # Fallback to environment or prompt\n",
    "            return os.environ.get(\"QA_EXTRACTOR_KEY\", \"secure_default_key_2024\")\n",
    "    \n",
    "    def _derive_key(self, password: str):\n",
    "        \"\"\"Derive encryption key from password\"\"\"\n",
    "        password_bytes = password.encode()\n",
    "        salt = b\"qa_extractor_salt_2024\"  # In production, use random salt per user\n",
    "        kdf = PBKDF2HMAC(\n",
    "            algorithm=hashes.SHA256(),\n",
    "            length=32,\n",
    "            salt=salt,\n",
    "            iterations=100000,\n",
    "        )\n",
    "        key = base64.urlsafe_b64encode(kdf.derive(password_bytes))\n",
    "        return key\n",
    "    \n",
    "    def encrypt_file(self, file_path: str, output_path: str = None):\n",
    "        \"\"\"Encrypt a file and save to output path\"\"\"\n",
    "        if not output_path:\n",
    "            output_path = file_path + \".encrypted\"\n",
    "            \n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        encrypted_data = self.cipher.encrypt(data)\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(encrypted_data)\n",
    "        \n",
    "        print(f\"Encrypted: {file_path} -> {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def decrypt_file(self, encrypted_path: str, output_path: str = None):\n",
    "        \"\"\"Decrypt a file and save to output path\"\"\"\n",
    "        if not output_path:\n",
    "            output_path = encrypted_path.replace(\".encrypted\", \"\")\n",
    "            \n",
    "        with open(encrypted_path, 'rb') as f:\n",
    "            encrypted_data = f.read()\n",
    "        \n",
    "        decrypted_data = self.cipher.decrypt(encrypted_data)\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(decrypted_data)\n",
    "        \n",
    "        print(f\"Decrypted: {encrypted_path} -> {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def encrypt_text(self, text: str) -> str:\n",
    "        \"\"\"Encrypt text data\"\"\"\n",
    "        return self.cipher.encrypt(text.encode()).decode()\n",
    "    \n",
    "    def decrypt_text(self, encrypted_text: str) -> str:\n",
    "        \"\"\"Decrypt text data\"\"\"\n",
    "        return self.cipher.decrypt(encrypted_text.encode()).decode()\n",
    "\n",
    "print(\"✅ Secure Data Handler ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0046ae13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Offline Model Manager ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 7B: Offline Model Manager\n",
    "\n",
    "class OfflineModelManager:\n",
    "    \"\"\"Manages model storage, loading, and security for offline operation\"\"\"\n",
    "    \n",
    "    def __init__(self, secure_storage_dir=\"./secure_models\"):\n",
    "        self.storage_dir = Path(secure_storage_dir)\n",
    "        self.storage_dir.mkdir(exist_ok=True)\n",
    "        self.security_handler = SecureDataHandler()\n",
    "        self.model_registry = self._load_model_registry()\n",
    "        \n",
    "    def _load_model_registry(self):\n",
    "        \"\"\"Load encrypted model registry\"\"\"\n",
    "        registry_path = self.storage_dir / \"model_registry.encrypted\"\n",
    "        if registry_path.exists():\n",
    "            try:\n",
    "                decrypted_data = self.security_handler.decrypt_text(\n",
    "                    registry_path.read_text()\n",
    "                )\n",
    "                return json.loads(decrypted_data)\n",
    "            except:\n",
    "                print(\" Could not decrypt model registry, creating new one\")\n",
    "        \n",
    "        return {\"models\": {}, \"active_model\": None, \"created_at\": datetime.now().isoformat()}\n",
    "    \n",
    "    def _save_model_registry(self):\n",
    "        \"\"\"Save encrypted model registry\"\"\"\n",
    "        registry_path = self.storage_dir / \"model_registry.encrypted\"\n",
    "        encrypted_data = self.security_handler.encrypt_text(\n",
    "            json.dumps(self.model_registry, indent=2)\n",
    "        )\n",
    "        registry_path.write_text(encrypted_data)\n",
    "    \n",
    "    def store_model_securely(self, model_path: str, model_name: str, description: str = \"\"):\n",
    "        \"\"\"Store model files with encryption\"\"\"\n",
    "        model_dir = self.storage_dir / model_name\n",
    "        model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Encrypt and store all model files\n",
    "        secured_files = {}\n",
    "        \n",
    "        for file_path in Path(model_path).glob(\"*\"):\n",
    "            if file_path.is_file():\n",
    "                encrypted_path = model_dir / f\"{file_path.name}.encrypted\"\n",
    "                self.security_handler.encrypt_file(str(file_path), str(encrypted_path))\n",
    "                secured_files[file_path.name] = str(encrypted_path)\n",
    "        \n",
    "        # Update registry\n",
    "        self.model_registry[\"models\"][model_name] = {\n",
    "            \"description\": description,\n",
    "            \"stored_at\": datetime.now().isoformat(),\n",
    "            \"files\": secured_files,\n",
    "            \"original_path\": str(model_path),\n",
    "            \"secure_path\": str(model_dir)\n",
    "        }\n",
    "        \n",
    "        self._save_model_registry()\n",
    "        print(f\" Model '{model_name}' stored securely with {len(secured_files)} files\")\n",
    "        return model_dir\n",
    "    \n",
    "    def load_model_securely(self, model_name: str, temp_dir: str = None):\n",
    "        \"\"\"Decrypt and load model for use\"\"\"\n",
    "        if model_name not in self.model_registry[\"models\"]:\n",
    "            raise ValueError(f\"Model '{model_name}' not found in secure storage\")\n",
    "        \n",
    "        model_info = self.model_registry[\"models\"][model_name]\n",
    "        \n",
    "        # Create temporary directory for decrypted files\n",
    "        if not temp_dir:\n",
    "            temp_dir = tempfile.mkdtemp(prefix=\"qa_model_\")\n",
    "        else:\n",
    "            Path(temp_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "        # Decrypt all model files to temp directory\n",
    "        for original_name, encrypted_path in model_info[\"files\"].items():\n",
    "            output_path = Path(temp_dir) / original_name\n",
    "            self.security_handler.decrypt_file(encrypted_path, str(output_path))\n",
    "        \n",
    "        print(f\" Model '{model_name}' loaded to temporary directory: {temp_dir}\")\n",
    "        return temp_dir\n",
    "    \n",
    "    def list_secure_models(self):\n",
    "        \"\"\"List all securely stored models\"\"\"\n",
    "        print(\"\\n SECURE MODEL REGISTRY:\")\n",
    "        for name, info in self.model_registry[\"models\"].items():\n",
    "            print(f\"  • {name}: {info['description']}\")\n",
    "            print(f\"    Stored: {info['stored_at']}\")\n",
    "            print(f\"    Files: {len(info['files'])}\")\n",
    "        \n",
    "        if self.model_registry[\"active_model\"]:\n",
    "            print(f\"\\n Active Model: {self.model_registry['active_model']}\")\n",
    "    \n",
    "    def cleanup_temp_files(self, temp_dir: str):\n",
    "        \"\"\"Securely delete temporary decrypted files\"\"\"\n",
    "        if Path(temp_dir).exists():\n",
    "            shutil.rmtree(temp_dir)\n",
    "            print(f\" Cleaned up temporary files: {temp_dir}\")\n",
    "\n",
    "print(\"✅ Offline Model Manager ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5295a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Air-Gapped Extractor ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 7C: Air-Gapped Environment Support\n",
    "\n",
    "class AirGappedExtractor(QABasedExtractor):\n",
    "    \"\"\"QA Extractor designed for completely offline/air-gapped environments\"\"\"\n",
    "    \n",
    "    def __init__(self, secure_model_name: str = None):\n",
    "        self.model_manager = OfflineModelManager()\n",
    "        self.temp_model_dir = None\n",
    "        self.model_name = secure_model_name or \"production_model\"\n",
    "        \n",
    "        # Load model from secure storage\n",
    "        self._load_secure_model()\n",
    "        \n",
    "    def _load_secure_model(self):\n",
    "        \"\"\"Load model from encrypted storage\"\"\"\n",
    "        try:\n",
    "            # Try to load from secure storage first\n",
    "            self.temp_model_dir = self.model_manager.load_model_securely(self.model_name)\n",
    "            \n",
    "            # Initialize QA pipeline with decrypted model\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=self.temp_model_dir,\n",
    "                tokenizer=self.temp_model_dir,\n",
    "                device=-1  # Force CPU for better compatibility\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Loaded secure model: {self.model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Could not load secure model, using base model: {e}\")\n",
    "            # Fallback to base model\n",
    "            super().__init__(model_name=\"deepset/xlm-roberta-large-squad2\")\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up temporary files when object is destroyed\"\"\"\n",
    "        if self.temp_model_dir:\n",
    "            self.model_manager.cleanup_temp_files(self.temp_model_dir)\n",
    "    \n",
    "    def extract_with_security_audit(self, text: str, questions: List[str]) -> Dict:\n",
    "        \"\"\"Extract with full security auditing\"\"\"\n",
    "        # Process document completely offline\n",
    "        # Log every action for security audit\n",
    "        # Add digital fingerprint to results\n",
    "        \n",
    "        \n",
    "        # Generate audit ID\n",
    "        audit_id = hashlib.sha256(\n",
    "            (text[:100] + str(datetime.now())).encode()\n",
    "        ).hexdigest()[:12]\n",
    "        \n",
    "        # Regular extraction\n",
    "        results = self.extract_with_questions(text, questions)\n",
    "        \n",
    "        # Add security metadata\n",
    "        security_info = {\n",
    "            'audit_id': audit_id,\n",
    "            'processing_mode': 'air_gapped',\n",
    "            'model_source': 'secure_storage' if self.temp_model_dir else 'fallback',\n",
    "            'data_hash': hashlib.sha256(text.encode()).hexdigest(),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'no_external_calls': True,\n",
    "            'encryption_used': True\n",
    "        }\n",
    "        \n",
    "        results['security_audit'] = security_info\n",
    "        return results\n",
    "\n",
    "print(\"✅ Air-Gapped Extractor ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0da0724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Edge-Optimized Extractor ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 7D: Edge Device Optimization\n",
    "\n",
    "class EdgeOptimizedExtractor:\n",
    "    \"\"\"Lightweight extractor optimized for edge devices and limited resources\"\"\"\n",
    "    \n",
    "    def __init__(self, model_size=\"small\", max_memory_mb=512):\n",
    "        self.max_memory_mb = max_memory_mb\n",
    "        self.model_size = model_size\n",
    "        \n",
    "        # Choose model based on resource constraints\n",
    "        if model_size == \"tiny\":\n",
    "            model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "        elif model_size == \"small\":\n",
    "            model_name = \"deepset/minilm-uncased-squad2\"\n",
    "        else:\n",
    "            model_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "        \n",
    "        # Initialize with memory optimization\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=model_name,\n",
    "            device=-1,  # CPU only for edge devices\n",
    "            batch_size=1,  # Process one at a time\n",
    "            max_length=256  # Reduced context length\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Edge-optimized extractor ready (model: {model_size})\")\n",
    "    \n",
    "    def extract_lightweight(self, text: str, max_questions: int = 5) -> Dict:\n",
    "        \"\"\"Memory-efficient extraction with limited questions\"\"\"\n",
    "        \n",
    "        # Truncate text if too long\n",
    "        if len(text) > 1000:\n",
    "            text = text[:1000] + \"...\"\n",
    "        \n",
    "        # Use only most important questions\n",
    "        important_questions = [\n",
    "            \"What is the main amount or total?\",\n",
    "            \"What is the document number?\",\n",
    "            \"What company or organization is mentioned?\",\n",
    "            \"What date is mentioned?\",\n",
    "            \"Who is the person or contact mentioned?\"\n",
    "        ][:max_questions]\n",
    "        \n",
    "        results = {}\n",
    "        for question in important_questions:\n",
    "            try:\n",
    "                result = self.qa_pipeline(\n",
    "                    question=question,\n",
    "                    context=text,\n",
    "                    max_answer_len=50  # Short answers only\n",
    "                )\n",
    "                \n",
    "                results[question] = {\n",
    "                    'answer': result['answer'],\n",
    "                    'confidence': result['score'],\n",
    "                    'extracted': result['score'] > 0.1  # Lower threshold for edge\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[question] = {'error': str(e), 'extracted': False}\n",
    "        \n",
    "        return {\n",
    "            'extractions': results,\n",
    "            'model_size': self.model_size,\n",
    "            'memory_optimized': True,\n",
    "            'edge_compatible': True\n",
    "        }\n",
    "\n",
    "print(\"✅ Edge-Optimized Extractor ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a38cb7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ❌ Could not load secure model, using base model: Model 'production_model' not found in secure storage\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Secure Offline System ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 7E: Secure Pipeline Integration\n",
    "\n",
    "class SecureOfflineSystem:\n",
    "    \"\"\"Combines everything into one complet secure, offline-capable extraction system\"\"\"\n",
    "    \n",
    "    def __init__(self, security_level=\"high\"):\n",
    "        self.security_level = security_level\n",
    "        self.security_handler = SecureDataHandler()\n",
    "        self.model_manager = OfflineModelManager()\n",
    "        \n",
    "        # Initialize appropriate extractor based on security level\n",
    "        if security_level == \"high\":\n",
    "            self.extractor = AirGappedExtractor()\n",
    "        elif security_level == \"edge\":\n",
    "            self.extractor = EdgeOptimizedExtractor()\n",
    "        else:\n",
    "            self.extractor = QABasedExtractor()\n",
    "        \n",
    "        # Security audit log\n",
    "        self.audit_log = []\n",
    "        \n",
    "    def process_document_securely(self, file_path: str, delete_after_processing: bool = True):\n",
    "        \"\"\"Process document with full security measures\"\"\"\n",
    "        # Generate unique ID for this processing\n",
    "        # Extract data with chosen security level\n",
    "        # Log all security events\n",
    "        # Optionally delete original file\n",
    "        # Encrypt results before saving\n",
    "        \n",
    "        \n",
    "        # Generate processing ID\n",
    "        process_id = secrets.token_hex(8)\n",
    "        \n",
    "        try:\n",
    "            # Log start of processing\n",
    "            self._log_security_event(\"PROCESSING_START\", {\n",
    "                'process_id': process_id,\n",
    "                'file_name': Path(file_path).name,\n",
    "                'file_size': Path(file_path).stat().st_size,\n",
    "                'security_level': self.security_level\n",
    "            })\n",
    "            \n",
    "            # Load and normalize document\n",
    "            document_data = load_and_normalize(file_path)\n",
    "            \n",
    "            # Extract text content\n",
    "            all_text = \"\"\n",
    "            for content_item in document_data['content']:\n",
    "                all_text += content_item['text'] + \" \"\n",
    "            \n",
    "            # Secure extraction\n",
    "            if hasattr(self.extractor, 'extract_with_security_audit'):\n",
    "                extraction_results = self.extractor.extract_with_security_audit(\n",
    "                    all_text, \n",
    "                    self.extractor.extraction_templates['german_invoice']\n",
    "                )\n",
    "            else:\n",
    "                extraction_results = self.extractor.extract_information(all_text)\n",
    "            \n",
    "            # Add security metadata\n",
    "            extraction_results['security_info'] = {\n",
    "                'process_id': process_id,\n",
    "                'security_level': self.security_level,\n",
    "                'offline_mode': True,\n",
    "                'encryption_available': True,\n",
    "                'audit_trail': len(self.audit_log)\n",
    "            }\n",
    "            \n",
    "            # Log successful processing\n",
    "            self._log_security_event(\"PROCESSING_SUCCESS\", {\n",
    "                'process_id': process_id,\n",
    "                'extractions_found': extraction_results.get('successful_extractions', 0)\n",
    "            })\n",
    "            \n",
    "            # Optionally delete source file for security\n",
    "            if delete_after_processing:\n",
    "                os.remove(file_path)\n",
    "                self._log_security_event(\"FILE_DELETED\", {'process_id': process_id})\n",
    "            \n",
    "            return extraction_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._log_security_event(\"PROCESSING_ERROR\", {\n",
    "                'process_id': process_id,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            raise\n",
    "    \n",
    "    def _log_security_event(self, event_type: str, details: dict):\n",
    "        \"\"\"Log security events for audit trail\"\"\"\n",
    "        event = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'event_type': event_type,\n",
    "            'details': details,\n",
    "            'system_user': os.getenv('USERNAME', 'unknown')\n",
    "        }\n",
    "        \n",
    "        self.audit_log.append(event)\n",
    "        \n",
    "        # Also log to secure file\n",
    "        log_file = Path(\"security_audit.log\")\n",
    "        with open(log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(event) + \"\\n\")\n",
    "    \n",
    "    def export_secure_results(self, results: dict, output_path: str):\n",
    "        \"\"\"Export results with encryption\"\"\"\n",
    "        \n",
    "        # Convert results to JSON\n",
    "        results_json = json.dumps(results, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Encrypt the results\n",
    "        encrypted_path = output_path + \".encrypted\"\n",
    "        encrypted_data = self.security_handler.encrypt_text(results_json)\n",
    "        \n",
    "        with open(encrypted_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(encrypted_data)\n",
    "        \n",
    "        print(f\" ✅ Results exported securely to: {encrypted_path}\")\n",
    "        return encrypted_path\n",
    "    \n",
    "    def get_security_audit_report(self):\n",
    "        \"\"\"Generate security audit report\"\"\"\n",
    "        report = {\n",
    "            'total_events': len(self.audit_log),\n",
    "            'security_level': self.security_level,\n",
    "            'recent_events': self.audit_log[-10:],  # Last 10 events\n",
    "            'event_types': {},\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Count event types\n",
    "        for event in self.audit_log:\n",
    "            event_type = event['event_type']\n",
    "            report['event_types'][event_type] = report['event_types'].get(event_type, 0) + 1\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize secure system\n",
    "secure_system = SecureOfflineSystem(security_level=\"high\")\n",
    "print(\"✅ Secure Offline System ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b730d",
   "metadata": {},
   "source": [
    "❌ Could not load secure model, using base model... = It's refusing to load a model that doesn't exist in secure storage and falling back safely to a known model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ad1b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TESTING SECURE OFFLINE SYSTEM\n",
      "==================================================\n",
      "Processing pdf file: test_secure_processing.pdf\n",
      "\n",
      "✅ SECURE PROCESSING RESULTS:\n",
      "   Security Level: high\n",
      "   Process ID: 54ef4048ba734541\n",
      "   Offline Mode: True\n",
      "   Extractions Found: 0\n",
      " ✅ Results exported securely to: secure_extraction_results.json.encrypted\n",
      "\n",
      " SECURITY AUDIT:\n",
      "   Total Events: 3\n",
      "   Event Types: {'PROCESSING_START': 1, 'PROCESSING_SUCCESS': 1, 'FILE_DELETED': 1}\n",
      "\n",
      " ✅ SECURE OFFLINE SYSTEM TEST COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# 7F: Test Complete Secure System\n",
    "\n",
    "def test_secure_offline_system():\n",
    "    \"\"\"Test the complete secure, offline system\"\"\"\n",
    "    \n",
    "    print(\" TESTING SECURE OFFLINE SYSTEM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test file\n",
    "    test_file = r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\\SWME_Rechnung_07122020.pdf\"\n",
    "    \n",
    "    if os.path.exists(test_file):\n",
    "        # Make a copy for testing (don't delete original)\n",
    "        test_copy = \"test_secure_processing.pdf\"\n",
    "        shutil.copy2(test_file, test_copy)\n",
    "        \n",
    "        try:\n",
    "            # Process with secure system\n",
    "            results = secure_system.process_document_securely(\n",
    "                test_copy, \n",
    "                delete_after_processing=True  # Will delete the copy\n",
    "            )\n",
    "            \n",
    "            print(\"\\n✅ SECURE PROCESSING RESULTS:\")\n",
    "            print(f\"   Security Level: {results['security_info']['security_level']}\")\n",
    "            print(f\"   Process ID: {results['security_info']['process_id']}\")\n",
    "            print(f\"   Offline Mode: {results['security_info']['offline_mode']}\")\n",
    "            print(f\"   Extractions Found: {results.get('successful_extractions', 0)}\")\n",
    "            \n",
    "            # Show some extractions\n",
    "            if 'extractions' in results:\n",
    "                print(\"\\n SAMPLE EXTRACTIONS:\")\n",
    "                count = 0\n",
    "                for question, result in results['extractions'].items():\n",
    "                    if result.get('answer') and count < 3:\n",
    "                        print(f\"   Q: {question[:50]}...\")\n",
    "                        print(f\"   A: {result['answer']} (conf: {result['confidence']:.3f})\")\n",
    "                        count += 1\n",
    "            \n",
    "            # Export results securely\n",
    "            encrypted_output = secure_system.export_secure_results(\n",
    "                results, \n",
    "                \"secure_extraction_results.json\"\n",
    "            )\n",
    "            \n",
    "            # Show security audit\n",
    "            audit_report = secure_system.get_security_audit_report()\n",
    "            print(f\"\\n SECURITY AUDIT:\")\n",
    "            print(f\"   Total Events: {audit_report['total_events']}\")\n",
    "            print(f\"   Event Types: {audit_report['event_types']}\")\n",
    "            \n",
    "            print(\"\\n ✅ SECURE OFFLINE SYSTEM TEST COMPLETE!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in secure processing: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ Test file not found - update path for testing\")\n",
    "\n",
    "# Run the test\n",
    "test_secure_offline_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selflearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
