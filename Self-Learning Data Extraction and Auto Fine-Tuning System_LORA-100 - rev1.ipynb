{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6a0788",
   "metadata": {},
   "source": [
    "## Self-Learning Data Extraction and Auto Fine-Tuning System for Structured and Unstructured Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f61df0",
   "metadata": {},
   "source": [
    "**1. Unified Data Handling**\n",
    "* Develop techniques to process and normalize structured (e.g., CSV, SQL, PDFs), unstructured (e.g., text, images), and streaming data from diverse sources.\n",
    "* Enable seamless integration with offline files, legacy datasets, and live data pipelines (e.g., via data federation or APIs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be5f127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (45.0.7)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
      "Requirement already satisfied: xlrd in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: torch in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: Pillow in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: seaborn in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from seaborn) (2.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: shap in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.48.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (2.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (0.61.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-learn->shap) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: lime in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.2.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (3.10.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (1.16.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (1.7.1)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from lime) (0.25.2)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (3.5)\n",
      "Requirement already satisfied: pillow>=10.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (11.1.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (2025.8.28)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-image>=0.12->lime) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-learn>=0.18->lime) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from matplotlib->lime) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from tqdm->lime) (0.4.6)\n",
      "Requirement already satisfied: peft in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (4.56.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from peft) (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=1.13.0->peft) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from transformers->peft) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from transformers->peft) (0.22.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (0.47.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from bitsandbytes) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from bitsandbytes) (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from torch>=2.0.0->accelerate) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "# all installs\n",
    "\n",
    "!pip install pdfplumber\n",
    "!pip install xlrd \n",
    "!pip install transformers>=4.21.0\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install pytesseract Pillow \n",
    "!pip install matplotlib seaborn\n",
    "!pip install shap\n",
    "!pip install lime\n",
    "\n",
    "\n",
    "\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5dfae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necesary libraries\n",
    "\n",
    "# Fix protobufs:\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\" \n",
    "os.environ[\"TRANSFORMERS_NO_SLOW_TOKENIZER\"] = \"1\"\n",
    "\n",
    "# Tesseract\n",
    "try:\n",
    "    import pytesseract\n",
    "    pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "except ImportError:\n",
    "    print(\"pytesseract not installed yet\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import sqlite3 \n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import torch, numpy, spacy\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from difflib import SequenceMatcher \n",
    "\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e38b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAFE_RUN = True | INTERACTIVE = False\n"
     ]
    }
   ],
   "source": [
    "# Safety switches for Run All\n",
    "\n",
    "SAFE_RUN = True       # Skip heavy/long steps (fine-tune) on first full run\n",
    "INTERACTIVE = False   # Avoid input() prompts during Run All\n",
    "print(\"SAFE_RUN =\", SAFE_RUN, \"| INTERACTIVE =\", INTERACTIVE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75315e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Simple type detection function \n",
    "\n",
    "# This function determines the file type based on the file extension\n",
    "\n",
    "def detect_file_type(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        return \"csv\"\n",
    "    elif ext == \".pdf\":\n",
    "        return \"pdf\"\n",
    "    elif ext == \".txt\":\n",
    "        return \"text\"\n",
    "    elif ext in [\".xlsx\", \".xls\"]:  \n",
    "        return \"excel\"\n",
    "    elif ext == \".db\": \n",
    "        return \"sqlite\"\n",
    "    elif ext in [\".png\", \".jpg\", \".jpeg\", \".tiff\", \".bmp\"]: \n",
    "        return \"image\"\n",
    "    else:\n",
    "        return \"unsupported\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1895bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File loading functions for different data types \n",
    "# These functions handle the actual data extraction from supported file formats\n",
    "# Each loader returns data in a standardized format for downstream processing\n",
    "\n",
    "def load_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.to_dict(orient=\"records\")  # returns list of row dictionaries\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    data = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                data.append(text)\n",
    "    return {\"pages\": data}\n",
    "\n",
    "def load_excel(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    return df.to_dict(orient=\"records\")\n",
    "\n",
    "def load_sqlite(file_path, table_name):\n",
    "    conn = sqlite3.connect(file_path)\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    return df.to_dict(orient=\"records\")\n",
    "\n",
    "def load_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return {\"text\": f.read()}\n",
    "    \n",
    "def load_image_with_ocr(file_path):\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        import pytesseract\n",
    "        \n",
    "        # Set Tesseract path for Windows\n",
    "        pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "        \n",
    "        image = Image.open(file_path)\n",
    "        # Enhanced OCR \n",
    "        text = pytesseract.image_to_string(\n",
    "            image, \n",
    "            config='--oem 3 --psm 6 -l eng+deu'  # Multi-language support\n",
    "        )\n",
    "        return {\"extracted_text\": text.strip(), \"source\": \"ocr\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"OCR failed: {str(e)}\", \"extracted_text\": \"\"}\n",
    "\n",
    "\n",
    "def load_file(file_path):\n",
    "    file_type = detect_file_type(file_path)\n",
    "    if file_type == \"csv\":\n",
    "        return load_csv(file_path)\n",
    "    elif file_type == \"pdf\":\n",
    "        return load_pdf(file_path)\n",
    "    elif file_type == \"text\":\n",
    "        return load_text(file_path)\n",
    "    elif file_type == \"excel\":  \n",
    "        return load_excel(file_path)  \n",
    "    elif file_type == \"sqlite\": \n",
    "        return load_sqlite(file_path) \n",
    "    elif file_type == \"image\":\n",
    "        return load_image_with_ocr(file_path)\n",
    "    else: \n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad26fb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '# Predicting Train Delays Using Deutsche Bahn (DB) Data\\n\\n## Overview\\n\\nThis project tackles the problem of predicting train arrival delays using \\nhistorical data from Deutsche Bahn (DB), Germany\\'s national railway. We approach the problem from two perspectives:\\n\\n- **Classification**: Will a train be delayed by more than 6 minutes?\\n- **Regression**: How many minutes will a train be delayed?\\n\\nThe goal is to support better scheduling decisions, optimize passenger information systems, and demonstrate the effectiveness\\nof supervised machine learning for real-world transportation challenges.\\n\\n\\n## Project Structure\\n\\nPredicting-Train-Delays/\\n├── README.md # Main project README (this file)\\n├── requirements.txt # List of required Python packages\\n├── DBtrainrides.csv # Dataset (to be downloaded)\\n\\n├── Classification/ # Classification task\\n│ ├── ML_Final_Classification.ipynb\\n│ └── README.md\\n\\n└── Regression/ # Regression task\\n├── ML_Final_Regression.ipynb\\n└── README.md\\n\\n## Dataset\\n\\n- Over **2 million rows** of DB train ride data\\n\\n- Columns include:\\n  - Scheduled vs. actual arrival/departure times\\n  - Delay flags\\n  - Station metadata\\n  - Geolocation, time, and date info\\n\\n- **Target variables:**\\n  - \"is_late\" (Classification): whether the train is >6 minutes late\\n  - \"delay_minutes\" (Regression): number of minutes delayed\\n\\nThe dataset is not stored in this repo due to GitHub size limitations.\\n\\nDownload it here: https://www.kaggle.com/datasets/nokkyu/deutsche-bahn-db-delays\\n\\n\\n## ML Tasks & Models\\n\\n### Classification Task\\n\\n> [See detailed Classification README →](Classification/README.md)\\n\\n**Goal:** Predict whether a train will be late (delay > 6 minutes)\\n\\n- Preprocessing:\\n  - Feature engineering (hour, weekday, month, etc.)\\n  - SMOTE for class imbalance\\n  - Encoding: frequency + one-hot\\n  - Scaling and missing value imputation\\n\\n- Models:\\n  - Logistic Regression\\n  - Random Forest\\n  - MLP (Neural Network)\\n  - XGBoost\\n\\n- Best Performance:\\n  - **XGBoost** showed highest ROC-AUC\\n  - **MLP** had best recall (which is important for detecting delays)\\n\\n### Regression Task\\n\\n> [See detailed Regression README →](Regression/README.md)\\n\\n**Goal:** Predict the number of minutes a train will be delayed\\n\\n- Steps:\\n  - Full EDA with heatmaps, charts\\n  - Data cleaning, datetime handling\\n  - New feature creation\\n  - Encoding + scaling\\n\\n- Models:\\n  - Linear Regression\\n  - Random Forest Regressor\\n  - XGBoost Regressor\\n  - MLP Regressor\\n\\n- Best Performance:\\n  - **MLP Regressor** achieved **R² = 0.93**\\n  - Demonstrated strong predictive power for delay estimation\\n\\n## Setup & Dependencies\\n\\n### Requirements\\n\\n```bash\\npip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn xgboost tensorflow\\nCompatible with Python 3.8+\\n\\nYou can also find all dependencies in requirements.txt.\\n\\n**How to Run**\\n\\nClone the repo and download the dataset (DBtrainrides.csv)\\n\\nUpdate the file path in both notebooks\\n\\nOpen notebooks in Jupyter or VS Code\\n\\nRun cells in order to reproduce analysis and results\\n\\n\\n**Notes**\\n\\nThe dataset is highly imbalanced; classification models prioritized recall\\n\\nRegression models included leakage prevention, model saving, and runtime tracking\\n\\nAll notebooks are fully annotated for readability and reproducibility\\n\\n**Authors**\\n\\nAna Bernal — ana.bernal@ue-germany.de\\n\\nAslican Alacal — aslican.alacal@ue-germany.de'}\n"
     ]
    }
   ],
   "source": [
    "# Testing text file\n",
    "\n",
    "file_path = \"example.txt\"\n",
    "data = load_file(r\"C:\\Users\\aslia\\OneDrive\\Desktop\\github\\Predicting-Train-Delays\\README.txt\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d267886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_live_data(api_url, headers=None, timeout=10):\n",
    "    try:\n",
    "        response = requests.get(api_url, headers=headers, timeout=timeout)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Handle different content types\n",
    "            content_type = response.headers.get('content-type', '')\n",
    "            \n",
    "            if 'json' in content_type:\n",
    "                data = response.json()\n",
    "            else:\n",
    "                data = response.text\n",
    "                \n",
    "            return [{\n",
    "                \"file_name\": f\"api_data_{int(time.time())}\",  # Unique timestamp\n",
    "                \"file_type\": \"json\" if 'json' in content_type else \"text\",\n",
    "                \"source\": \"live_api\",\n",
    "                \"content\": [{\n",
    "                    \"section_id\": 0,\n",
    "                    \"text\": str(data),\n",
    "                    \"metadata\": {\n",
    "                        \"source\": api_url,\n",
    "                        \"timestamp\": time.time(),\n",
    "                        \"status_code\": response.status_code\n",
    "                    }\n",
    "                }]\n",
    "            }]\n",
    "        else:\n",
    "            raise Exception(f\"API returned status code: {response.status_code}\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to fetch live data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0687f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Extracted Content\n",
    "\n",
    "# These functions convert extracted data into a standardized format for consistent processing\n",
    "# All functions return a list of dictionaries with 'section_id', 'text', and 'metadata' fields\n",
    "\"\"\"Detects file type cals the appropriate normalization function and\n",
    "Returns unified document structure\"\"\"\n",
    "\n",
    "def normalize_csv(file_path):  # Normalizes CSV data by converting each row into a standardized content block.\n",
    "    df = pd.read_csv(file_path)\n",
    "    content = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content.append({ \"section_id\":idx, \n",
    "                        \"text\":str(row.to_dict()), \"metadata\": {\"row\": idx} # Convert row data to string representation\n",
    "                        }\n",
    "                       )\n",
    "    return content\n",
    "\n",
    "def normalize_pdf(file_path): # All functions return a list of dictionaries with 'section_id', 'text', and 'metadata' fields\n",
    "    content = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:   # Only include pages with extractable text\n",
    "                content.append({\n",
    "                    \"section_id\": i,\n",
    "                    \"text\": text,\n",
    "                    \"metadata\": {\"page\": i + 1}\n",
    "                })\n",
    "    return content\n",
    "\n",
    "def normalize_text(file_path): # Normalizes text files by treating each non-empty line as a separate content section.\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    content = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():  # skip blank lines\n",
    "            content.append({\n",
    "                \"section_id\": i,\n",
    "                \"text\": line.strip(),\n",
    "                \"metadata\": {\"line\": i + 1}\n",
    "            })\n",
    "    return content\n",
    "\n",
    "def normalize_excel(file_path): # Excel loader legacy dataset\n",
    "    df = pd.read_excel(file_path)\n",
    "    content = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content.append({\n",
    "            \"section_id\": idx,\n",
    "            \"text\": str(row.to_dict()),\n",
    "            \"metadata\": {\"row\": idx}\n",
    "        })\n",
    "    return content\n",
    "\n",
    "\n",
    "# SQLite support  dataset\n",
    "\n",
    "def normalize_sqlite(db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    content = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content.append({\n",
    "            \"section_id\": idx,\n",
    "            \"text\": str(row.to_dict()),\n",
    "            \"metadata\": {\"row\": idx}\n",
    "        })\n",
    "    return content\n",
    "\n",
    "\n",
    "# image normalization function\n",
    "\n",
    "def normalize_image(file_path):\n",
    "    ocr_result = load_image_with_ocr(file_path)\n",
    "    \n",
    "    if ocr_result.get(\"error\"):\n",
    "        return [{\"section_id\": 0, \"text\": \"\", \"metadata\": {\"error\": ocr_result[\"error\"]}}]\n",
    "    \n",
    "    text = ocr_result[\"extracted_text\"]\n",
    "    if not text:\n",
    "        return [{\"section_id\": 0, \"text\": \"\", \"metadata\": {\"error\": \"No text found\"}}]\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    content = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():\n",
    "            content.append({\n",
    "                \"section_id\": i,\n",
    "                \"text\": line.strip(),\n",
    "                \"metadata\": {\"line\": i + 1, \"source\": \"ocr\"}\n",
    "            })\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b953d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Master function that loads and normalizes any supported file type.\n",
    "\n",
    "# Creates a unified data structure regardles of input file format.\n",
    "    \n",
    "def load_and_normalize(file_path, table_name=None):\n",
    "    \n",
    "    try:  # Check if file exists\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError (f\"File not found: {file_path}\")\n",
    "    \n",
    "        file_type = detect_file_type(file_path)\n",
    "        file_name = os.path.basename(file_path) # Extract filename without path\n",
    "        print(f\"Processing {file_type} file: {file_name}\")\n",
    "        \n",
    "# appropriatee normalization function based on file type\n",
    "\n",
    "        if file_type == 'csv':\n",
    "            content = normalize_csv(file_path)\n",
    "        elif file_type == 'pdf':\n",
    "            content = normalize_pdf(file_path)\n",
    "        elif file_type == 'text':\n",
    "            content = normalize_text(file_path)\n",
    "        elif file_type == 'excel':\n",
    "            content = normalize_excel(file_path)\n",
    "        elif file_type == 'sqlite':\n",
    "            if not table_name:\n",
    "                raise ValueError(\"sqlite files required a table_name\")\n",
    "            content = normalize_sqlite(file_path, table_name)\n",
    "        elif file_type == 'image': \n",
    "            content = normalize_image(file_path)\n",
    "    \n",
    "        elif file_type == 'unsupported':\n",
    "            raise ValueError(f\"Unsupported file type for: {file_name}\")\n",
    "    \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type:{file_type}\")\n",
    "    \n",
    "# Return standardized document structure                        \n",
    "        return { \n",
    "                \"file_name\": file_name,\n",
    "                \"file_type\": file_type,\n",
    "                \"source\": \"offline\",\n",
    "                \"content\": content,\n",
    "                \"processed_at\": time.time() # Adds timestamp\n",
    "            }\n",
    "    except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a01d097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pdf file: Rechnung.pdf\n",
      "{'file_name': 'Rechnung.pdf', 'file_type': 'pdf', 'source': 'offline', 'content': [{'section_id': 0, 'text': 'DEUBA GmbH & Co. KG | Zum Wiesenhof 84 | 66663 Merzig Rechnung\\nNordson\\nAhmed Ebada Datum 06.02.2020\\nKapellenstr 12 Rechnungsnummer 1407606058\\nKundenreferenz 1226475257\\n85622 Feldkirchen\\nKundennummer 21144538789\\nLieferdatum 05.02.2020\\nSeite 1 von 1\\nPosBeschreibung Menge Preis Betrag\\nEUR EUR\\n1 191474 1 13,95 13,95\\nMülleimer mit Schiebedeckel Kunststoff Silber 50 Liter\\nVersandart: DPD\\nGesamtnettowert (EUR) 11,72\\nMehrwertsteuer 19,00% 2,23\\nGesamtbruttowert (EUR) 13,95\\nRechnungsdatum = Lieferdatum\\nDiese Rechnung ist Bestandteil Ihres Auftrags vom 05.02.2020\\nNur die in diesem Dokument als solche erkennbaren Produkte sind FSC zertifiziert.\\nFSC Zertifizierungsnummer TSUD-COC-000791\\nWir haben Ihre Zahlung am 05.02.2020 erhalten.\\nZahlungsbedingung: ManoMano\\nEs gelten unsere Ihnen bekannten Allgemeinen Geschäftsbedingungen.\\nwww.deubaservice.de\\nRechnungsnummer:\\n1407606058\\nWir sind 24 Stunden am Tag für Sie da!\\nBearbeiten Sie Reklamationen ganz bequem Postleitzahl:\\nvon zu Hause aus - ohne Öffnungszeiten 85622\\nDEUBA GmbH & Co. KG HRA 9472 Kontakt: Bank:\\nZum Wiesenhof 84 e-Mail: kontakt@deuba.info Bank1Saar\\n66663 Merzig , Germany USt-ID: DE815720919 Fon: +49 6861 9010000 IBAN: DE54591900000117582000\\nGeschäftsführer: Steuernummer: 040/152/07650 Fax: +49 6861 9010099 BIC: SABADE5SXXX\\nHenning Valentin, Thorsten Schneider, Sitz Gesellschaft: Merzig Internet: www.deuba.info Konto: 117582000\\nMarius Friedrich Registergericht: Saarbrücken Shop: www.deuba24online.de BLZ: 591 900 00', 'metadata': {'page': 1}}], 'processed_at': 1758209477.2730706}\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "data = load_and_normalize(r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\Rechnung.pdf\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9c8be",
   "metadata": {},
   "source": [
    "**2. Intelligent Information Extraction**\n",
    "* Design or adapt machine learning and NLP models to automatically extract key entities, values, and patterns from heterogeneous data.\n",
    "* Support document parsing, table recognition, entity linking, and contextual extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d86690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding missing imports\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20a55320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QABasedExtractor\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "class QABasedExtractor:\n",
    "    def __init__(self, model_name=\"deepset/xlm-roberta-large-squad2\", local_dir=\"./lora_fine_tuned_model\"):\n",
    "        \n",
    "        # Always define to avoid AttributeError\n",
    "        self.ner_pipeline = None\n",
    "        \n",
    "        # Check for LoRA fine-tuned model\n",
    "        if (os.path.isdir(local_dir) and \n",
    "            os.path.exists(os.path.join(local_dir, \"adapter_config.json\"))):\n",
    "            \n",
    "            print(f\"Loading LoRA fine-tuned model from {local_dir}\")\n",
    "            \n",
    "            # Load base model and tokenizer separately\n",
    "            base_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)  # Use base model tokenizer\n",
    "            \n",
    "            # Load LoRA adapter\n",
    "            model = PeftModel.from_pretrained(base_model, local_dir)\n",
    "            \n",
    "            # Create pipeline with both model and tokenizer\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,  # Explicitly provide tokenizer\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            \n",
    "        elif (os.path.isdir(local_dir) and \n",
    "              os.path.exists(os.path.join(local_dir, \"config.json\"))):\n",
    "            \n",
    "            print(f\"Loading full fine-tuned model from {local_dir}\")\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=local_dir,\n",
    "                tokenizer=local_dir,\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Loading base model: {model_name}\")\n",
    "    \n",
    "            # Load model and tokenizer explicitly\n",
    "            base_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    \n",
    "            # Create pipeline with both model and tokenizer\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=base_model,\n",
    "                tokenizer=tokenizer,  # Explicitly provide tokenizer\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "        \n",
    "        self.model_name = local_dir if os.path.isdir(local_dir) else model_name\n",
    "\n",
    "        # Updated question templates with better targeting\n",
    "        \n",
    "        self.extraction_templates = {\n",
    "            \"invoice\": [\n",
    "                \"What number appears after 'Invoice Number' or 'Rechnungsnummer'?\",\n",
    "                \"What amount appears after 'Total' or 'Gesamtbetrag' or 'Endbetrag'?\",\n",
    "                \"What company name appears at the top of the document?\",\n",
    "                \"What date appears after 'Invoice Date' or 'Rechnungsdatum'?\",\n",
    "                \"What is the largest monetary amount mentioned?\",\n",
    "                \"What customer name appears on the invoice?\",\n",
    "                \"What tax amount is mentioned?\",\n",
    "                \"What is the net amount before tax?\"\n",
    "            ],\n",
    "            \"german_invoice\": [\n",
    "                \"Welche Nummer steht nach 'Rechnungsnummer'?\",\n",
    "                \"Welcher Betrag steht nach 'Gesamtbetrag' oder 'Endbetrag'?\",\n",
    "                \"Wie heißt die Firma auf der Rechnung?\",\n",
    "                \"Welches Datum steht nach 'Rechnungsdatum'?\",\n",
    "                \"What number appears after 'Rechnungsnummer'?\",\n",
    "                \"What amount appears after 'Gesamtbetrag' or 'Total'?\",\n",
    "                \"What company name is mentioned?\",\n",
    "                \"What date appears after 'Rechnungsdatum'?\",\n",
    "                \"What is the highest amount in Euro mentioned?\",\n",
    "                \"What is the MwSt or USt amount?\",\n",
    "                \"What services or products are listed?\",\n",
    "                \"What is the invoice number?\",\n",
    "                \"What is the Rechnungsnummer?\",\n",
    "                \"What is the total amount?\",\n",
    "                \"What is the Gesamtbetrag?\", \n",
    "                \"What is the Endbetrag?\",\n",
    "                \"What is the vendor name?\",\n",
    "                \"What is the company name?\",\n",
    "                \"What is the Firmenname?\",\n",
    "                \"What is the invoice date?\",\n",
    "                \"What is the Rechnungsdatum?\",\n",
    "                \"What is the billing date?\",\n",
    "                \"Who is the customer?\",\n",
    "                \"What is the Kunde?\",\n",
    "                \"What services were provided?\",\n",
    "                \"What is the Leistung?\",\n",
    "                \"What is the tax amount?\",\n",
    "                \"What is the Mehrwertsteuer?\",\n",
    "                \"What is the USt?\",\n",
    "                \"What is the net amount?\",\n",
    "                \"What is the Nettobetrag?\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"What are the most important numbers in this document?\",\n",
    "                \"What companies or organizations are mentioned?\",\n",
    "                \"What dates are mentioned?\",\n",
    "                \"What monetary amounts are mentioned?\",\n",
    "                \"What are the key facts in this document?\",\n",
    "                \"What is the main topic or subject of this document?\",\n",
    "                \"What names of people are mentioned?\",\n",
    "                \"What locations or addresses are mentioned?\",\n",
    "                \"What email addresses or contact information is provided?\",\n",
    "                \"What phone numbers are listed?\",\n",
    "                \"What percentages or statistics are mentioned?\",\n",
    "                \"What products or services are described?\",\n",
    "                \"What deadlines or time periods are mentioned?\",\n",
    "                \"What requirements or specifications are listed?\",\n",
    "                \"What actions or tasks are described?\",\n",
    "                \"What problems or issues are identified?\",\n",
    "                \"What solutions or recommendations are provided?\",\n",
    "                \"What project names or codes are mentioned?\",\n",
    "                \"What versions or releases are referenced?\",\n",
    "                \"What technologies or tools are discussed?\",\n",
    "                \"What departments or teams are mentioned?\",\n",
    "                \"What metrics or measurements are provided?\",\n",
    "                \"What goals or objectives are stated?\",\n",
    "                \"What risks or concerns are identified?\",\n",
    "                \"What benefits or advantages are highlighted?\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Enhanced preprocessing for invoices\"\"\"\n",
    "        # basic cleaning\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        # invoice-specific preprocessing\n",
    "        # context markers to help AI understand\n",
    "        text = re.sub(r'(Rechnungsnummer|Invoice Number)[\\s:]*([A-Z0-9\\-]+)', \n",
    "                      r'The invoice number is \\2. Rechnungsnummer: \\2', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        text = re.sub(r'(Gesamtbetrag|Total|Endbetrag)[\\s:]*([€$]?\\s*[\\d,\\.]+)', \n",
    "                      r'The total amount is \\2. Gesamtbetrag: \\2', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # limit length\n",
    "        if len(text) > 2000:\n",
    "            text = text[:2000] + \"...\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_entities_with_ner(self, text: str) -> Dict:\n",
    "        \"\"\"Enhanced NER extraction\"\"\"\n",
    "        ner = getattr(self, \"ner_pipeline\", None)\n",
    "        if not ner:\n",
    "            return {\n",
    "                'organizations': [],\n",
    "                'money': [],\n",
    "                'dates': [],\n",
    "                'persons': []\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            entities = ner(text)\n",
    "            processed = {'organizations': [], 'money': [], 'dates': [], 'persons': []}\n",
    "            for entity in entities:\n",
    "                entity_type = entity.get('entity_group', '')\n",
    "                word = entity.get('word', '').strip()\n",
    "                if entity_type == 'ORG' and len(word) > 2:\n",
    "                    processed['organizations'].append(word)\n",
    "                elif entity_type == 'PER' and len(word) > 2:\n",
    "                    processed['persons'].append(word)\n",
    "            return processed\n",
    "        except Exception:\n",
    "            return {}\n",
    "        \n",
    "    def extract_with_questions(self, text: str, questions: List[str], \n",
    "                             confidence_threshold: float = 0.05) -> Dict:  # Lower threshold\n",
    "        text = self.preprocess_text(text)\n",
    "        results = {}\n",
    "        \n",
    "        for question in questions:\n",
    "            try:\n",
    "                qa_result = self.qa_pipeline(\n",
    "                    question=question,\n",
    "                    context=text,\n",
    "                    max_answer_len=150  # Longer answers\n",
    "                )\n",
    "                \n",
    "                results[question] = {\n",
    "                    'answer': qa_result['answer'] if qa_result['score'] >= confidence_threshold else None,\n",
    "                    'confidence': qa_result['score'],\n",
    "                    'start_pos': qa_result.get('start', 0),\n",
    "                    'end_pos': qa_result.get('end', 0),\n",
    "                    'extracted': qa_result['score'] >= confidence_threshold\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[question] = {\n",
    "                    'answer': None,\n",
    "                    'confidence': 0.0,\n",
    "                    'error': str(e),\n",
    "                    'extracted': False\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def auto_detect_document_type(self, text: str) -> str:\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Better German detection\n",
    "        german_keywords = ['rechnung', 'rechnungsnummer', 'mehrwertsteuer', 'ust', 'gesamtbetrag', 'firmenname']\n",
    "        if any(keyword in text_lower for keyword in german_keywords):\n",
    "            return 'german_invoice'\n",
    "        \n",
    "        # English invoice detection\n",
    "        if any(word in text_lower for word in ['invoice', 'bill to', 'invoice number']):\n",
    "            return 'invoice'\n",
    "        \n",
    "        return 'general'\n",
    "    \n",
    "    def extract_information(self, text: str, document_type: Optional[str] = None, \n",
    "                          custom_questions: Optional[List[str]] = None) -> Dict:\n",
    "        \n",
    "        # Autodetect if not provided\n",
    "        if document_type is None:\n",
    "            document_type = self.auto_detect_document_type(text)\n",
    "        \n",
    "        # Choose questions\n",
    "        if custom_questions:\n",
    "            questions = custom_questions\n",
    "        else:\n",
    "            questions = self.extraction_templates.get(document_type, self.extraction_templates['general'])\n",
    "        \n",
    "        # Extract with QA\n",
    "        qa_results = self.extract_with_questions(text, questions)\n",
    "        \n",
    "        # Extract entities with NER if available\n",
    "        ner_results = self.extract_entities_with_ner(text)\n",
    "        \n",
    "        # Combine results\n",
    "        successful = sum(1 for r in qa_results.values() if r.get('extracted'))\n",
    "        \n",
    "        return {\n",
    "            'document_type': document_type,\n",
    "            'extraction_timestamp': datetime.now().isoformat(),\n",
    "            'model_used': self.model_name,\n",
    "            'total_questions': len(questions),\n",
    "            'successful_extractions': successful,\n",
    "            'success_rate': successful / len(questions) if questions else 0,\n",
    "            'extractions': qa_results,\n",
    "            'entities': ner_results\n",
    "        }\n",
    "        \n",
    "    # Only uses high-confidence predictions for further training\n",
    "    # This prevents error propagation\n",
    "    def get_high_confidence_extractions(self, results: Dict, min_confidence: float = 0.3) -> Dict:\n",
    "        \"\"\"Lower confidence threshold for invoice extraction\"\"\"\n",
    "        high_conf = {}\n",
    "        \n",
    "        for question, result in results['extractions'].items():\n",
    "            if result.get('confidence', 0) >= min_confidence and result.get('answer'):\n",
    "                high_conf[question] = result\n",
    "        \n",
    "        return {\n",
    "            'document_type': results['document_type'],\n",
    "            'high_confidence_extractions': high_conf,\n",
    "            'total_high_confidence': len(high_conf)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7785e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data processing pipeline with QA extraction\n",
    "\n",
    "def process_document_with_qa(document_data: Dict, custom_questions: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a normalized document using QA-based extraction.\n",
    "    \"\"\"\n",
    "    extractor = QABasedExtractor()\n",
    "    \n",
    "    # Combine all text content from the document\n",
    "    all_text = \"\"\n",
    "    for content_item in document_data['content']:\n",
    "        all_text += content_item['text'] + \" \"\n",
    "    \n",
    "    \"\"\"Takes your normalized document (which has text split by pages/sections)\n",
    "Combines everything into one big text string\n",
    "This gives the AI the full context to answer questions\"\"\"\n",
    "\n",
    "    # Extract information using QA\n",
    "    extraction_results = extractor.extract_information(\n",
    "        text=all_text,\n",
    "        custom_questions=custom_questions\n",
    "    )\n",
    "    \"\"\"Passes the combined text to your AI extractor\n",
    "Uses custom questions if provided (like your German questions)\n",
    "Gets back structured answers with confidence scores\"\"\"\n",
    "\n",
    "    # extraction results to document data\n",
    "    \n",
    "    document_data['qa_extraction'] = extraction_results\n",
    "    document_data['high_confidence_extractions'] = extractor.get_high_confidence_extractions(\n",
    "        extraction_results\n",
    "    )\n",
    "    \"\"\"Adds AI results to  original document structure\n",
    "Creates two versions: all results + high-confidence only\n",
    "Preserves original data while adding AI insights\"\"\"\n",
    "    \n",
    "    return document_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee48d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration with existing pipeline\n",
    "def enhanced_load_and_normalize_with_qa(file_path: str, table_name: Optional[str] = None, \n",
    "                                       custom_questions: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced version of load_and_normalize that includes QA extraction.\n",
    "    \"\"\"\n",
    "    # Use existing normalization function\n",
    "    document_data = load_and_normalize(file_path, table_name)\n",
    "    \n",
    "    # QA-based extraction\n",
    "    enhanced_data = process_document_with_qa(document_data, custom_questions)\n",
    "    \"\"\"Takes the normalized document\n",
    "Runs AI question-answering\n",
    "Adds intelligent extraction results\n",
    "Returns enhanced document with AI insights\"\"\"\n",
    "    return enhanced_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11a80b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAFE_RUN=True: skipping fine-tune auto-trigger\n"
     ]
    }
   ],
   "source": [
    "# Auto-trigger fine-tuning when enough new feedback exists\n",
    "try:\n",
    "    if not SAFE_RUN:\n",
    "        print(\"Checking if fine-tuning should run from feedback...\")\n",
    "        maybe_finetune(threshold_new_rows=5)\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"SAFE_RUN=True: skipping fine-tune auto-trigger\")\n",
    "except NameError:\n",
    "    print(\"Skipping auto-trigger: maybe_finetune not defined yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1328736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pdf file: Rechnung.pdf\n",
      "Loading base model: deepset/xlm-roberta-large-squad2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Document: Rechnung.pdf\n",
      "✅ Type: german_invoice\n",
      "✅ Success Rate: 61.3%\n",
      "✅ Extractions: 19/31\n",
      "\n",
      "=== High Confidence Results ===\n",
      "Q: Wie heißt die Firma auf der Rechnung?\n",
      "A:  Nordson Ahmed Ebada (confidence: 0.906)\n",
      "----------------------------------------\n",
      "Q: Welches Datum steht nach 'Rechnungsdatum'?\n",
      "A:  Lieferdatum (confidence: 0.929)\n",
      "----------------------------------------\n",
      "Q: What date appears after 'Rechnungsdatum'?\n",
      "A:  Lieferdatum (confidence: 0.865)\n",
      "----------------------------------------\n",
      "Q: What is the invoice number?\n",
      "A:  1407606058. (confidence: 0.561)\n",
      "----------------------------------------\n",
      "Q: What is the Rechnungsnummer?\n",
      "A:  1407606058. (confidence: 0.308)\n",
      "----------------------------------------\n",
      "Q: What is the total amount?\n",
      "A:  Gesamtbruttowert (EUR) 13,95 (confidence: 0.375)\n",
      "----------------------------------------\n",
      "Q: What is the Rechnungsdatum?\n",
      "A:  Lieferdatum (confidence: 0.678)\n",
      "----------------------------------------\n",
      "Q: Who is the customer?\n",
      "A:  Nordson Ahmed Ebada (confidence: 0.301)\n",
      "----------------------------------------\n",
      "Q: What is the Mehrwertsteuer?\n",
      "A:  19,00% (confidence: 0.823)\n",
      "----------------------------------------\n",
      "Q: What is the net amount?\n",
      "A:  (EUR) 11,72 (confidence: 0.340)\n",
      "----------------------------------------\n",
      "\n",
      "=== Named Entities Found ===\n"
     ]
    }
   ],
   "source": [
    "# Testing with German PDF\n",
    "german_path = r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\Rechnung.pdf\"\n",
    "\n",
    "# Load and process\n",
    "result = enhanced_load_and_normalize_with_qa(german_path)\n",
    "\n",
    "# Show improved results\n",
    "print(f\"✅ Document: {result['file_name']}\")\n",
    "print(f\"✅ Type: {result['qa_extraction']['document_type']}\")\n",
    "print(f\"✅ Success Rate: {result['qa_extraction']['success_rate']:.1%}\")\n",
    "print(f\"✅ Extractions: {result['qa_extraction']['successful_extractions']}/{result['qa_extraction']['total_questions']}\")\n",
    "\n",
    "print(\"\\n=== High Confidence Results ===\")\n",
    "for question, answer in result['high_confidence_extractions']['high_confidence_extractions'].items():\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer['answer']} (confidence: {answer['confidence']:.3f})\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Show NER entities if found\n",
    "if result['qa_extraction'].get('entities'):\n",
    "    print(\"\\n=== Named Entities Found ===\")\n",
    "    for entity_type, entities in result['qa_extraction']['entities'].items():\n",
    "        if entities:\n",
    "            print(f\"{entity_type}: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63cb69de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Multi-Format Document Extraction\n",
      "\n",
      " Testing: README.txt\n",
      "Processing text file: README.txt\n",
      "Loading base model: deepset/xlm-roberta-large-squad2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Type: general\n",
      "✅ Success Rate: 28.0%\n",
      "   Q: What companies or organizations are mentioned?...\n",
      "   A:  Deutsche Bahn\n",
      "   Q: What requirements or specifications are listed?...\n",
      "   A:  Python packages\n",
      "   Q: What actions or tasks are described?...\n",
      "   A:  Classification task\n"
     ]
    }
   ],
   "source": [
    "# Quick test of  multi-format system (extra .txt file)\n",
    "\n",
    "print(\"Testing Multi-Format Document Extraction\")\n",
    "\n",
    "test_files = [\n",
    "    r\"C:\\Users\\aslia\\OneDrive\\Desktop\\github\\Predicting-Train-Delays\\README.txt\"]\n",
    "\n",
    "\n",
    "for file_path in test_files:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"\\n Testing: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            result = enhanced_load_and_normalize_with_qa(file_path)\n",
    "            print(f\"✅ Type: {result['qa_extraction']['document_type']}\")\n",
    "            print(f\"✅ Success Rate: {result['qa_extraction']['success_rate']:.1%}\")\n",
    "            \n",
    "            # Show top 3 extractions\n",
    "            count = 0\n",
    "            for question, answer in result['qa_extraction']['extractions'].items():\n",
    "                if answer.get('answer') and count < 3:\n",
    "                    print(f\"   Q: {question[:50]}...\")\n",
    "                    print(f\"   A: {answer['answer']}\")\n",
    "                    count += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "    else:\n",
    "        print(f\" File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4182610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing OCR Integration with YOUR System ===\n",
      "Processing image file: invoice_sample.png\n",
      "Loading base model: deepset/xlm-roberta-large-squad2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OCR + QA Success!\n",
      "✅ Document: invoice_sample.png\n",
      "✅ Type: invoice\n",
      "✅ AI Extractions: 2\n",
      "Q: What customer name appears on the invoice?\n",
      "A:  Richard Sanchez (confidence: 0.980)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing with OCR + QA system\n",
    "\n",
    "print(\"=== Testing OCR Integration with YOUR System ===\")\n",
    "\n",
    "# Test image path\n",
    "image_path = r\"C:\\Users\\aslia\\Downloads\\invoice_sample.png\"\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    \n",
    "    # Use  existing pipeline\n",
    "    result = enhanced_load_and_normalize_with_qa(image_path)\n",
    "    \n",
    "    print(f\"✅ OCR + QA Success!\")\n",
    "    print(f\"✅ Document: {result['file_name']}\")\n",
    "    print(f\"✅ Type: {result['qa_extraction']['document_type']}\")\n",
    "    print(f\"✅ AI Extractions: {result['qa_extraction']['successful_extractions']}\")\n",
    "    \n",
    "    # Show results using system\n",
    "    for question, answer in result['high_confidence_extractions']['high_confidence_extractions'].items():\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {answer['answer']} (confidence: {answer['confidence']:.3f})\")\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"Add your image path to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca90862",
   "metadata": {},
   "source": [
    "**3. Self-Learning and Auto Fine-Tuning**\n",
    "* Build a feedback loop that captures user corrections, validation mismatches, or annotation logs.\n",
    "* Implement automatic model fine-tuning or retraining using this feedback without manual intervention.\n",
    "* Ensure continuous performance improvement while avoiding overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed5f4f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERACTIVE=False: auto-confirming predicted answer\n",
      "Feedback logged.\n"
     ]
    }
   ],
   "source": [
    "# Phase 3A: Feedback Collection\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Path to store feedback logs\n",
    "feedback_file = Path(\"feedback_log.csv\")\n",
    "\n",
    "# Create file with headers if it doesn't exist\n",
    "\n",
    "if not feedback_file.exists():\n",
    "    with open(feedback_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"timestamp\", \"question\", \"context\", \"predicted_answer\", \"correct_answer\"])\n",
    "\n",
    "# Captures human feedback for reinforcement\n",
    "def log_feedback(question, context, predicted_answer, correct_answer=None):\n",
    "    \"\"\"\n",
    "    Logs model output and user-provided corrections to a CSV file.\n",
    "    \"\"\"\n",
    "    with open(feedback_file, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            datetime.now().isoformat(),\n",
    "            question,\n",
    "            context,\n",
    "            predicted_answer,\n",
    "            correct_answer if correct_answer else \"\"\n",
    "        ])\n",
    "\n",
    "# Example usage after prediction step:\n",
    "\n",
    "question = \"What is the capital of Germany?\"\n",
    "context = \"Berlin is the capital and largest city of Germany.\"\n",
    "predicted_answer = \"Berlin\"\n",
    "\n",
    "# Suppose user confirms or corrects:\n",
    "\n",
    "if INTERACTIVE:\n",
    "    correct_answer = input(f\"Predicted answer: {predicted_answer}\\nIf wrong, type correct answer (or press Enter to confirm): \")\n",
    "else:\n",
    "    print(\"INTERACTIVE=False: auto-confirming predicted answer\")\n",
    "    correct_answer = predicted_answer\n",
    "\n",
    "print(\"Feedback logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd6f35",
   "metadata": {},
   "source": [
    "🔹feedback_log.csv is created the first time you run it. After each prediction, the system:\n",
    "   Shows the predicted answer.\n",
    "   Lets the user correct it or press Enter to confirm.\n",
    "\n",
    "Appends a new row with timestamp, Q, context, prediction, and correction.\n",
    "\n",
    "🔹This creates your personal dataset for fine-tuning.\n",
    " It ensures the model improves based on your domain-specific corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "309852af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback_dataset.json created!\n"
     ]
    }
   ],
   "source": [
    "# Phase 3B: Prepare Feedback for Fine-Tuning\n",
    "\n",
    "# Load feedback\n",
    "feedback_file = \"feedback_log.csv\"\n",
    "\n",
    "# Prepare SQuAD-style dataset(current model deepset/xlm-roberta-large-squad2 uses this format)\n",
    "data = {\n",
    "    \"version\": \"v2.0\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"feedback_data\",\n",
    "            \"paragraphs\": []\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "import hashlib \n",
    "with open(feedback_file, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        context = row[\"context\"]\n",
    "        question = row[\"question\"]\n",
    "        answer_text = row[\"correct_answer\"] or row[\"predicted_answer\"]\n",
    "\n",
    "        # Find start index of answer in context (required for SQuAD format)\n",
    "        start_index = context.find(answer_text)\n",
    "        if start_index == -1:\n",
    "            continue  # skip if answer not found in context\n",
    "\n",
    "        # Force string types and stable ID\n",
    "        safe_q = str(question)\n",
    "        safe_ctx = str(context)\n",
    "        qid = hashlib.sha1((safe_q + \"||\" + safe_ctx).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        paragraph = {\n",
    "            \"context\": safe_ctx,\n",
    "            \"qas\": [\n",
    "                {\n",
    "                    \"id\": qid,\n",
    "                    \"question\": safe_q,\n",
    "                    \"answers\": [\n",
    "                        {\n",
    "                            \"text\": answer_text,\n",
    "                            \"answer_start\": start_index\n",
    "                        }\n",
    "                    ],\n",
    "                    \"is_impossible\": False\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        data[\"data\"][0][\"paragraphs\"].append(paragraph)\n",
    "\n",
    "# Save dataset\n",
    "with open(\"feedback_dataset.json\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(data, out_file, ensure_ascii=False, indent=2)\n",
    "print(\"feedback_dataset.json created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b74aa",
   "metadata": {},
   "source": [
    "* Read feedback_log.csv ( manually collected corrections).\n",
    "\n",
    "* Convert it to SQuAD-style JSON : required for most extractive QA fine-tuning (current model deepset/xlm-roberta-large-squad2 uses this format).\n",
    "\n",
    "* Save it as feedback_dataset.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29b57e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data if feedback_log.csv is empty\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "if not os.path.exists(\"feedback_log.csv\") or os.path.getsize(\"feedback_log.csv\") <= 100:\n",
    "    print(\"Creating sample feedback data...\")\n",
    "    \n",
    "    with open(\"feedback_log.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"timestamp\", \"question\", \"context\", \"predicted_answer\", \"correct_answer\"])\n",
    "        \n",
    "        # Add sample entries\n",
    "        samples = [\n",
    "            [datetime.now().isoformat(), \"What is the invoice number?\", \"Invoice RE-001 Total: 1000 EUR\", \"RE-001\", \"RE-001\"],\n",
    "            [datetime.now().isoformat(), \"What is the total amount?\", \"Invoice RE-001 Total: 1000 EUR\", \"1000 EUR\", \"1000 EUR\"],\n",
    "            [datetime.now().isoformat(), \"What is the company name?\", \"ACME Corp Invoice RE-001\", \"ACME Corp\", \"ACME Corp\"]\n",
    "        ]\n",
    "        \n",
    "        for sample in samples:\n",
    "            writer.writerow(sample)\n",
    "    \n",
    "    print(\"✅ Sample feedback data created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be651aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback_dataset.json loaded: 3 paragraphs\n"
     ]
    }
   ],
   "source": [
    "# Phase 3C:load the feedback_dataset.json into a Hugging Face Dataset object.\n",
    "\n",
    "if os.path.exists(\"feedback_dataset.json\"):\n",
    "    with open(\"feedback_dataset.json\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"feedback_dataset.json loaded: {len(data['data'][0]['paragraphs'])} paragraphs\")\n",
    "else:\n",
    "    print(\"feedback_dataset.json not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc3ad8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,574,914 || all params: 560,417,796 || trainable%: 0.2810\n",
      " Starting LoRA fine-tuning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA fine-tuning complete! Saved to './lora_fine_tuned_model'\n"
     ]
    }
   ],
   "source": [
    "# Phase 3D: Fine-tuning with LoRA\n",
    "\n",
    "import os, json, torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    TrainingArguments, Trainer, DefaultDataCollator\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load your SQuAD-style feedback JSON\n",
    "\n",
    "with open(\"feedback_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "examples = []\n",
    "for article in data[\"data\"]:\n",
    "    for para in article[\"paragraphs\"]:\n",
    "        ctx = para[\"context\"]\n",
    "        for qa in para[\"qas\"]:\n",
    "            ans = qa[\"answers\"][0] if qa.get(\"answers\") else {\"text\": \"\", \"answer_start\": 0}\n",
    "            examples.append({\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"context\": ctx,\n",
    "                \"answer_text\": ans.get(\"text\", \"\"),\n",
    "                \"answer_start\": ans.get(\"answer_start\", 0),\n",
    "            })\n",
    "\n",
    "def make_features(ex):\n",
    "    tok = tokenizer(\n",
    "        ex[\"question\"],\n",
    "        ex[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    offsets = tok.pop(\"offset_mapping\")\n",
    "    seq_ids = tok.sequence_ids()\n",
    "\n",
    "    # Context token span\n",
    "    start_ctx = 0\n",
    "    while start_ctx < len(seq_ids) and seq_ids[start_ctx] != 1:\n",
    "        start_ctx += 1\n",
    "    end_ctx = len(seq_ids) - 1\n",
    "    while end_ctx >= 0 and seq_ids[end_ctx] != 1:\n",
    "        end_ctx -= 1\n",
    "\n",
    "    start_char = ex[\"answer_start\"]\n",
    "    end_char = start_char + len(ex[\"answer_text\"])\n",
    "\n",
    "    if ex[\"answer_text\"] == \"\":\n",
    "        start_pos = end_pos = start_ctx\n",
    "    elif not (offsets[start_ctx][0] <= start_char and offsets[end_ctx][1] >= end_char):\n",
    "        start_pos = end_pos = start_ctx\n",
    "    else:\n",
    "        s = start_ctx\n",
    "        while s < len(offsets) and offsets[s][0] <= start_char:\n",
    "            s += 1\n",
    "        start_pos = s - 1\n",
    "        e = end_ctx\n",
    "        while e > 0 and offsets[e][1] >= end_char:\n",
    "            e -= 1\n",
    "        end_pos = e + 1\n",
    "\n",
    "    tok[\"start_positions\"] = start_pos\n",
    "    tok[\"end_positions\"] = end_pos\n",
    "    return tok\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.features = [make_features(ex) for ex in examples]\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, i):\n",
    "        return {k: torch.tensor(v) for k, v in self.features[i].items()}\n",
    "\n",
    "train_dataset = QADataset(examples)\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "\n",
    "#base model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# LoRA Configuration \n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    "    inference_mode=False,\n",
    "    r=16,  # Rank - higher = more parameters but slower\n",
    "    lora_alpha=32,  # Scaling parameter\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    target_modules=[\"query\", \"value\"]  # Which layers to adapt\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  \n",
    "\n",
    "# training arguments for LoRA\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./lora_fine_tuned_model\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=3e-4, \n",
    "    per_device_train_batch_size=4,  # Can use larger batch\n",
    "    num_train_epochs=2,  # Fewer epochs needed\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    "    report_to=None,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_strategy=\"epoch\",\n",
    "    warmup_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DefaultDataCollator(),\n",
    ")\n",
    "\n",
    "print(\" Starting LoRA fine-tuning\")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./lora_fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./lora_fine_tuned_model\")\n",
    "print(\"✅ LoRA fine-tuning complete! Saved to './lora_fine_tuned_model'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d387ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3E:Automatic fine-tuning \n",
    "\n",
    "# This is the \"self-learning\" part that makes this system continuously improve without human intervention\n",
    "# heart of the self-learning system\n",
    "\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    TrainingArguments, Trainer, DefaultDataCollator\n",
    ")\n",
    "\n",
    "#1.  File Management Setup\n",
    "\"\"\"Defines paths for state tracking, feedback storage, and model output\n",
    "STATE_FILE: Tracks training history (.self_tune_state.json)\n",
    "FEEDBACK_CSV: User corrections (feedback_log.csv)\n",
    "FEEDBACK_JSON: Training-ready format (feedback_dataset.json)\"\"\"\n",
    "\n",
    "STATE_FILE = Path(\".self_tune_state.json\") # Tracks training history\n",
    "FEEDBACK_CSV = Path(\"feedback_log.csv\")  # User corrections\n",
    "FEEDBACK_JSON = Path(\"feedback_dataset.json\") # Training-ready format\n",
    "OUTPUT_DIR = Path(\"./lora_fine_tuned_model\") # Where improved model is saved\n",
    "\n",
    "BASE_MODEL = \"deepset/xlm-roberta-large-squad2\"\n",
    "\n",
    "\n",
    "#2. State Tracking Functions\n",
    "\"\"\"_load_state() and _save_state(): Remember how many feedback rows were used for training\n",
    "_feedback_count(): Count new user corrections since last training\n",
    "Prevents retraining on the same data repeatedly\"\"\"\n",
    "\n",
    "def _load_state(): # + save?state Remember how many feedback rows were used for training\n",
    "    if STATE_FILE.exists():\n",
    "        return json.loads(STATE_FILE.read_text(encoding=\"utf-8\"))\n",
    "    return {\"last_trained_count\": 0, \"last_trained_at\": 0}\n",
    "\n",
    "def _save_state(state):\n",
    "    STATE_FILE.write_text(json.dumps(state, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _feedback_count():   # Count new user corrections since last training\n",
    "    if not FEEDBACK_CSV.exists():\n",
    "        return 0\n",
    "    with FEEDBACK_CSV.open(newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = list(reader)\n",
    "        return max(0, len(rows) - 1)  # minus header\n",
    "\n",
    "#3. Data Conversion Pipeline\n",
    "\"\"\"_build_squad_from_csv(): Converts user feedback CSV into SQuAD format (the training format AI models expect)\n",
    "_load_examples(): Loads and prepares training examples\n",
    "_features_maker(): Tokenizes questions/contexts and maps answer positions\"\"\"\n",
    "\n",
    "def _build_squad_from_csv(csv_path=FEEDBACK_CSV, out_json=FEEDBACK_JSON): #cnverts feedbacks CSV into SQuAD format (the training format AI models expect)\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(\"feedback_log.csv not found\")\n",
    "    import hashlib \n",
    "    data = {\"version\": \"v2.0\", \"data\": [{\"title\": \"feedback_data\", \"paragraphs\": []}]}\n",
    "    with csv_path.open(newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            ctx = row[\"context\"]\n",
    "            q = row[\"question\"]\n",
    "            ans_text = row[\"correct_answer\"] or row[\"predicted_answer\"]\n",
    "            start = ctx.find(ans_text)\n",
    "            if start == -1:\n",
    "                continue\n",
    "            safe_q = str(q)\n",
    "            safe_ctx = str(ctx)\n",
    "            qid = hashlib.sha1((safe_q + \"||\" + safe_ctx).encode(\"utf-8\")).hexdigest()\n",
    "            data[\"data\"][0][\"paragraphs\"].append({\n",
    "                \"context\": safe_ctx,\n",
    "                \"qas\": [{\n",
    "                    \"id\": qid,\n",
    "                    \"question\": safe_q,\n",
    "                    \"answers\": [{\"text\": ans_text, \"answer_start\": start}],\n",
    "                    \"is_impossible\": False\n",
    "                }]\n",
    "            })\n",
    "    out_json.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return out_json\n",
    "\n",
    "\n",
    "def _load_examples(feedback_json=FEEDBACK_JSON):\n",
    "    d = json.loads(Path(feedback_json).read_text(encoding=\"utf-8\"))\n",
    "    examples = []\n",
    "    for article in d[\"data\"]:\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            ctx = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                ans = qa[\"answers\"][0] if qa.get(\"answers\") else {\"text\": \"\", \"answer_start\": 0}\n",
    "                examples.append({\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"context\": ctx,\n",
    "                    \"answer_text\": ans.get(\"text\", \"\"),\n",
    "                    \"answer_start\": ans.get(\"answer_start\", 0),\n",
    "                })\n",
    "    return examples\n",
    "\n",
    "\n",
    "\n",
    "def _features_maker(tokenizer): \n",
    "    def make_features(ex):\n",
    "        tok = tokenizer(\n",
    "            ex[\"question\"], ex[\"context\"],\n",
    "            truncation=\"only_second\", max_length=384, stride=128,\n",
    "            return_offsets_mapping=True, padding=\"max_length\",\n",
    "        )\n",
    "        offsets = tok.pop(\"offset_mapping\")\n",
    "        seq_ids = tok.sequence_ids()\n",
    "        start_ctx = 0\n",
    "        while start_ctx < len(seq_ids) and seq_ids[start_ctx] != 1:\n",
    "            start_ctx += 1\n",
    "        end_ctx = len(seq_ids) - 1\n",
    "        while end_ctx >= 0 and seq_ids[end_ctx] != 1:\n",
    "            end_ctx -= 1\n",
    "        start_char = ex[\"answer_start\"]\n",
    "        end_char = start_char + len(ex[\"answer_text\"])\n",
    "        if ex[\"answer_text\"] == \"\":\n",
    "            start_pos = end_pos = start_ctx\n",
    "        elif not (offsets[start_ctx][0] <= start_char and offsets[end_ctx][1] >= end_char):\n",
    "            start_pos = end_pos = start_ctx\n",
    "        else:\n",
    "            s = start_ctx\n",
    "            while s < len(offsets) and offsets[s][0] <= start_char:\n",
    "                s += 1\n",
    "            start_pos = s - 1\n",
    "            e = end_ctx\n",
    "            while e > 0 and offsets[e][1] >= end_char:\n",
    "                e -= 1\n",
    "            end_pos = e + 1\n",
    "        tok[\"start_positions\"] = start_pos\n",
    "        tok[\"end_positions\"] = end_pos\n",
    "        return tok\n",
    "    return make_features\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer):\n",
    "        maker = _features_maker(tokenizer)\n",
    "        self.features = [maker(ex) for ex in examples]\n",
    "    def __len__(self): return len(self.features)\n",
    "    def __getitem__(self, i): return {k: torch.tensor(v) for k, v in self.features[i].items()}\n",
    "\n",
    "\n",
    "\n",
    "#4. LoRA Fine-Tuning Functions\n",
    "# train_from_feedback function\n",
    "\n",
    "\"\"\"train_from_feedback_lora(): The actual fast LoRA fine-tuning process\n",
    "QADataset: PyTorch dataset class for training\n",
    "Much faster than full fine-tuning\"\"\"\n",
    "\n",
    "def train_from_feedback_lora(base_model=None, output_dir=\"./lora_fine_tuned_model\", epochs=2, lr=3e-4, batch_size=4):\n",
    "    \"\"\"Fast LoRA fine-tuning from feedback\"\"\"\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    base = base_model or BASE_MODEL\n",
    "    \n",
    "    _build_squad_from_csv(FEEDBACK_CSV, FEEDBACK_JSON)\n",
    "    examples = _load_examples(FEEDBACK_JSON)\n",
    "    \n",
    "    if not examples:\n",
    "        print(\"No examples to train.\")\n",
    "        return False\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base)\n",
    "    dataset = QADataset(examples, tokenizer)\n",
    "    print(f\" LoRA training samples: {len(dataset)} (base={base})\")\n",
    "\n",
    "    # Load base model\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(base)\n",
    "    \n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.QUESTION_ANS,\n",
    "        inference_mode=False,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query\", \"value\"]\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        eval_strategy=\"no\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=5,\n",
    "        save_total_limit=1,\n",
    "        push_to_hub=False,\n",
    "        report_to=None,\n",
    "        dataloader_pin_memory=False,\n",
    "        warmup_steps=10,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=DefaultDataCollator(),\n",
    "    )\n",
    "    \n",
    "    print(\" Starting fast LoRA training...\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(str(output_dir))\n",
    "    tokenizer.save_pretrained(str(output_dir))\n",
    "    print(f\"✅ LoRA model saved to {output_dir}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# 5. Auto-Trigger automatically retrains when enough feedback collected actuaşl \"self-training\" part\n",
    "\n",
    "\n",
    "\"\"\"maybe_finetune_lora(): The \"smart automation\" that checks if enough new feedback exists\n",
    "Default threshold: 5 new corrections trigger automatic retraining\n",
    "Updates state tracking after successful training\"\"\"\n",
    "\n",
    "def maybe_finetune_lora(threshold_new_rows=5):\n",
    "    \"\"\"Auto-trigger LoRA fine-tuning\"\"\"\n",
    "    state = _load_state()\n",
    "    total = _feedback_count()\n",
    "    new = total - state.get(\"last_trained_count\", 0)\n",
    "    print(f\"Feedback rows: {total} (new since last train: {new})\")\n",
    "    if new >= threshold_new_rows:\n",
    "        print(\" Starting fast LoRA fine-tuning...\")\n",
    "        ok = train_from_feedback_lora()  # Much faster!\n",
    "        if ok:\n",
    "            state[\"last_trained_count\"] = total\n",
    "            state[\"last_trained_at\"] = int(time.time())\n",
    "            _save_state(state)\n",
    "            print(\"✅ LoRA fine-tuned and state updated.\")\n",
    "    else:\n",
    "        print(f\"Not enough new feedback yet (need {threshold_new_rows}).\")\n",
    "        \n",
    "        \"\"\"How It Works in Practice:\n",
    "You use the system → AI makes predictions\n",
    "You correct wrong answers → Logged to feedback_log.csv\n",
    "Background check → maybe_finetune() counts new corrections\n",
    "Auto-training → When you have 5+ new corrections, it automatically fine-tunes\n",
    "Improved model → Next time you use the system, it's smarter\n",
    "Why This Is Powerful:\n",
    "Zero manual work: No need to manually retrain\n",
    "Continuous improvement: Gets better with each correction you make\n",
    "Domain-specific: Learns YOUR specific documents and terminology\n",
    "Memory: Never forgets previous corrections\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aea508",
   "metadata": {},
   "source": [
    "**4.\tModel Evaluation and Versioning**\n",
    "* Define evaluation metrics for accuracy, confidence, and error detection.\n",
    "* Track model versions and performance history, and allow safe promotion of improved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4A Model Evaluation and Versioning System\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, metrics_file=\"model_metrics.json\"):\n",
    "        # Creates a tracker that saves results to a JSON file\n",
    "        self.metrics_file = Path(metrics_file)\n",
    "        self.metrics_history = self._load_metrics() # Loads previous results\n",
    "        \n",
    "    def _load_metrics(self):\n",
    "        if self.metrics_file.exists():\n",
    "            return json.loads(self.metrics_file.read_text())\n",
    "        return {\"evaluations\": []}\n",
    "    \n",
    "    def _save_metrics(self):\n",
    "        self.metrics_file.write_text(json.dumps(self.metrics_history, indent=2))\n",
    "    \n",
    "    def create_ground_truth_template(self, invoice_files): # invoice_files is a LIST of paths\n",
    "        \"\"\"Create template for manual ground truth annotation\"\"\" \n",
    "        template = {}\n",
    "        for file_path in invoice_files:\n",
    "            filename = Path(file_path).name  # Extract just filename (eg \"invoice1.pdf\")\n",
    "            template[filename] = {\n",
    "                \"invoice_number\": \"\",\n",
    "                \"total_amount\": \"\",\n",
    "                \"company_name\": \"\",\n",
    "                \"invoice_date\": \"\",\n",
    "                \"tax_amount\": \"\",\n",
    "                \"net_amount\": \"\",\n",
    "                \"customer_name\": \"\",\n",
    "                \"notes\": \"Manual annotation needed\"\n",
    "            }\n",
    "        \n",
    "        # Save template\n",
    "        with open(\"ground_truth_template.json\", \"w\") as f:\n",
    "            json.dump(template, f, indent=2)\n",
    "        \n",
    "        print(\"Ground truth template created: ground_truth_template.json\")\n",
    "        print(\"Please fill in the correct values manually\")\n",
    "        return template\n",
    "    \n",
    "    def evaluate_extraction_results(self, results_dict, ground_truth=None, document_name=\"\"):\n",
    "        \"\"\"Evaluate extraction quality with multiple metrics\"\"\"\n",
    "        evaluation = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"document_name\": document_name,\n",
    "            \"model_version\": results_dict.get('qa_extraction', {}).get('model_used', 'unknown'),\n",
    "            \"document_type\": results_dict.get('qa_extraction', {}).get('document_type', 'unknown'),\n",
    "            \"total_questions\": results_dict.get('qa_extraction', {}).get('total_questions', 0),\n",
    "            \"successful_extractions\": results_dict.get('qa_extraction', {}).get('successful_extractions', 0),\n",
    "            \"success_rate\": results_dict.get('qa_extraction', {}).get('success_rate', 0),\n",
    "            \"avg_confidence\": self._calculate_avg_confidence(results_dict),\n",
    "            \"high_confidence_count\": len(results_dict.get('high_confidence_extractions', {}).get('high_confidence_extractions', {}))\n",
    "        } \n",
    "        \n",
    "        # Add ground truth comparison if available\n",
    "        if ground_truth:\n",
    "            evaluation.update(self._compare_with_ground_truth(results_dict, ground_truth))\n",
    "        \n",
    "        self.metrics_history[\"evaluations\"].append(evaluation)\n",
    "        self._save_metrics()\n",
    "        return evaluation\n",
    "    \n",
    "    def _calculate_avg_confidence(self, results_dict):\n",
    "        extractions = results_dict.get('qa_extraction', {}).get('extractions', {})\n",
    "        confidences = [result.get('confidence', 0) for result in extractions.values() \n",
    "                      if result.get('confidence') is not None]\n",
    "        return sum(confidences) / len(confidences) if confidences else 0\n",
    "    \n",
    "    def _compare_with_ground_truth(self, results_dict, ground_truth):\n",
    "        \"\"\"Compare extractions with ground truth for accuracy metrics\"\"\"\n",
    "        extractions = results_dict.get('qa_extraction', {}).get('extractions', {})\n",
    "        \n",
    "        # Key extraction mapping\n",
    "        key_mappings = {\n",
    "            \"invoice_number\": [\"rechnungsnummer\", \"invoice number\", \"nummer\"],\n",
    "            \"total_amount\": [\"gesamtbetrag\", \"total\", \"endbetrag\", \"amount\"],\n",
    "            \"company_name\": [\"company\", \"firma\", \"firmenname\", \"vendor\"],\n",
    "            \"invoice_date\": [\"datum\", \"date\", \"rechnungsdatum\"],\n",
    "            \"tax_amount\": [\"mwst\", \"ust\", \"tax\", \"mehrwertsteuer\"],\n",
    "            \"net_amount\": [\"netto\", \"net\", \"nettobetrag\"]\n",
    "        }\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        detailed_results = {}\n",
    "        \n",
    "        for gt_key, gt_value in ground_truth.items():\n",
    "            if not gt_value:  # Skip empty ground truth values\n",
    "                continue\n",
    "                \n",
    "            best_match = None\n",
    "            best_similarity = 0\n",
    "            \n",
    "            # Find best matching extraction\n",
    "            for question, result in extractions.items():\n",
    "                if result.get('answer') and any(keyword in question.lower() for keyword in key_mappings.get(gt_key, [])):\n",
    "                    similarity = self._answers_match_score(result['answer'], gt_value)\n",
    "                    if similarity > best_similarity:\n",
    "                        best_similarity = similarity\n",
    "                        best_match = result['answer']\n",
    "            \n",
    "            total += 1\n",
    "            is_correct = best_similarity >= 0.8\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            \n",
    "            detailed_results[gt_key] = {\n",
    "                \"ground_truth\": gt_value,\n",
    "                \"predicted\": best_match,\n",
    "                \"similarity\": best_similarity,\n",
    "                \"correct\": is_correct\n",
    "            }\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        return {\n",
    "            \"ground_truth_accuracy\": accuracy,\n",
    "            \"correct_answers\": correct,\n",
    "            \"total_compared\": total,\n",
    "            \"detailed_results\": detailed_results\n",
    "        }\n",
    "    \n",
    "    def _answers_match_score(self, predicted, actual):\n",
    "        \"\"\"Calculate similarity score between predicted and actual answers\"\"\"\n",
    "        if not predicted or not actual:\n",
    "            return 0\n",
    "        \n",
    "        # Clean and normalize\n",
    "        pred_clean = str(predicted).lower().strip()\n",
    "        actual_clean = str(actual).lower().strip()\n",
    "        \n",
    "        # Exact match\n",
    "        if pred_clean == actual_clean:\n",
    "            return 1.0\n",
    "        \n",
    "        # Fuzzy matching\n",
    "        similarity = SequenceMatcher(None, pred_clean, actual_clean).ratio()\n",
    "        \n",
    "        # Bonus for number matching (important for invoices)\n",
    "    \n",
    "        pred_numbers = re.findall(r'\\d+[\\.,]?\\d*', pred_clean)\n",
    "        actual_numbers = re.findall(r'\\d+[\\.,]?\\d*', actual_clean)\n",
    "        \n",
    "        if pred_numbers and actual_numbers:\n",
    "            # Normalize numbers for comparison\n",
    "            pred_num = pred_numbers[0].replace(',', '.')\n",
    "            actual_num = actual_numbers[0].replace(',', '.')\n",
    "            try:\n",
    "                if float(pred_num) == float(actual_num):\n",
    "                    similarity = max(similarity, 0.9)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def batch_evaluate_invoices(self, invoice_folder, ground_truth_file=None):\n",
    "        \"\"\" Runs your AI on multiple invoices to generates performance report.\"\"\"\n",
    "        invoice_folder = Path(invoice_folder)\n",
    "        results = []\n",
    "        \n",
    "        # Load ground truth\n",
    "        ground_truth_data = {}\n",
    "        if ground_truth_file and Path(ground_truth_file).exists():\n",
    "            with open(ground_truth_file) as f:\n",
    "                ground_truth_data = json.load(f)\n",
    "        \n",
    "        # Process each file type separately to fix the glob() error\n",
    "        supported_patterns = [\"*.pdf\", \"*.png\", \"*.jpg\", \"*.jpeg\", \"*.tiff\", \"*.bmp\"]\n",
    "        \n",
    "        for pattern in supported_patterns:\n",
    "            for invoice_file in invoice_folder.glob(pattern):  # ONE pattern at a time\n",
    "                print(f\"Processing: {invoice_file.name}\")\n",
    "                \n",
    "                try:\n",
    "                    # Extract using your existing pipeline\n",
    "                    result = enhanced_load_and_normalize_with_qa(str(invoice_file))\n",
    "                    \n",
    "                    # Get ground truth for this file\n",
    "                    gt = ground_truth_data.get(invoice_file.name, {})\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    evaluation = self.evaluate_extraction_results(result, gt, invoice_file.name)\n",
    "                    results.append(evaluation)\n",
    "                    \n",
    "                    print(f\"✅ Success rate: {evaluation['success_rate']:.1%}\")\n",
    "                    if gt:\n",
    "                        print(f\"✅ Accuracy: {evaluation.get('ground_truth_accuracy', 0):.1%}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error processing {invoice_file.name}: {e}\")\n",
    "        \n",
    "        # Generate summary report\n",
    "        if results:\n",
    "            self._generate_evaluation_report(results)\n",
    "        else:\n",
    "            print(\"❌ No files were processed successfully!\")\n",
    "            print(\" Check if files exist in:\", str(invoice_folder.absolute()))\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _generate_evaluation_report(self, results):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        if not results:\n",
    "            print(\"No results to report\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "=== INVOICE EXTRACTION EVALUATION REPORT ===\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "    OVERALL PERFORMANCE:\n",
    "• Total invoices processed: {len(results)}\n",
    "• Average success rate: {df['success_rate'].mean():.1%}\n",
    "• Average confidence: {df['avg_confidence'].mean():.3f}\n",
    "• High confidence extractions: {df['high_confidence_count'].mean():.1f}/question\n",
    "\n",
    "    ACCURACY METRICS:\n",
    "\"\"\"\n",
    "        \n",
    "        if 'ground_truth_accuracy' in df.columns:\n",
    "            accuracy_data = df.dropna(subset=['ground_truth_accuracy'])\n",
    "            if not accuracy_data.empty:\n",
    "                report += f\"\"\"• Ground truth accuracy: {accuracy_data['ground_truth_accuracy'].mean():.1%}\n",
    "• Correct answers: {accuracy_data['correct_answers'].sum()}/{accuracy_data['total_compared'].sum()}\n",
    "• Best performing invoice: {accuracy_data.loc[accuracy_data['ground_truth_accuracy'].idxmax(), 'document_name']}\n",
    "• Worst performing invoice: {accuracy_data.loc[accuracy_data['ground_truth_accuracy'].idxmin(), 'document_name']}\n",
    "\"\"\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    " PERFORMANCE DISTRIBUTION:\n",
    "• Success rate std dev: {df['success_rate'].std():.3f}\n",
    "• Confidence std dev: {df['avg_confidence'].std():.3f}\n",
    "\n",
    "    RECOMMENDATIONS:\n",
    "\"\"\"\n",
    "        \n",
    "        avg_success = df['success_rate'].mean()\n",
    "        if avg_success < 0.7:\n",
    "            report += \"• Consider expanding question templates\\n\"\n",
    "        if avg_success > 0.8:\n",
    "            report += \"• System performing well - ready for production\\n\"\n",
    "        \n",
    "        avg_confidence = df['avg_confidence'].mean()\n",
    "        if avg_confidence < 0.5:\n",
    "            report += \"• Low confidence scores - may need more training data\\n\"\n",
    "        \n",
    "        print(report)\n",
    "        \n",
    "        # Save report\n",
    "        with open(f\"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\", \"w\") as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Model Version Manager :  Tracks different model versions and safely promotes improvements\n",
    "class ModelVersionManager:\n",
    "    def __init__(self, versions_file=\"model_versions.json\"):\n",
    "        self.versions_file = Path(versions_file)\n",
    "        self.versions = self._load_versions()\n",
    "    \n",
    "    def _load_versions(self):\n",
    "        if self.versions_file.exists():\n",
    "            return json.loads(self.versions_file.read_text())\n",
    "        return {\"versions\": [], \"current_version\": None}\n",
    "    \n",
    "    def _save_versions(self):\n",
    "        self.versions_file.write_text(json.dumps(self.versions, indent=2))\n",
    "    \n",
    "    def register_new_version(self, model_path, performance_metrics, description=\"\"):\n",
    "        \"\"\"Registers each new fine-tuned model with its performance scores\"\"\"\n",
    "        version_info = {\n",
    "            \"version_id\": f\"v{len(self.versions['versions']) + 1}\", #v1,v2,v3\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_path\": str(model_path),  # ./lora_fine_tuned_model\n",
    "            \"performance\": performance_metrics,  # Success rates, accuracy\n",
    "            \"description\": description,\n",
    "            \"is_active\": False       # Not active yet\n",
    "        }\n",
    "        \n",
    "        self.versions[\"versions\"].append(version_info)\n",
    "        self._save_versions()\n",
    "        return version_info[\"version_id\"]\n",
    "    \n",
    "    def promote_version(self, version_id, min_success_rate=0.7, min_accuracy=0.6):\n",
    "        \"\"\" Safety checks before making model active,\n",
    "        Only promotes models that perform better than thresholds (prevents regression)\"\"\"\n",
    "        version = self._find_version(version_id)\n",
    "        if not version:\n",
    "            return False, \"Version not found\"\n",
    "        \n",
    "        # Check performance criteria\n",
    "        success_rate = version[\"performance\"].get(\"avg_success_rate\", 0)\n",
    "        accuracy = version[\"performance\"].get(\"avg_accuracy\", 0)\n",
    "        \n",
    "        if success_rate < min_success_rate:\n",
    "            return False, f\"Success rate {success_rate:.2%} below threshold {min_success_rate:.2%}\"\n",
    "        \n",
    "        if accuracy > 0 and accuracy < min_accuracy:\n",
    "            return False, f\"Accuracy {accuracy:.2%} below threshold {min_accuracy:.2%}\"\n",
    "        \n",
    "        # Deactivate current version\n",
    "        for v in self.versions[\"versions\"]:\n",
    "            v[\"is_active\"] = False\n",
    "        \n",
    "        # Activate new version\n",
    "        version[\"is_active\"] = True\n",
    "        self.versions[\"current_version\"] = version_id\n",
    "        self._save_versions()\n",
    "        \n",
    "        return True, f\"Version {version_id} promoted successfully\"\n",
    "    \n",
    "    def _find_version(self, version_id):\n",
    "        for version in self.versions[\"versions\"]:\n",
    "            if version[\"version_id\"] == version_id:\n",
    "                return version\n",
    "        return None\n",
    "    \n",
    "    \"\"\"Research Paper Benefits:\n",
    "Quantitative results showing improvement over time\n",
    "Comparison metrics before/after self-learning\n",
    "Performance graphs demonstrating ReST effectiveness\n",
    "Statistical validation of your approach\"\"\"\n",
    "\n",
    "# Model Evaluation and Versioning System ready after this,  next: 100 invoices for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f9404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STARTING LARGE-SCALE DATASET SETUP\n",
      "==================================================\n",
      "\n",
      " Scanning: C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\new_invoices_dataset\n",
      "\n",
      " Scanning: C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\n",
      " Processing: new_invoices_dataset\n",
      " Copied 100 files...\n",
      " Copied 100 files from this folder\n",
      " Processing: invoice_dataset\n",
      " Copied 6 files from this folder\n",
      "\n",
      "============================================================\n",
      " DATASET SETUP COMPLETE\n",
      "============================================================\n",
      " Total invoices ready: 106\n",
      " Location: c:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\large_scale_invoice_dataset\n",
      " Errors: 0\n",
      " Only 106 files found - may need more datasets\n",
      "\n",
      "✅ Ready for large-scale evaluation with 106 documents\n",
      " Files location: ./large_scale_invoice_dataset/\n",
      " Next: Run your evaluation system on this dataset\n"
     ]
    }
   ],
   "source": [
    "# Step 4B: Large-Scale Dataset Setup for 100+ Invoice Evaluation\n",
    "\n",
    "\n",
    "def setup_large_scale_invoice_dataset():\n",
    "    \"\"\"\n",
    "    Dataset setup for large-scale evaluation\n",
    "    Automatically discovers and organizes invoices from multiple source folders\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration for dataset sources\n",
    "    DATASET_SOURCES = [\n",
    "        \n",
    "        # downloaded dataset\n",
    "        r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\new_invoices_dataset\",\n",
    "\n",
    "        # Original test files\n",
    "        r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\",\n",
    "        \n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Supported file extensions\n",
    "    SUPPORTED_EXTENSIONS = ['.pdf', '.png', '.jpg', '.jpeg', '.tiff', '.bmp']\n",
    "\n",
    "    \n",
    "    # Create main evaluation directory\n",
    "    EVAL_DIR = Path(\"./large_scale_invoice_dataset\")\n",
    "    EVAL_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {'total_copied': 0, 'errors': []}\n",
    "    \n",
    "    # Scan all source directories\n",
    "    for source_dir in DATASET_SOURCES:\n",
    "        source_path = Path(source_dir)\n",
    "        \n",
    "        if not source_path.exists():\n",
    "            print(f\" Source not found: {source_dir}\")\n",
    "            stats['errors'].append(f\"Directory not found: {source_dir}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n Scanning: {source_dir}\")\n",
    "        folder_stats = {'pdf': 0, 'image': 0, 'other': 0, 'total': 0}\n",
    "        \n",
    "        # Scan all source directories\n",
    "    for source_dir in DATASET_SOURCES:\n",
    "        source_path = Path(source_dir)\n",
    "        \n",
    "        if not source_path.exists():\n",
    "            print(f\" Source not found: {source_dir}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\" Processing: {source_path.name}\")\n",
    "        folder_count = 0\n",
    "        \n",
    "        # Find and copy all supported files\n",
    "        for ext in SUPPORTED_EXTENSIONS:\n",
    "            pattern = f\"**/*{ext}\"\n",
    "            for file_path in source_path.glob(pattern):\n",
    "                if file_path.is_file():\n",
    "                    # Create unique filename\n",
    "                    counter = 1\n",
    "                    dest_name = file_path.name\n",
    "                    dest_path = EVAL_DIR / dest_name\n",
    "                    \n",
    "                    while dest_path.exists():\n",
    "                        stem = file_path.stem\n",
    "                        suffix = file_path.suffix\n",
    "                        dest_name = f\"{stem}_{counter}{suffix}\"\n",
    "                        dest_path = EVAL_DIR / dest_name\n",
    "                        counter += 1\n",
    "                    \n",
    "                    try:\n",
    "                        shutil.copy2(file_path, dest_path)\n",
    "                        stats['total_copied'] += 1\n",
    "                        folder_count += 1\n",
    "                        \n",
    "                        # Progress indicator\n",
    "                        if stats['total_copied'] % 100 == 0:\n",
    "                            print(f\" Copied {stats['total_copied']} files...\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        stats['errors'].append(f\"Failed: {file_path.name}\")\n",
    "        \n",
    "        print(f\" Copied {folder_count} files from this folder\")\n",
    "        \n",
    "# Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\" DATASET SETUP COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\" Total invoices ready: {stats['total_copied']}\")\n",
    "    print(f\" Location: {EVAL_DIR.absolute()}\")\n",
    "    print(f\" Errors: {len(stats['errors'])}\")\n",
    "    if stats['total_copied'] >= 1000:\n",
    "        print(\"✅ 1000+ REQUIREMENT MET - READY FOR LARGE-SCALE TESTING!\")\n",
    "    else:\n",
    "        print(f\" Only {stats['total_copied']} files found - may need more datasets\")\n",
    "        \n",
    "    # Save simple report\n",
    "    with open(\"dataset_summary.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            'total_files': stats['total_copied'],\n",
    "            'setup_completed': datetime.now().isoformat(),\n",
    "            'dataset_location': str(EVAL_DIR.absolute()),\n",
    "            'ready_for_evaluation': stats['total_copied'] >= 1000\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Execute the setup\n",
    "print(\" STARTING LARGE-SCALE DATASET SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "setup_stats = setup_large_scale_invoice_dataset()\n",
    "\n",
    "if setup_stats['total_copied'] > 0:\n",
    "    # Create evaluator for large-scale testing\n",
    "    evaluator = ModelEvaluator()\n",
    "    print(f\"\\n✅ Ready for large-scale evaluation with {setup_stats['total_copied']} documents\")\n",
    "    print(\" Files location: ./large_scale_invoice_dataset/\")\n",
    "    print(\" Next: Run your evaluation system on this dataset\")\n",
    "else:\n",
    "    print(\"\\n No files copied - please check your source paths\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e08aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running SMART AUTOMATED evaluation on 100 files (from 106 available)...\n",
      "     PDFs: 75\n",
      "     Images: 31\n",
      " STARTING FULLY AUTOMATED EVALUATION PIPELINE\n",
      " PROCESSING MAXIMUM 100 FILES FOR QUICK RESULTS\n",
      "============================================================\n",
      "\n",
      " Step 1: Creating smart ground truth...\n",
      " Processing maximum 100 files for ground truth...\n",
      " Processing 1/100: invoice_10451.pdf\n",
      "Processing pdf file: invoice_10451.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      " Processing 2/100: invoice_10452.pdf\n",
      "Processing pdf file: invoice_10452.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      " Processing 3/100: invoice_10453.pdf\n",
      "Processing pdf file: invoice_10453.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      " Processing 4/100: invoice_10454.pdf\n",
      "Processing pdf file: invoice_10454.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      " Processing 5/100: invoice_10455.pdf\n",
      "Processing pdf file: invoice_10455.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      " Processing 6/100: invoice_10456.pdf\n",
      "Processing pdf file: invoice_10456.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.88)\n",
      " Processing 7/100: invoice_10457.pdf\n",
      "Processing pdf file: invoice_10457.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      " Processing 8/100: invoice_10458.pdf\n",
      "Processing pdf file: invoice_10458.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.89)\n",
      " Processing 9/100: invoice_10459.pdf\n",
      "Processing pdf file: invoice_10459.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.84)\n",
      " Processing 10/100: invoice_10460.pdf\n",
      "Processing pdf file: invoice_10460.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      " Processing 11/100: invoice_10461.pdf\n",
      "Processing pdf file: invoice_10461.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.79)\n",
      " Processing 12/100: invoice_10462.pdf\n",
      "Processing pdf file: invoice_10462.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.96)\n",
      " Processing 13/100: invoice_10463.pdf\n",
      "Processing pdf file: invoice_10463.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.88)\n",
      " Processing 14/100: invoice_10464.pdf\n",
      "Processing pdf file: invoice_10464.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      " Processing 15/100: invoice_10465.pdf\n",
      "Processing pdf file: invoice_10465.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.75)\n",
      " Processing 16/100: invoice_10466.pdf\n",
      "Processing pdf file: invoice_10466.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      " Processing 17/100: invoice_10467.pdf\n",
      "Processing pdf file: invoice_10467.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.88)\n",
      " Processing 18/100: invoice_10468.pdf\n",
      "Processing pdf file: invoice_10468.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.92)\n",
      " Processing 19/100: invoice_10469.pdf\n",
      "Processing pdf file: invoice_10469.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      " Processing 20/100: invoice_10470.pdf\n",
      "Processing pdf file: invoice_10470.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.80)\n",
      " Processing 21/100: invoice_10471.pdf\n",
      "Processing pdf file: invoice_10471.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.87)\n",
      " Processing 22/100: invoice_10472.pdf\n",
      "Processing pdf file: invoice_10472.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.94)\n",
      " Processing 23/100: invoice_10473.pdf\n",
      "Processing pdf file: invoice_10473.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 1.00)\n",
      " Processing 24/100: invoice_10474.pdf\n",
      "Processing pdf file: invoice_10474.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      " Processing 25/100: invoice_10475.pdf\n",
      "Processing pdf file: invoice_10475.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 1.00)\n",
      " Processing 26/100: invoice_10476.pdf\n",
      "Processing pdf file: invoice_10476.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      " Processing 27/100: invoice_10477.pdf\n",
      "Processing pdf file: invoice_10477.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      " Processing 28/100: invoice_10478.pdf\n",
      "Processing pdf file: invoice_10478.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 1.00)\n",
      " Processing 29/100: invoice_10479.pdf\n",
      "Processing pdf file: invoice_10479.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.88)\n",
      " Processing 30/100: invoice_10480.pdf\n",
      "Processing pdf file: invoice_10480.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      " Processing 31/100: invoice_10481.pdf\n",
      "Processing pdf file: invoice_10481.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      " Processing 32/100: invoice_10482.pdf\n",
      "Processing pdf file: invoice_10482.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.85)\n",
      " Processing 33/100: invoice_10483.pdf\n",
      "Processing pdf file: invoice_10483.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      " Processing 34/100: invoice_10484.pdf\n",
      "Processing pdf file: invoice_10484.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      " Processing 35/100: invoice_10485.pdf\n",
      "Processing pdf file: invoice_10485.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      " Processing 36/100: invoice_10486.pdf\n",
      "Processing pdf file: invoice_10486.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      " Processing 37/100: invoice_10487.pdf\n",
      "Processing pdf file: invoice_10487.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.88)\n",
      " Processing 38/100: invoice_10488.pdf\n",
      "Processing pdf file: invoice_10488.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      " Processing 39/100: invoice_10489.pdf\n",
      "Processing pdf file: invoice_10489.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 1.00)\n",
      " Processing 40/100: invoice_10490.pdf\n",
      "Processing pdf file: invoice_10490.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      " Processing 41/100: invoice_10491.pdf\n",
      "Processing pdf file: invoice_10491.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      " Processing 42/100: invoice_10492.pdf\n",
      "Processing pdf file: invoice_10492.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.86)\n",
      " Processing 43/100: invoice_10493.pdf\n",
      "Processing pdf file: invoice_10493.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      " Processing 44/100: invoice_10494.pdf\n",
      "Processing pdf file: invoice_10494.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.90)\n",
      " Processing 45/100: invoice_10495.pdf\n",
      "Processing pdf file: invoice_10495.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.77)\n",
      " Processing 46/100: invoice_10496.pdf\n",
      "Processing pdf file: invoice_10496.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.86)\n",
      " Processing 47/100: invoice_10497.pdf\n",
      "Processing pdf file: invoice_10497.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      " Processing 48/100: invoice_10498.pdf\n",
      "Processing pdf file: invoice_10498.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.82)\n",
      " Processing 49/100: invoice_10499.pdf\n",
      "Processing pdf file: invoice_10499.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.80)\n",
      " Processing 50/100: invoice_10500.pdf\n",
      "Processing pdf file: invoice_10500.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      " Processing 51/100: invoice_10501.pdf\n",
      "Processing pdf file: invoice_10501.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.86)\n",
      " Processing 52/100: invoice_10502.pdf\n",
      "Processing pdf file: invoice_10502.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      " Processing 53/100: invoice_10503.pdf\n",
      "Processing pdf file: invoice_10503.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.87)\n",
      " Processing 54/100: invoice_10504.pdf\n",
      "Processing pdf file: invoice_10504.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      " Processing 55/100: invoice_10505.pdf\n",
      "Processing pdf file: invoice_10505.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.93)\n",
      " Processing 56/100: invoice_10506.pdf\n",
      "Processing pdf file: invoice_10506.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.84)\n",
      " Processing 57/100: invoice_10507.pdf\n",
      "Processing pdf file: invoice_10507.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.95)\n",
      " Processing 58/100: invoice_10508.pdf\n",
      "Processing pdf file: invoice_10508.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.87)\n",
      " Processing 59/100: invoice_10509.pdf\n",
      "Processing pdf file: invoice_10509.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 1.00)\n",
      " Processing 60/100: invoice_10510.pdf\n",
      "Processing pdf file: invoice_10510.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.90)\n",
      " Processing 61/100: invoice_10511.pdf\n",
      "Processing pdf file: invoice_10511.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.82)\n",
      " Processing 62/100: invoice_10512.pdf\n",
      "Processing pdf file: invoice_10512.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.83)\n",
      " Processing 63/100: invoice_10513.pdf\n",
      "Processing pdf file: invoice_10513.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      " Processing 64/100: invoice_10514.pdf\n",
      "Processing pdf file: invoice_10514.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.90)\n",
      " Processing 65/100: invoice_10515.pdf\n",
      "Processing pdf file: invoice_10515.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      " Processing 66/100: invoice_10516.pdf\n",
      "Processing pdf file: invoice_10516.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.84)\n",
      " Processing 67/100: invoice_10517.pdf\n",
      "Processing pdf file: invoice_10517.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      " Processing 68/100: invoice_10518.pdf\n",
      "Processing pdf file: invoice_10518.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.92)\n",
      " Processing 69/100: invoice_10519.pdf\n",
      "Processing pdf file: invoice_10519.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.82)\n",
      " Processing 70/100: invoice_10520.pdf\n",
      "Processing pdf file: invoice_10520.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.86)\n",
      " Processing 71/100: Invoice_252-27878353-TI-1.pdf\n",
      "Processing pdf file: Invoice_252-27878353-TI-1.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P2' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.94)\n",
      " Processing 72/100: Rechnung 412955.pdf\n",
      "Processing pdf file: Rechnung 412955.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.92)\n",
      " Processing 73/100: Rechnung_1407606058.pdf\n",
      "Processing pdf file: Rechnung_1407606058.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.81)\n",
      " Processing 74/100: Strom Rechnung_October.pdf\n",
      "Processing pdf file: Strom Rechnung_October.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.79)\n",
      " Processing 75/100: SWME_Rechnung_07122020.pdf\n",
      "Processing pdf file: SWME_Rechnung_07122020.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.85)\n",
      " Processing 76/100: batch2-0121.jpg\n",
      "Processing image file: batch2-0121.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Insufficient validation - skipped\n",
      " Processing 77/100: batch2-0122.jpg\n",
      "Processing image file: batch2-0122.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Insufficient validation - skipped\n",
      " Processing 78/100: batch2-0123.jpg\n",
      "Processing image file: batch2-0123.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Insufficient validation - skipped\n",
      " Processing 79/100: batch2-0124.jpg\n",
      "Processing image file: batch2-0124.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.60)\n",
      " Processing 80/100: batch2-0125.jpg\n",
      "Processing image file: batch2-0125.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      " Processing 81/100: batch2-0126.jpg\n",
      "Processing image file: batch2-0126.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      " Processing 82/100: batch2-0127.jpg\n",
      "Processing image file: batch2-0127.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.70)\n",
      " Processing 83/100: batch2-0128.jpg\n",
      "Processing image file: batch2-0128.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 1.00)\n",
      " Processing 84/100: batch2-0129.jpg\n",
      "Processing image file: batch2-0129.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.73)\n",
      " Processing 85/100: batch2-0130.jpg\n",
      "Processing image file: batch2-0130.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      " Processing 86/100: batch2-0133.jpg\n",
      "Processing image file: batch2-0133.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 4 fields (score: 0.76)\n",
      " Processing 87/100: batch2-0134.jpg\n",
      "Processing image file: batch2-0134.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.70)\n",
      " Processing 88/100: batch2-0135.jpg\n",
      "Processing image file: batch2-0135.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      " Processing 89/100: batch2-0136.jpg\n",
      "Processing image file: batch2-0136.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.70)\n",
      " Processing 90/100: batch2-0137.jpg\n",
      "Processing image file: batch2-0137.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      " Processing 91/100: batch2-0138.jpg\n",
      "Processing image file: batch2-0138.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.65)\n",
      " Processing 92/100: batch2-0139.jpg\n",
      "Processing image file: batch2-0139.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.85)\n",
      " Processing 93/100: batch2-0140.jpg\n",
      "Processing image file: batch2-0140.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Insufficient validation - skipped\n",
      " Processing 94/100: batch2-0141.jpg\n",
      "Processing image file: batch2-0141.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.70)\n",
      " Processing 95/100: batch2-0142.jpg\n",
      "Processing image file: batch2-0142.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.70)\n",
      " Processing 96/100: batch2-0143.jpg\n",
      "Processing image file: batch2-0143.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Insufficient validation - skipped\n",
      " Processing 97/100: batch2-0154.jpg\n",
      "Processing image file: batch2-0154.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 2 fields (score: 0.65)\n",
      " Processing 98/100: batch2-0155.jpg\n",
      "Processing image file: batch2-0155.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 3 fields (score: 0.67)\n",
      " Processing 99/100: batch2-0156.jpg\n",
      "Processing image file: batch2-0156.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 1.00)\n",
      " Processing 100/100: batch2-0157.jpg\n",
      "Processing image file: batch2-0157.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Validated 1 fields (score: 0.70)\n",
      "\n",
      "✅ Smart ground truth created for 95 files (processed 100 total)\n",
      "✅ Ground truth created for 95 files\n",
      "\n",
      " Step 2: Running automated evaluation...\n",
      " PROCESSING FILES FOR EVALUATION RESULTS...\n",
      " Evaluating 1/95: invoice_10451.pdf\n",
      "Processing pdf file: invoice_10451.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 2/95: invoice_10452.pdf\n",
      "Processing pdf file: invoice_10452.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 3/95: invoice_10453.pdf\n",
      "Processing pdf file: invoice_10453.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 4/95: invoice_10454.pdf\n",
      "Processing pdf file: invoice_10454.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 5/95: invoice_10455.pdf\n",
      "Processing pdf file: invoice_10455.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 6/95: invoice_10456.pdf\n",
      "Processing pdf file: invoice_10456.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 7/95: invoice_10457.pdf\n",
      "Processing pdf file: invoice_10457.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 8/95: invoice_10458.pdf\n",
      "Processing pdf file: invoice_10458.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 9/95: invoice_10459.pdf\n",
      "Processing pdf file: invoice_10459.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 10/95: invoice_10460.pdf\n",
      "Processing pdf file: invoice_10460.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 11/95: invoice_10461.pdf\n",
      "Processing pdf file: invoice_10461.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 12/95: invoice_10462.pdf\n",
      "Processing pdf file: invoice_10462.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 13/95: invoice_10463.pdf\n",
      "Processing pdf file: invoice_10463.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 14/95: invoice_10464.pdf\n",
      "Processing pdf file: invoice_10464.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 15/95: invoice_10465.pdf\n",
      "Processing pdf file: invoice_10465.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 16/95: invoice_10466.pdf\n",
      "Processing pdf file: invoice_10466.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 17/95: invoice_10467.pdf\n",
      "Processing pdf file: invoice_10467.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 18/95: invoice_10468.pdf\n",
      "Processing pdf file: invoice_10468.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 19/95: invoice_10469.pdf\n",
      "Processing pdf file: invoice_10469.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 20/95: invoice_10470.pdf\n",
      "Processing pdf file: invoice_10470.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 21/95: invoice_10471.pdf\n",
      "Processing pdf file: invoice_10471.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 22/95: invoice_10472.pdf\n",
      "Processing pdf file: invoice_10472.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 22.6%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 23/95: invoice_10473.pdf\n",
      "Processing pdf file: invoice_10473.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 24/95: invoice_10474.pdf\n",
      "Processing pdf file: invoice_10474.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 25/95: invoice_10475.pdf\n",
      "Processing pdf file: invoice_10475.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 26/95: invoice_10476.pdf\n",
      "Processing pdf file: invoice_10476.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 35.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 27/95: invoice_10477.pdf\n",
      "Processing pdf file: invoice_10477.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 35.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 28/95: invoice_10478.pdf\n",
      "Processing pdf file: invoice_10478.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 29/95: invoice_10479.pdf\n",
      "Processing pdf file: invoice_10479.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 30/95: invoice_10480.pdf\n",
      "Processing pdf file: invoice_10480.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 31/95: invoice_10481.pdf\n",
      "Processing pdf file: invoice_10481.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 32/95: invoice_10482.pdf\n",
      "Processing pdf file: invoice_10482.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 19.4%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 33/95: invoice_10483.pdf\n",
      "Processing pdf file: invoice_10483.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 34/95: invoice_10484.pdf\n",
      "Processing pdf file: invoice_10484.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 35/95: invoice_10485.pdf\n",
      "Processing pdf file: invoice_10485.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 36/95: invoice_10486.pdf\n",
      "Processing pdf file: invoice_10486.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 37/95: invoice_10487.pdf\n",
      "Processing pdf file: invoice_10487.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 38/95: invoice_10488.pdf\n",
      "Processing pdf file: invoice_10488.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 39/95: invoice_10489.pdf\n",
      "Processing pdf file: invoice_10489.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 40/95: invoice_10490.pdf\n",
      "Processing pdf file: invoice_10490.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 41/95: invoice_10491.pdf\n",
      "Processing pdf file: invoice_10491.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 42/95: invoice_10492.pdf\n",
      "Processing pdf file: invoice_10492.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 43/95: invoice_10493.pdf\n",
      "Processing pdf file: invoice_10493.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 44/95: invoice_10494.pdf\n",
      "Processing pdf file: invoice_10494.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 35.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 45/95: invoice_10495.pdf\n",
      "Processing pdf file: invoice_10495.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 46/95: invoice_10496.pdf\n",
      "Processing pdf file: invoice_10496.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 35.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 47/95: invoice_10497.pdf\n",
      "Processing pdf file: invoice_10497.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 38.7%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 48/95: invoice_10498.pdf\n",
      "Processing pdf file: invoice_10498.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 49/95: invoice_10499.pdf\n",
      "Processing pdf file: invoice_10499.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 50/95: invoice_10500.pdf\n",
      "Processing pdf file: invoice_10500.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 51/95: invoice_10501.pdf\n",
      "Processing pdf file: invoice_10501.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 52/95: invoice_10502.pdf\n",
      "Processing pdf file: invoice_10502.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 53/95: invoice_10503.pdf\n",
      "Processing pdf file: invoice_10503.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 54/95: invoice_10504.pdf\n",
      "Processing pdf file: invoice_10504.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 55/95: invoice_10505.pdf\n",
      "Processing pdf file: invoice_10505.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 35.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 56/95: invoice_10506.pdf\n",
      "Processing pdf file: invoice_10506.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 57/95: invoice_10507.pdf\n",
      "Processing pdf file: invoice_10507.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 58/95: invoice_10508.pdf\n",
      "Processing pdf file: invoice_10508.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 59/95: invoice_10509.pdf\n",
      "Processing pdf file: invoice_10509.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 60/95: invoice_10510.pdf\n",
      "Processing pdf file: invoice_10510.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 61/95: invoice_10511.pdf\n",
      "Processing pdf file: invoice_10511.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 22.6%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 62/95: invoice_10512.pdf\n",
      "Processing pdf file: invoice_10512.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 63/95: invoice_10513.pdf\n",
      "Processing pdf file: invoice_10513.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 64/95: invoice_10514.pdf\n",
      "Processing pdf file: invoice_10514.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 35.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 65/95: invoice_10515.pdf\n",
      "Processing pdf file: invoice_10515.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 66/95: invoice_10516.pdf\n",
      "Processing pdf file: invoice_10516.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 67/95: invoice_10517.pdf\n",
      "Processing pdf file: invoice_10517.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.8%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 68/95: invoice_10518.pdf\n",
      "Processing pdf file: invoice_10518.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 32.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 69/95: invoice_10519.pdf\n",
      "Processing pdf file: invoice_10519.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 41.9%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 70/95: invoice_10520.pdf\n",
      "Processing pdf file: invoice_10520.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 29.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 71/95: Invoice_252-27878353-TI-1.pdf\n",
      "Processing pdf file: Invoice_252-27878353-TI-1.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P2' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 77.4%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 72/95: Rechnung 412955.pdf\n",
      "Processing pdf file: Rechnung 412955.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 71.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 73/95: Rechnung_1407606058.pdf\n",
      "Processing pdf file: Rechnung_1407606058.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 64.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 74/95: Strom Rechnung_October.pdf\n",
      "Processing pdf file: Strom Rechnung_October.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 58.1%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 75/95: SWME_Rechnung_07122020.pdf\n",
      "Processing pdf file: SWME_Rechnung_07122020.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 61.3%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 76/95: batch2-0124.jpg\n",
      "Processing image file: batch2-0124.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 12.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 77/95: batch2-0125.jpg\n",
      "Processing image file: batch2-0125.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 12.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 78/95: batch2-0126.jpg\n",
      "Processing image file: batch2-0126.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 79/95: batch2-0127.jpg\n",
      "Processing image file: batch2-0127.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 80/95: batch2-0128.jpg\n",
      "Processing image file: batch2-0128.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 12.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 81/95: batch2-0129.jpg\n",
      "Processing image file: batch2-0129.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 41.9%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 82/95: batch2-0130.jpg\n",
      "Processing image file: batch2-0130.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 12.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 83/95: batch2-0133.jpg\n",
      "Processing image file: batch2-0133.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 38.7%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 84/95: batch2-0134.jpg\n",
      "Processing image file: batch2-0134.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 37.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 85/95: batch2-0135.jpg\n",
      "Processing image file: batch2-0135.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 12.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 86/95: batch2-0136.jpg\n",
      "Processing image file: batch2-0136.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 87/95: batch2-0137.jpg\n",
      "Processing image file: batch2-0137.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 12.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 88/95: batch2-0138.jpg\n",
      "Processing image file: batch2-0138.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 89/95: batch2-0139.jpg\n",
      "Processing image file: batch2-0139.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 90/95: batch2-0141.jpg\n",
      "Processing image file: batch2-0141.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 37.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 91/95: batch2-0142.jpg\n",
      "Processing image file: batch2-0142.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 92/95: batch2-0154.jpg\n",
      "Processing image file: batch2-0154.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 93/95: batch2-0155.jpg\n",
      "Processing image file: batch2-0155.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 37.5%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 94/95: batch2-0156.jpg\n",
      "Processing image file: batch2-0156.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      " Evaluating 95/95: batch2-0157.jpg\n",
      "Processing image file: batch2-0157.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Success rate: 25.0%\n",
      "   ✅ Accuracy: 100.0%\n",
      "\n",
      "============================================================\n",
      " FULLY AUTOMATED EVALUATION COMPLETE!\n",
      " EVALUATION RESULTS SUMMARY:\n",
      "    Files Processed: 95\n",
      "   ✅ Average Success Rate: 30.7%\n",
      "    Average Accuracy: 100.0%\n",
      "    Average Confidence: 0.155\n",
      "    Ground Truth Files: 95\n",
      "============================================================\n",
      " EVALUATION COMPLETED! YOU HAVE YOUR RESULTS!\n",
      "✅ Total Results: 95 documents evaluated\n",
      " Performance Report saved to: automation_report.json\n"
     ]
    }
   ],
   "source": [
    "# Step 4C: Smart Automated Batch Evaluation--100\n",
    "\n",
    "class SmartAutomatedEvaluator:\n",
    "    \"\"\"Combines smart validation with full automation for zero human intervention\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluator = ModelEvaluator()\n",
    "        self.confidence_patterns = {\n",
    "            \"invoice_number\": [r\"[A-Z0-9\\-]{5,15}\", r\"\\d{6,12}\"],\n",
    "            \"total_amount\": [r\"[\\d,\\.]+\\s*€\", r\"€\\s*[\\d,\\.]+\", r\"\\d+[,\\.]\\d{2}\"],\n",
    "            \"company_name\": [r\"[A-Z][a-zA-Z\\s&,\\.]{3,30}\"],\n",
    "            \"invoice_date\": [r\"\\d{1,2}[./\\-]\\d{1,2}[./\\-]\\d{2,4}\"]\n",
    "        }\n",
    "    \n",
    "    def create_smart_ground_truth(self, invoice_folder=\"./large_scale_invoice_dataset\", max_files=100):\n",
    "        \"\"\"Creates validated ground truth automatically - LIMITED TO 100 FILES\"\"\"\n",
    "        \n",
    "        print(f\" Processing maximum {max_files} files for ground truth...\")\n",
    "        \n",
    "        smart_gt = {}\n",
    "        validation_scores = {}\n",
    "        processed_count = 0\n",
    "        \n",
    "        # Process each file type with LIMIT\n",
    "        for pattern in [\"*.pdf\", \"*.png\", \"*.jpg\", \"*.jpeg\"]:\n",
    "            if processed_count >= max_files:\n",
    "                break\n",
    "                \n",
    "            for invoice_file in Path(invoice_folder).glob(pattern):\n",
    "                if processed_count >= max_files:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    print(f\" Processing {processed_count + 1}/{max_files}: {invoice_file.name}\")\n",
    "                    \n",
    "                    # Extract with existing system\n",
    "                    result = enhanced_load_and_normalize_with_qa(str(invoice_file))\n",
    "                    extractions = result['qa_extraction']['extractions']\n",
    "                    \n",
    "                    # Smart validation for each extraction\n",
    "                    validated_data = {}\n",
    "                    file_validation_score = 0\n",
    "                    total_validations = 0\n",
    "                    \n",
    "                    for question, answer_data in extractions.items():\n",
    "                        confidence = answer_data.get('confidence', 0)\n",
    "                        answer = answer_data.get('answer', '')\n",
    "                        \n",
    "                        if answer is None:\n",
    "                            answer = ''\n",
    "                        else:\n",
    "                            answer = str(answer).strip()\n",
    "                        \n",
    "                        if not answer or len(answer) < 2:\n",
    "                            continue\n",
    "                        \n",
    "                        # Multi-criteria validation\n",
    "                        validation_score = 0\n",
    "                        field = self._map_question_to_field(question)\n",
    "                        \n",
    "                        # Confidence threshold\n",
    "                        if confidence >= 0.3:\n",
    "                            validation_score += 0.3\n",
    "                        \n",
    "                        # Pattern matching\n",
    "                        if field and self._validate_pattern(field, answer):\n",
    "                            validation_score += 0.4\n",
    "                        \n",
    "                        # Length and format check\n",
    "                        if 2 <= len(answer) <= 50 and not answer.isspace():\n",
    "                            validation_score += 0.2\n",
    "                        \n",
    "                        # Cross-reference check\n",
    "                        if self._cross_validate_answer(answer, extractions, field):\n",
    "                            validation_score += 0.1\n",
    "                        \n",
    "                        # Accept if validation score >= 0.6\n",
    "                        if validation_score >= 0.6 and field:\n",
    "                            validated_data[field] = answer\n",
    "                            file_validation_score += validation_score\n",
    "                            total_validations += 1\n",
    "                    \n",
    "                    # Include files with ANY validated extractions (lowered threshold)\n",
    "                    if len(validated_data) >= 1:\n",
    "                        smart_gt[invoice_file.name] = validated_data\n",
    "                        validation_scores[invoice_file.name] = file_validation_score / total_validations if total_validations > 0 else 0\n",
    "                        print(f\"   ✅ Validated {len(validated_data)} fields (score: {validation_scores[invoice_file.name]:.2f})\")\n",
    "                    else:\n",
    "                        print(f\"   ❌ Insufficient validation - skipped\")\n",
    "                    \n",
    "                    processed_count += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Error: {e}\")\n",
    "                    processed_count += 1  # Count even failed files\n",
    "        \n",
    "        # Save smart ground truth\n",
    "        with open(\"smart_auto_ground_truth.json\", \"w\") as f:\n",
    "            json.dump(smart_gt, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n✅ Smart ground truth created for {len(smart_gt)} files (processed {processed_count} total)\")\n",
    "        return smart_gt\n",
    "    \n",
    "    def _validate_pattern(self, field, value):\n",
    "        \"\"\"Validate using regex patterns with null checking\"\"\"\n",
    "        if not value or field not in self.confidence_patterns:\n",
    "            return True\n",
    "        \n",
    "        patterns = self.confidence_patterns[field]\n",
    "        return any(re.search(pattern, str(value), re.IGNORECASE) for pattern in patterns)\n",
    "    \n",
    "    def _cross_validate_answer(self, answer, all_extractions, field):\n",
    "        \"\"\"Cross-validate with other answers\"\"\"\n",
    "        if not answer:\n",
    "            return False\n",
    "            \n",
    "        similar_count = 0\n",
    "        for q, data in all_extractions.items():\n",
    "            other_answer = data.get('answer', '')\n",
    "            if other_answer and self._similarity_score(answer, other_answer) > 0.7:\n",
    "                similar_count += 1\n",
    "        return similar_count >= 1\n",
    "    \n",
    "    def _similarity_score(self, text1, text2):\n",
    "        \"\"\"Calculate similarity between two text strings\"\"\"\n",
    "        if not text1 or not text2:\n",
    "            return 0\n",
    "        return SequenceMatcher(None, str(text1).lower(), str(text2).lower()).ratio()\n",
    "    \n",
    "    def _map_question_to_field(self, question):\n",
    "        \"\"\"Map questions to standard ground truth fields\"\"\"\n",
    "        if not question:\n",
    "            return None\n",
    "            \n",
    "        q_lower = question.lower()\n",
    "        if any(kw in q_lower for kw in [\"rechnungsnummer\", \"invoice number\", \"nummer\"]):\n",
    "            return \"invoice_number\"\n",
    "        elif any(kw in q_lower for kw in [\"gesamtbetrag\", \"total\", \"amount\", \"endbetrag\"]):\n",
    "            return \"total_amount\"\n",
    "        elif any(kw in q_lower for kw in [\"company\", \"firma\", \"vendor\", \"firmenname\"]):\n",
    "            return \"company_name\"\n",
    "        elif any(kw in q_lower for kw in [\"datum\", \"date\", \"rechnungsdatum\"]):\n",
    "            return \"invoice_date\"\n",
    "        return None\n",
    "    \n",
    "    def run_fully_automated_evaluation(self, max_files=100):\n",
    "        \"\"\"Complete automated pipeline with LIMIT - ACTUALLY COMPLETES EVALUATION\"\"\"\n",
    "        \n",
    "        print(f\" STARTING FULLY AUTOMATED EVALUATION PIPELINE\")\n",
    "        print(f\" PROCESSING MAXIMUM {max_files} FILES FOR QUICK RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Create smart ground truth with LIMIT\n",
    "        print(\"\\n Step 1: Creating smart ground truth...\")\n",
    "        smart_gt = self.create_smart_ground_truth(max_files=max_files)\n",
    "        \n",
    "        if not smart_gt:\n",
    "            print(\"❌ No valid ground truth data created - check your invoice files\")\n",
    "            return {}, {}, []\n",
    "        \n",
    "        print(f\"✅ Ground truth created for {len(smart_gt)} files\")\n",
    "        \n",
    "        # Step 2: ACTUALLY RUN THE EVALUATION NOW\n",
    "        print(\"\\n Step 2: Running automated evaluation...\")\n",
    "        print(\" PROCESSING FILES FOR EVALUATION RESULTS...\")\n",
    "        \n",
    "        # Get the files that have ground truth\n",
    "        files_to_evaluate = list(smart_gt.keys())[:max_files]  # Limit to first N files\n",
    "        \n",
    "        evaluation_results = []\n",
    "        \n",
    "        for i, filename in enumerate(files_to_evaluate):\n",
    "            file_path = Path(\"./large_scale_invoice_dataset\") / filename\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                continue\n",
    "                \n",
    "            print(f\" Evaluating {i+1}/{len(files_to_evaluate)}: {filename}\")\n",
    "            \n",
    "            try:\n",
    "                # Extract with your system\n",
    "                result = enhanced_load_and_normalize_with_qa(str(file_path))\n",
    "                \n",
    "                # Get ground truth for this file\n",
    "                gt = smart_gt.get(filename, {})\n",
    "                \n",
    "                # Evaluate using your existing evaluator\n",
    "                evaluation = self.evaluator.evaluate_extraction_results(result, gt, filename)\n",
    "                evaluation_results.append(evaluation)\n",
    "                \n",
    "                print(f\"   ✅ Success rate: {evaluation['success_rate']:.1%}\")\n",
    "                if gt:\n",
    "                    print(f\"   ✅ Accuracy: {evaluation.get('ground_truth_accuracy', 0):.1%}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error evaluating {filename}: {e}\")\n",
    "        \n",
    "        # Step 3: Generate comprehensive report\n",
    "        automation_report = {\n",
    "            \"pipeline_completed_at\": datetime.now().isoformat(),\n",
    "            \"files_processed\": len(files_to_evaluate),\n",
    "            \"successful_evaluations\": len(evaluation_results),\n",
    "            \"smart_ground_truth\": {\n",
    "                \"files_validated\": len(smart_gt),\n",
    "                \"validation_method\": \"multi_criteria_smart_validation\"\n",
    "            },\n",
    "            \"evaluation_results\": {\n",
    "                \"total_processed\": len(evaluation_results),\n",
    "                \"avg_success_rate\": sum(r['success_rate'] for r in evaluation_results) / len(evaluation_results) if evaluation_results else 0,\n",
    "                \"avg_confidence\": sum(r['avg_confidence'] for r in evaluation_results) / len(evaluation_results) if evaluation_results else 0,\n",
    "                \"avg_accuracy\": sum(r.get('ground_truth_accuracy', 0) for r in evaluation_results) / len(evaluation_results) if evaluation_results else 0\n",
    "            },\n",
    "            \"automation_level\": \"full_zero_intervention\"\n",
    "        }\n",
    "        \n",
    "        with open(\"automation_report.json\", \"w\") as f:\n",
    "            json.dump(automation_report, f, indent=2)\n",
    "        \n",
    "        # FINAL RESULTS SUMMARY\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(f\" FULLY AUTOMATED EVALUATION COMPLETE!\")\n",
    "        print(f\" EVALUATION RESULTS SUMMARY:\")\n",
    "        print(f\"    Files Processed: {len(evaluation_results)}\")\n",
    "        print(f\"   ✅ Average Success Rate: {automation_report['evaluation_results']['avg_success_rate']:.1%}\")\n",
    "        print(f\"    Average Accuracy: {automation_report['evaluation_results']['avg_accuracy']:.1%}\")\n",
    "        print(f\"    Average Confidence: {automation_report['evaluation_results']['avg_confidence']:.3f}\")\n",
    "        print(f\"    Ground Truth Files: {len(smart_gt)}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return automation_report, smart_gt, evaluation_results\n",
    "\n",
    "# EXECUTE THE FIXED VERSION - WILL COMPLETE IN ~10-15 MINUTES\n",
    "if os.path.exists(\"./large_scale_invoice_dataset\"):\n",
    "    pdf_files = list(Path(\"./large_scale_invoice_dataset\").glob(\"*.pdf\"))\n",
    "    image_files = list(Path(\"./large_scale_invoice_dataset\").glob(\"*.png\")) + list(Path(\"./large_scale_invoice_dataset\").glob(\"*.jpg\"))\n",
    "    \n",
    "    total_files = len(pdf_files) + len(image_files)\n",
    "    \n",
    "    if total_files > 0:\n",
    "        print(f\" Running SMART AUTOMATED evaluation on 100 files (from {total_files} available)...\")\n",
    "        print(f\"     PDFs: {len(pdf_files)}\")\n",
    "        print(f\"     Images: {len(image_files)}\")\n",
    "        \n",
    "        smart_evaluator = SmartAutomatedEvaluator()\n",
    "        \n",
    "        # THIS WILL ACTUALLY COMPLETE THE EVALUATION!\n",
    "        automation_report, ground_truth, evaluation_results = smart_evaluator.run_fully_automated_evaluation(max_files=100)\n",
    "        \n",
    "        if evaluation_results:\n",
    "            print(\" EVALUATION COMPLETED! YOU HAVE YOUR RESULTS!\")\n",
    "            print(f\"✅ Total Results: {len(evaluation_results)} documents evaluated\")\n",
    "            print(f\" Performance Report saved to: automation_report.json\")\n",
    "        else:\n",
    "            print(\" Evaluation completed but no valid results generated\")\n",
    "    else:\n",
    "        print(\"❌ No supported files (PDF/PNG/JPG) found in ./large_scale_invoice_dataset\")\n",
    "else:\n",
    "    print(\"❌ ./large_scale_invoice_dataset folder not found\")\n",
    "    print(\"   Please ensure Step 4A copied the files correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb04469",
   "metadata": {},
   "source": [
    "**IMPORTANT** 100% accuracy ABOVE is artificial because the system created its own \"answer key\" and then compared against it**\n",
    "\n",
    "**4C = Automated, rule-based, synthetic evaluation**\n",
    "\n",
    "**4D = Automated, consensus-based, statistically robust evaluation (closer to real accuracy)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab19b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " INITIALIZING ROBUST AUTOMATED ACCURACY EVALUATOR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92c103dad40483dadc7fce80598de7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f20d4c339e44a7989a15503b8450ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d772d099d84f99842bb9f8a6ceb9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb16b9b74c54bdb88f8c6a4ab2b3776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d518593957e74af08a27ef8bd080826d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef89cccc4b384fc0a113135d81077154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee5030d4c1b49daa5311cbf2a86bd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcac2cef7de49efb10d05bd803b0174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed49ab859c9f4d1e9df8db56d7836e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RUNNING CONSENSUS-BASED ACCURACY EVALUATION\n",
      " Processing up to 100 files with multiple AI models\n",
      "============================================================\n",
      " Found 106 files, processing 100\n",
      "\n",
      "[1/100] Processing: invoice_10451.pdf\n",
      "Processing pdf file: invoice_10451.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[2/100] Processing: invoice_10452.pdf\n",
      "Processing pdf file: invoice_10452.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[3/100] Processing: invoice_10453.pdf\n",
      "Processing pdf file: invoice_10453.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 1.000\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[4/100] Processing: invoice_10454.pdf\n",
      "Processing pdf file: invoice_10454.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[5/100] Processing: invoice_10455.pdf\n",
      "Processing pdf file: invoice_10455.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[6/100] Processing: invoice_10456.pdf\n",
      "Processing pdf file: invoice_10456.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[7/100] Processing: invoice_10457.pdf\n",
      "Processing pdf file: invoice_10457.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[8/100] Processing: invoice_10458.pdf\n",
      "Processing pdf file: invoice_10458.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[9/100] Processing: invoice_10459.pdf\n",
      "Processing pdf file: invoice_10459.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[10/100] Processing: invoice_10460.pdf\n",
      "Processing pdf file: invoice_10460.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[11/100] Processing: invoice_10461.pdf\n",
      "Processing pdf file: invoice_10461.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[12/100] Processing: invoice_10462.pdf\n",
      "Processing pdf file: invoice_10462.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[13/100] Processing: invoice_10463.pdf\n",
      "Processing pdf file: invoice_10463.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[14/100] Processing: invoice_10464.pdf\n",
      "Processing pdf file: invoice_10464.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[15/100] Processing: invoice_10465.pdf\n",
      "Processing pdf file: invoice_10465.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[16/100] Processing: invoice_10466.pdf\n",
      "Processing pdf file: invoice_10466.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[17/100] Processing: invoice_10467.pdf\n",
      "Processing pdf file: invoice_10467.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[18/100] Processing: invoice_10468.pdf\n",
      "Processing pdf file: invoice_10468.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[19/100] Processing: invoice_10469.pdf\n",
      "Processing pdf file: invoice_10469.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[20/100] Processing: invoice_10470.pdf\n",
      "Processing pdf file: invoice_10470.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[21/100] Processing: invoice_10471.pdf\n",
      "Processing pdf file: invoice_10471.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[22/100] Processing: invoice_10472.pdf\n",
      "Processing pdf file: invoice_10472.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[23/100] Processing: invoice_10473.pdf\n",
      "Processing pdf file: invoice_10473.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[24/100] Processing: invoice_10474.pdf\n",
      "Processing pdf file: invoice_10474.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[25/100] Processing: invoice_10475.pdf\n",
      "Processing pdf file: invoice_10475.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[26/100] Processing: invoice_10476.pdf\n",
      "Processing pdf file: invoice_10476.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[27/100] Processing: invoice_10477.pdf\n",
      "Processing pdf file: invoice_10477.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 1.000\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[28/100] Processing: invoice_10478.pdf\n",
      "Processing pdf file: invoice_10478.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[29/100] Processing: invoice_10479.pdf\n",
      "Processing pdf file: invoice_10479.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[30/100] Processing: invoice_10480.pdf\n",
      "Processing pdf file: invoice_10480.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[31/100] Processing: invoice_10481.pdf\n",
      "Processing pdf file: invoice_10481.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[32/100] Processing: invoice_10482.pdf\n",
      "Processing pdf file: invoice_10482.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[33/100] Processing: invoice_10483.pdf\n",
      "Processing pdf file: invoice_10483.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[34/100] Processing: invoice_10484.pdf\n",
      "Processing pdf file: invoice_10484.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[35/100] Processing: invoice_10485.pdf\n",
      "Processing pdf file: invoice_10485.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[36/100] Processing: invoice_10486.pdf\n",
      "Processing pdf file: invoice_10486.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[37/100] Processing: invoice_10487.pdf\n",
      "Processing pdf file: invoice_10487.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[38/100] Processing: invoice_10488.pdf\n",
      "Processing pdf file: invoice_10488.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[39/100] Processing: invoice_10489.pdf\n",
      "Processing pdf file: invoice_10489.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[40/100] Processing: invoice_10490.pdf\n",
      "Processing pdf file: invoice_10490.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[41/100] Processing: invoice_10491.pdf\n",
      "Processing pdf file: invoice_10491.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[42/100] Processing: invoice_10492.pdf\n",
      "Processing pdf file: invoice_10492.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[43/100] Processing: invoice_10493.pdf\n",
      "Processing pdf file: invoice_10493.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[44/100] Processing: invoice_10494.pdf\n",
      "Processing pdf file: invoice_10494.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[45/100] Processing: invoice_10495.pdf\n",
      "Processing pdf file: invoice_10495.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[46/100] Processing: invoice_10496.pdf\n",
      "Processing pdf file: invoice_10496.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[47/100] Processing: invoice_10497.pdf\n",
      "Processing pdf file: invoice_10497.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[48/100] Processing: invoice_10498.pdf\n",
      "Processing pdf file: invoice_10498.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[49/100] Processing: invoice_10499.pdf\n",
      "Processing pdf file: invoice_10499.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[50/100] Processing: invoice_10500.pdf\n",
      "Processing pdf file: invoice_10500.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[51/100] Processing: invoice_10501.pdf\n",
      "Processing pdf file: invoice_10501.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 1.000\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[52/100] Processing: invoice_10502.pdf\n",
      "Processing pdf file: invoice_10502.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[53/100] Processing: invoice_10503.pdf\n",
      "Processing pdf file: invoice_10503.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[54/100] Processing: invoice_10504.pdf\n",
      "Processing pdf file: invoice_10504.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[55/100] Processing: invoice_10505.pdf\n",
      "Processing pdf file: invoice_10505.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[56/100] Processing: invoice_10506.pdf\n",
      "Processing pdf file: invoice_10506.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[57/100] Processing: invoice_10507.pdf\n",
      "Processing pdf file: invoice_10507.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[58/100] Processing: invoice_10508.pdf\n",
      "Processing pdf file: invoice_10508.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[59/100] Processing: invoice_10509.pdf\n",
      "Processing pdf file: invoice_10509.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[60/100] Processing: invoice_10510.pdf\n",
      "Processing pdf file: invoice_10510.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[61/100] Processing: invoice_10511.pdf\n",
      "Processing pdf file: invoice_10511.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[62/100] Processing: invoice_10512.pdf\n",
      "Processing pdf file: invoice_10512.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[63/100] Processing: invoice_10513.pdf\n",
      "Processing pdf file: invoice_10513.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[64/100] Processing: invoice_10514.pdf\n",
      "Processing pdf file: invoice_10514.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[65/100] Processing: invoice_10515.pdf\n",
      "Processing pdf file: invoice_10515.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[66/100] Processing: invoice_10516.pdf\n",
      "Processing pdf file: invoice_10516.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[67/100] Processing: invoice_10517.pdf\n",
      "Processing pdf file: invoice_10517.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[68/100] Processing: invoice_10518.pdf\n",
      "Processing pdf file: invoice_10518.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[69/100] Processing: invoice_10519.pdf\n",
      "Processing pdf file: invoice_10519.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 1.000\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[70/100] Processing: invoice_10520.pdf\n",
      "Processing pdf file: invoice_10520.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[71/100] Processing: Invoice_252-27878353-TI-1.pdf\n",
      "Processing pdf file: Invoice_252-27878353-TI-1.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P2' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.500\n",
      "   Primary Accuracy: 0.333\n",
      "\n",
      "[72/100] Processing: Rechnung 412955.pdf\n",
      "Processing pdf file: Rechnung 412955.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.500\n",
      "   Primary Accuracy: 0.333\n",
      "\n",
      "[73/100] Processing: Rechnung_1407606058.pdf\n",
      "Processing pdf file: Rechnung_1407606058.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.583\n",
      "   Primary Accuracy: 0.333\n",
      "\n",
      "[74/100] Processing: Strom Rechnung_October.pdf\n",
      "Processing pdf file: Strom Rechnung_October.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.500\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[75/100] Processing: SWME_Rechnung_07122020.pdf\n",
      "Processing pdf file: SWME_Rechnung_07122020.pdf\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.417\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[76/100] Processing: batch2-0121.jpg\n",
      "Processing image file: batch2-0121.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[77/100] Processing: batch2-0122.jpg\n",
      "Processing image file: batch2-0122.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[78/100] Processing: batch2-0123.jpg\n",
      "Processing image file: batch2-0123.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 1.000\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[79/100] Processing: batch2-0124.jpg\n",
      "Processing image file: batch2-0124.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[80/100] Processing: batch2-0125.jpg\n",
      "Processing image file: batch2-0125.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[81/100] Processing: batch2-0126.jpg\n",
      "Processing image file: batch2-0126.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.500\n",
      "\n",
      "[82/100] Processing: batch2-0127.jpg\n",
      "Processing image file: batch2-0127.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.667\n",
      "\n",
      "[83/100] Processing: batch2-0128.jpg\n",
      "Processing image file: batch2-0128.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 0.500\n",
      "\n",
      "[84/100] Processing: batch2-0129.jpg\n",
      "Processing image file: batch2-0129.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.333\n",
      "\n",
      "[85/100] Processing: batch2-0130.jpg\n",
      "Processing image file: batch2-0130.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.500\n",
      "\n",
      "[86/100] Processing: batch2-0133.jpg\n",
      "Processing image file: batch2-0133.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.667\n",
      "\n",
      "[87/100] Processing: batch2-0134.jpg\n",
      "Processing image file: batch2-0134.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.667\n",
      "\n",
      "[88/100] Processing: batch2-0135.jpg\n",
      "Processing image file: batch2-0135.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 0.500\n",
      "\n",
      "[89/100] Processing: batch2-0136.jpg\n",
      "Processing image file: batch2-0136.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.667\n",
      "\n",
      "[90/100] Processing: batch2-0137.jpg\n",
      "Processing image file: batch2-0137.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[91/100] Processing: batch2-0138.jpg\n",
      "Processing image file: batch2-0138.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.500\n",
      "\n",
      "[92/100] Processing: batch2-0139.jpg\n",
      "Processing image file: batch2-0139.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.667\n",
      "\n",
      "[93/100] Processing: batch2-0140.jpg\n",
      "Processing image file: batch2-0140.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[94/100] Processing: batch2-0141.jpg\n",
      "Processing image file: batch2-0141.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.667\n",
      "\n",
      "[95/100] Processing: batch2-0142.jpg\n",
      "Processing image file: batch2-0142.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.667\n",
      "\n",
      "[96/100] Processing: batch2-0143.jpg\n",
      "Processing image file: batch2-0143.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.833\n",
      "   Primary Accuracy: 0.000\n",
      "\n",
      "[97/100] Processing: batch2-0154.jpg\n",
      "Processing image file: batch2-0154.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[98/100] Processing: batch2-0155.jpg\n",
      "Processing image file: batch2-0155.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.750\n",
      "   Primary Accuracy: 0.667\n",
      "\n",
      "[99/100] Processing: batch2-0156.jpg\n",
      "Processing image file: batch2-0156.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 1.000\n",
      "\n",
      "[100/100] Processing: batch2-0157.jpg\n",
      "Processing image file: batch2-0157.jpg\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Consensus Strength: 0.917\n",
      "   Primary Accuracy: 0.500\n",
      "\n",
      "============================================================\n",
      " AUTOMATED ACCURACY EVALUATION COMPLETE!\n",
      "============================================================\n",
      " Files Processed: 100\n",
      " Mean Accuracy: 65.7%\n",
      " Consensus Strength: 0.825\n",
      " Reliability Rate: 95.0%\n",
      " Statistical Confidence: SIGNIFICANT\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 4D: Real Accuracy Evaluation (needed for comprehensive, transparent, and scalable evaluation for reseach)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import pipeline\n",
    "import statistics\n",
    "\n",
    "class AccuracyEvaluator:\n",
    "    \"\"\"\n",
    "    Reliable accuracy evaluation using multiple AI models as cross-validators\n",
    "    + Statistical confidence measures + Synthetic ground truth generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Multiple different QA models for cross-validation\n",
    "        self.models = {\n",
    "            'primary': pipeline(\"question-answering\", model=\"deepset/xlm-roberta-large-squad2\"),\n",
    "            'validator1': pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\"),\n",
    "            'validator2': pipeline(\"question-answering\", model=\"deepset/minilm-uncased-squad2\"),\n",
    "        }\n",
    "        \n",
    "        # Different extraction strategies\n",
    "        self.strategies = ['conservative', 'aggressive', 'balanced']\n",
    "        \n",
    "    def multi_model_consensus_evaluation(self, dataset_folder=\"./large_scale_invoice_dataset\", max_files=100):\n",
    "        \"\"\"\n",
    "        Use multiple AI models to create consensus-based ground truth\n",
    "        This gives realistic accuracy without manual annotation\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\" RUNNING CONSENSUS-BASED ACCURACY EVALUATION\")\n",
    "        print(f\" Processing up to {max_files} files with multiple AI models\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get all files\n",
    "        all_files = []\n",
    "        for pattern in [\"*.pdf\", \"*.png\", \"*.jpg\", \"*.jpeg\"]:\n",
    "            files = list(Path(dataset_folder).glob(pattern))\n",
    "            all_files.extend(files)\n",
    "        \n",
    "        # Process actual number of files (up to max_files)\n",
    "        files_to_process = all_files[:max_files]\n",
    "        print(f\" Found {len(all_files)} files, processing {len(files_to_process)}\")\n",
    "        \n",
    "        evaluation_results = []\n",
    "        consensus_ground_truth = {}\n",
    "        \n",
    "        # Key questions for invoice extraction\n",
    "        key_questions = [\n",
    "            \"What is the invoice number?\",\n",
    "            \"What is the total amount?\", \n",
    "            \"What is the company name?\",\n",
    "            \"What is the invoice date?\"\n",
    "        ]\n",
    "        \n",
    "        for i, file_path in enumerate(files_to_process):\n",
    "            print(f\"\\n[{i+1}/{len(files_to_process)}] Processing: {file_path.name}\")\n",
    "            \n",
    "            try:\n",
    "                # Extract text\n",
    "                result = enhanced_load_and_normalize_with_qa(str(file_path))\n",
    "                all_text = \"\"\n",
    "                for content_item in result['content']:\n",
    "                    all_text += content_item['text'] + \" \"\n",
    "                \n",
    "                # Get predictions from all models\n",
    "                model_predictions = {}\n",
    "                for model_name, model_pipeline in self.models.items():\n",
    "                    predictions = {}\n",
    "                    for question in key_questions:\n",
    "                        try:\n",
    "                            pred = model_pipeline(question=question, context=all_text)\n",
    "                            predictions[question] = {\n",
    "                                'answer': pred['answer'],\n",
    "                                'confidence': pred['score']\n",
    "                            }\n",
    "                        except:\n",
    "                            predictions[question] = {'answer': '', 'confidence': 0.0}\n",
    "                    \n",
    "                    model_predictions[model_name] = predictions\n",
    "                \n",
    "                # Calculate consensus and disagreement\n",
    "                consensus_data = self._calculate_consensus(model_predictions, key_questions)\n",
    "                \n",
    "                # Evaluate primary system against consensus\n",
    "                primary_accuracy = self._evaluate_against_consensus(\n",
    "                    result['qa_extraction']['extractions'], \n",
    "                    consensus_data['consensus_answers']\n",
    "                )\n",
    "                \n",
    "                file_result = {\n",
    "                    'filename': file_path.name,\n",
    "                    'consensus_strength': consensus_data['consensus_strength'],\n",
    "                    'model_agreement_rate': consensus_data['agreement_rate'],\n",
    "                    'primary_model_accuracy': primary_accuracy,\n",
    "                    'confidence_variance': consensus_data['confidence_variance'],\n",
    "                    'reliable_extraction': consensus_data['consensus_strength'] > 0.6\n",
    "                }\n",
    "                \n",
    "                evaluation_results.append(file_result)\n",
    "                consensus_ground_truth[file_path.name] = consensus_data\n",
    "                \n",
    "                print(f\"   Consensus Strength: {consensus_data['consensus_strength']:.3f}\")\n",
    "                print(f\"   Primary Accuracy: {primary_accuracy:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Generate comprehensive statistics\n",
    "        return self.statistical_report(evaluation_results, consensus_ground_truth)\n",
    "    \n",
    "    def _calculate_consensus(self, model_predictions, questions):\n",
    "        \"\"\"Calculate consensus between multiple models\"\"\"\n",
    "        consensus_answers = {}\n",
    "        agreement_scores = []\n",
    "        confidence_variances = []\n",
    "        \n",
    "        for question in questions:\n",
    "            answers = []\n",
    "            confidences = []\n",
    "            \n",
    "            # Collect all model answers for this question\n",
    "            for model_name, predictions in model_predictions.items():\n",
    "                if question in predictions:\n",
    "                    answer = predictions[question]['answer']\n",
    "                    confidence = predictions[question]['confidence']\n",
    "                    \n",
    "                    if answer and len(answer.strip()) > 0:\n",
    "                        answers.append(answer.strip())\n",
    "                        confidences.append(confidence)\n",
    "            \n",
    "            if not answers:\n",
    "                consensus_answers[question] = {'answer': '', 'consensus_confidence': 0.0}\n",
    "                continue\n",
    "            \n",
    "            # Find most common answer (simple consensus)\n",
    "            answer_counts = {}\n",
    "            for answer in answers:\n",
    "                answer_counts[answer] = answer_counts.get(answer, 0) + 1\n",
    "            \n",
    "            if answer_counts:\n",
    "                # Most frequent answer\n",
    "                consensus_answer = max(answer_counts, key=answer_counts.get)\n",
    "                consensus_confidence = answer_counts[consensus_answer] / len(answers)\n",
    "                \n",
    "                consensus_answers[question] = {\n",
    "                    'answer': consensus_answer,\n",
    "                    'consensus_confidence': consensus_confidence,\n",
    "                    'model_count': len(answers)\n",
    "                }\n",
    "                \n",
    "                # Calculate agreement rate\n",
    "                agreement_rate = answer_counts[consensus_answer] / len(answers)\n",
    "                agreement_scores.append(agreement_rate)\n",
    "                \n",
    "                # Calculate confidence variance\n",
    "                if len(confidences) > 1:\n",
    "                    conf_variance = np.var(confidences)\n",
    "                    confidence_variances.append(conf_variance)\n",
    "        \n",
    "        return {\n",
    "            'consensus_answers': consensus_answers,\n",
    "            'consensus_strength': np.mean(agreement_scores) if agreement_scores else 0,\n",
    "            'agreement_rate': np.mean(agreement_scores) if agreement_scores else 0,\n",
    "            'confidence_variance': np.mean(confidence_variances) if confidence_variances else 0\n",
    "        }\n",
    "    \n",
    "    def _evaluate_against_consensus(self, primary_extractions, consensus_answers):\n",
    "        \"\"\"Evaluate primary model against consensus\"\"\"\n",
    "        if not consensus_answers:\n",
    "            return 0.0\n",
    "        \n",
    "        correct_matches = 0\n",
    "        total_comparisons = 0\n",
    "        \n",
    "        for question, consensus_data in consensus_answers.items():\n",
    "            if not consensus_data['answer']:\n",
    "                continue\n",
    "                \n",
    "            # Find matching question in primary extractions\n",
    "            primary_answer = None\n",
    "            for primary_q, primary_data in primary_extractions.items():\n",
    "                if self._questions_similar(question, primary_q):\n",
    "                    primary_answer = primary_data.get('answer', '')\n",
    "                    break\n",
    "            \n",
    "            if primary_answer:\n",
    "                # Calculate similarity\n",
    "                similarity = self._calculate_answer_similarity(\n",
    "                    primary_answer, \n",
    "                    consensus_data['answer']\n",
    "                )\n",
    "                \n",
    "                # Consider it correct if similarity > 0.7\n",
    "                if similarity > 0.7:\n",
    "                    correct_matches += 1\n",
    "                \n",
    "                total_comparisons += 1\n",
    "        \n",
    "        return correct_matches / total_comparisons if total_comparisons > 0 else 0.0\n",
    "    \n",
    "    def _questions_similar(self, q1, q2):\n",
    "        \"\"\"Check if two questions are asking for similar information\"\"\"\n",
    "        q1_lower = q1.lower()\n",
    "        q2_lower = q2.lower()\n",
    "        \n",
    "        # Key term matching\n",
    "        key_terms = {\n",
    "            'invoice': ['invoice', 'rechnungsnummer', 'nummer'],\n",
    "            'amount': ['amount', 'total', 'gesamtbetrag', 'betrag'],\n",
    "            'company': ['company', 'firma', 'vendor', 'firmenname'],\n",
    "            'date': ['date', 'datum', 'rechnungsdatum']\n",
    "        }\n",
    "        \n",
    "        for category, terms in key_terms.items():\n",
    "            q1_has_term = any(term in q1_lower for term in terms)\n",
    "            q2_has_term = any(term in q2_lower for term in terms)\n",
    "            \n",
    "            if q1_has_term and q2_has_term:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _calculate_answer_similarity(self, answer1, answer2):\n",
    "        \"\"\"Calculate similarity between two answers\"\"\"\n",
    "        if not answer1 or not answer2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Clean answers\n",
    "        a1 = str(answer1).lower().strip()\n",
    "        a2 = str(answer2).lower().strip()\n",
    "        \n",
    "        # Exact match\n",
    "        if a1 == a2:\n",
    "            return 1.0\n",
    "        \n",
    "        # Sequence similarity\n",
    "        similarity = SequenceMatcher(None, a1, a2).ratio()\n",
    "        \n",
    "        # Number matching for amounts\n",
    "        nums1 = re.findall(r'\\d+[,\\.]?\\d*', a1)\n",
    "        nums2 = re.findall(r'\\d+[,\\.]?\\d*', a2)\n",
    "        \n",
    "        if nums1 and nums2:\n",
    "            try:\n",
    "                n1 = float(nums1[0].replace(',', '.'))\n",
    "                n2 = float(nums2[0].replace(',', '.'))\n",
    "                if n1 == n2:\n",
    "                    similarity = max(similarity, 0.9)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def statistical_report(self, evaluation_results, consensus_ground_truth):\n",
    "        \"\"\"Generate comprehensive statistical report\"\"\"\n",
    "        if not evaluation_results:\n",
    "            return {\"error\": \"No evaluation results\"}\n",
    "        \n",
    "        # Calculate statistics\n",
    "        accuracies = [r['primary_model_accuracy'] for r in evaluation_results]\n",
    "        consensus_strengths = [r['consensus_strength'] for r in evaluation_results]\n",
    "        agreement_rates = [r['model_agreement_rate'] for r in evaluation_results]\n",
    "        \n",
    "        # Reliability indicators\n",
    "        reliable_files = [r for r in evaluation_results if r['reliable_extraction']]\n",
    "        \n",
    "        report = {\n",
    "            \"evaluation_summary\": {\n",
    "                \"total_files_processed\": len(evaluation_results),\n",
    "                \"reliable_extractions\": len(reliable_files),\n",
    "                \"reliability_rate\": len(reliable_files) / len(evaluation_results)\n",
    "            },\n",
    "            \"accuracy_metrics\": {\n",
    "                \"mean_accuracy\": np.mean(accuracies),\n",
    "                \"median_accuracy\": np.median(accuracies),\n",
    "                \"std_accuracy\": np.std(accuracies),\n",
    "                \"accuracy_distribution\": {\n",
    "                    \"high_accuracy_files\": len([a for a in accuracies if a > 0.8]),\n",
    "                    \"medium_accuracy_files\": len([a for a in accuracies if 0.5 <= a <= 0.8]),\n",
    "                    \"low_accuracy_files\": len([a for a in accuracies if a < 0.5])\n",
    "                }\n",
    "            },\n",
    "            \"consensus_validation\": {\n",
    "                \"mean_consensus_strength\": np.mean(consensus_strengths),\n",
    "                \"mean_model_agreement\": np.mean(agreement_rates),\n",
    "                \"consensus_reliability\": \"HIGH\" if np.mean(consensus_strengths) > 0.7 else \"MEDIUM\" if np.mean(consensus_strengths) > 0.5 else \"LOW\"\n",
    "            },\n",
    "            \"statistical_confidence\": {\n",
    "                \"sample_size\": len(evaluation_results),\n",
    "                \"confidence_interval_95\": self.confidence_interval(accuracies),\n",
    "                \"statistical_significance\": \"SIGNIFICANT\" if len(evaluation_results) > 30 else \"LIMITED_SAMPLE\"\n",
    "            },\n",
    "            \"detailed_results\": evaluation_results[:10],  # First 10 for inspection\n",
    "            \"methodology\": \"multi_model_consensus_validation\",\n",
    "            \"generated_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\" AUTOMATED ACCURACY EVALUATION COMPLETE!\")\n",
    "        print(f\"=\"*60)\n",
    "        print(f\" Files Processed: {report['evaluation_summary']['total_files_processed']}\")\n",
    "        print(f\" Mean Accuracy: {report['accuracy_metrics']['mean_accuracy']:.1%}\")\n",
    "        print(f\" Consensus Strength: {report['consensus_validation']['mean_consensus_strength']:.3f}\")\n",
    "        print(f\" Reliability Rate: {report['evaluation_summary']['reliability_rate']:.1%}\")\n",
    "        print(f\" Statistical Confidence: {report['statistical_confidence']['statistical_significance']}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def confidence_interval(self, values, confidence=0.95):\n",
    "        \"\"\"Calculate confidence interval for accuracy\"\"\"\n",
    "        if len(values) < 2:\n",
    "            return [0, 0]\n",
    "        \n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        n = len(values)\n",
    "        \n",
    "        # Simple confidence interval\n",
    "        margin = 1.96 * (std / np.sqrt(n))  # 95% confidence\n",
    "        \n",
    "        return [max(0, mean - margin), min(1, mean + margin)]\n",
    "\n",
    "# Initialize and run the robust evaluation\n",
    "print(\" INITIALIZING ROBUST AUTOMATED ACCURACY EVALUATOR...\")\n",
    "robust_evaluator = AccuracyEvaluator()\n",
    "\n",
    "# Run on all files\n",
    "accuracy_report = robust_evaluator.multi_model_consensus_evaluation(\n",
    "    dataset_folder=\"./large_scale_invoice_dataset\",\n",
    "    max_files=100  # Process ALL your files\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10994fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Selected 4 files for feedback:\n",
      "   • batch2-0124.jpg\n",
      "   • batch2-0125.jpg\n",
      "   • batch2-0128.jpg\n",
      "   • batch2-0130.jpg\n"
     ]
    }
   ],
   "source": [
    "#4E AUTO-SELECT files with lowest performance from recent evaluation\n",
    "def get_low_performance_files(min_files=3, max_files=5):\n",
    "    \"\"\"Automatically select files that need feedback based on performance\"\"\"\n",
    "    \n",
    "    # Check if we have evaluation results\n",
    "    if os.path.exists(\"model_metrics.json\"):\n",
    "        with open(\"model_metrics.json\", 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        # Get recent evaluations and sort by success rate\n",
    "        evaluations = metrics.get(\"evaluations\", [])\n",
    "        if evaluations:\n",
    "            # Sort by success rate lowest first\n",
    "            sorted_evals = sorted(evaluations, key=lambda x: x.get('success_rate', 0))\n",
    "            \n",
    "            # Get lowest performing files\n",
    "            low_perf_files = []\n",
    "            for eval_data in sorted_evals[:max_files]:\n",
    "                filename = eval_data.get('document_name', '')\n",
    "                if filename and os.path.exists(f\"./large_scale_invoice_dataset/{filename}\"):\n",
    "                    low_perf_files.append(filename)\n",
    "            \n",
    "            if len(low_perf_files) >= min_files:\n",
    "                return low_perf_files[:max_files]\n",
    "    \n",
    "    # Fallback: get any available files\n",
    "    available_files = []\n",
    "    if os.path.exists(\"./large_scale_invoice_dataset\"):\n",
    "        for file_ext in [\"*.pdf\", \"*.png\", \"*.jpg\", \"*.jpeg\"]:\n",
    "            available_files.extend([f.name for f in Path(\"./large_scale_invoice_dataset\").glob(file_ext)])\n",
    "    \n",
    "    return available_files[:max_files] if available_files else []\n",
    "\n",
    "# Get files dynamically\n",
    "low_performance_files = get_low_performance_files(min_files=2, max_files=4)\n",
    "\n",
    "if not low_performance_files:\n",
    "    print(\"❌ No files found for feedback collection!\")\n",
    "    print(\" Make sure you have files in ./large_scale_invoice_dataset/ or run evaluation first\")\n",
    "else:\n",
    "    print(f\" Selected {len(low_performance_files)} files for feedback:\")\n",
    "    for filename in low_performance_files:\n",
    "        print(f\"   • {filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda7c7b",
   "metadata": {},
   "source": [
    "**5. Scalability and Adaptability**\n",
    "* Ensure the system scales across multiple domains (finance, healthcare, legal, etc.) and data types.\n",
    "* Support plug-and-play modularity to integrate new extraction modules or data sources easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8034dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Domain Template Manager initialized with 6 domains and 15+ document types\n"
     ]
    }
   ],
   "source": [
    "# Phase 5A: Domain-Specific Question Templates and Extraction Modules\n",
    "\n",
    "class DomainTemplateManager:\n",
    "    \"\"\"Manages extraction templates for different industries and document types\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.domain_templates = {\n",
    "            \"finance\": {\n",
    "                \"invoice\": [\n",
    "                    \"What is the invoice number?\",\n",
    "                    \"What is the total amount?\",\n",
    "                    \"What is the due date?\",\n",
    "                    \"Who is the vendor?\",\n",
    "                    \"What is the tax amount?\",\n",
    "                    \"What is the net amount?\",\n",
    "                    \"What payment terms are specified?\",\n",
    "                    \"What is the purchase order number?\"\n",
    "                ],\n",
    "                \"financial_statement\": [\n",
    "                    \"What is the total revenue?\",\n",
    "                    \"What is the net income?\",\n",
    "                    \"What is the reporting period?\",\n",
    "                    \"What are the total assets?\",\n",
    "                    \"What are the total liabilities?\",\n",
    "                    \"What is the cash flow from operations?\",\n",
    "                    \"What is the debt-to-equity ratio?\",\n",
    "                    \"What is the earnings per share?\"\n",
    "                ],\n",
    "                \"contract\": [\n",
    "                    \"What is the contract value?\",\n",
    "                    \"What is the contract duration?\",\n",
    "                    \"Who are the contracting parties?\",\n",
    "                    \"What is the effective date?\",\n",
    "                    \"What is the termination date?\",\n",
    "                    \"What are the payment terms?\",\n",
    "                    \"What penalties are specified?\",\n",
    "                    \"What deliverables are mentioned?\"\n",
    "                ]\n",
    "            },\n",
    "            \"healthcare\": {\n",
    "                \"medical_record\": [\n",
    "                    \"What is the patient name?\",\n",
    "                    \"What is the patient ID?\",\n",
    "                    \"What is the diagnosis?\",\n",
    "                    \"What medications are prescribed?\",\n",
    "                    \"What is the treatment plan?\",\n",
    "                    \"What are the vital signs?\",\n",
    "                    \"What allergies are documented?\",\n",
    "                    \"What is the next appointment date?\"\n",
    "                ],\n",
    "                \"lab_report\": [\n",
    "                    \"What tests were performed?\",\n",
    "                    \"What are the test results?\",\n",
    "                    \"What is the reference range?\",\n",
    "                    \"What is the specimen type?\",\n",
    "                    \"When was the sample collected?\",\n",
    "                    \"Who is the ordering physician?\",\n",
    "                    \"Are any results abnormal?\",\n",
    "                    \"What follow-up is recommended?\"\n",
    "                ],\n",
    "                \"prescription\": [\n",
    "                    \"What medication is prescribed?\",\n",
    "                    \"What is the dosage?\",\n",
    "                    \"What is the frequency?\",\n",
    "                    \"How long is the treatment duration?\",\n",
    "                    \"Who is the prescribing doctor?\",\n",
    "                    \"What is the patient name?\",\n",
    "                    \"Are there any warnings?\",\n",
    "                    \"How many refills are allowed?\"\n",
    "                ]\n",
    "            },\n",
    "            \"legal\": {\n",
    "                \"contract\":[\n",
    "                    \"What is the contract value?\",\n",
    "                    \"What is the contract duration?\", \n",
    "                    \"Who are the contracting parties?\",\n",
    "                    \"What is the effective date?\",\n",
    "                    \"What is the termination date?\",\n",
    "                    \"What are the payment terms?\",\n",
    "                    \"What penalties are specified?\",\n",
    "                    \"What deliverables are mentioned?\"\n",
    "                ],\n",
    "                \n",
    "                \"court_document\": [\n",
    "                    \"What is the case number?\",\n",
    "                    \"Who are the plaintiff and defendant?\",\n",
    "                    \"What court is handling the case?\",\n",
    "                    \"What is the filing date?\",\n",
    "                    \"What relief is sought?\",\n",
    "                    \"What are the key facts?\",\n",
    "                    \"What laws are cited?\",\n",
    "                    \"What is the next hearing date?\"\n",
    "                ],\n",
    "                \"legal_notice\": [\n",
    "                    \"Who is the sender?\",\n",
    "                    \"Who is the recipient?\",\n",
    "                    \"What is the subject matter?\",\n",
    "                    \"What action is demanded?\",\n",
    "                    \"What is the deadline for response?\",\n",
    "                    \"What legal basis is cited?\",\n",
    "                    \"What consequences are threatened?\",\n",
    "                    \"Is legal representation mentioned?\"\n",
    "                ]\n",
    "            },\n",
    "            \"hr\": {\n",
    "                \"resume\": [\n",
    "                    \"What is the full name of the candidate?\",\n",
    "                    \"What email address is provided for contact?\",\n",
    "                    \"What phone number is listed?\",\n",
    "                    \"What is the most recent job title?\",\n",
    "                    \"What company does the candidate currently work for?\",\n",
    "                    \"How many years of total experience are mentioned?\",\n",
    "                    \"What degree or education is mentioned?\",\n",
    "                    \"What programming languages are listed?\",\n",
    "                    \"What technical skills are mentioned?\",\n",
    "                    \"What university or school is mentioned?\",\n",
    "                    \"What certifications are listed?\",\n",
    "                    \"What projects are described?\",\n",
    "                    \"What achievements are highlighted?\",\n",
    "                    \"What software tools are mentioned?\",\n",
    "                    \"What languages does the candidate speak?\"\n",
    "                    ],\n",
    "                \"employee_record\": [\n",
    "                    \"What is the employee ID?\",\n",
    "                    \"What is the employee name?\",\n",
    "                    \"What is their department?\",\n",
    "                    \"What is their position?\",\n",
    "                    \"What is their salary?\",\n",
    "                    \"When was their hire date?\",\n",
    "                    \"Who is their manager?\",\n",
    "                    \"What benefits are they enrolled in?\"\n",
    "                ],\n",
    "                \"performance_review\": [\n",
    "                    \"What is the review period?\",\n",
    "                    \"What is the overall rating?\",\n",
    "                    \"What are the key achievements?\",\n",
    "                    \"What areas need improvement?\",\n",
    "                    \"What goals are set for next period?\",\n",
    "                    \"Is a promotion recommended?\",\n",
    "                    \"What training is suggested?\",\n",
    "                    \"What is the salary recommendation?\"\n",
    "                ]\n",
    "            },\n",
    "            \"education\": {\n",
    "                \"transcript\": [\n",
    "                    \"What is the student name?\",\n",
    "                    \"What is the student ID?\",\n",
    "                    \"What degree program?\",\n",
    "                    \"What is the GPA?\",\n",
    "                    \"What courses were completed?\",\n",
    "                    \"What grades were received?\",\n",
    "                    \"What is the graduation date?\",\n",
    "                    \"Are there any honors or distinctions?\"\n",
    "                ],\n",
    "                \"research_paper\": [\n",
    "                    \"What is the title?\",\n",
    "                    \"Who are the authors?\",\n",
    "                    \"What is the abstract?\",\n",
    "                    \"What methodology is used?\",\n",
    "                    \"What are the key findings?\",\n",
    "                    \"What conclusions are drawn?\",\n",
    "                    \"What future work is suggested?\",\n",
    "                    \"What references are cited?\"\n",
    "                ]\n",
    "            },\n",
    "            \"retail\": {\n",
    "                \"receipt\": [\n",
    "                    \"What store issued this receipt?\",\n",
    "                    \"What is the transaction date?\",\n",
    "                    \"What items were purchased?\",\n",
    "                    \"What are the item prices?\",\n",
    "                    \"What is the subtotal?\",\n",
    "                    \"What taxes were applied?\",\n",
    "                    \"What is the total amount?\",\n",
    "                    \"What payment method was used?\"\n",
    "                ],\n",
    "                \"inventory_report\": [\n",
    "                    \"What products are listed?\",\n",
    "                    \"What are the current stock levels?\",\n",
    "                    \"What is the reorder point?\",\n",
    "                    \"What is the unit cost?\",\n",
    "                    \"What is the total inventory value?\",\n",
    "                    \"Which items are low in stock?\",\n",
    "                    \"What is the turnover rate?\",\n",
    "                    \"When was the last inventory count?\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_domain_templates(self, domain: str, document_type: str = None) -> List[str]:\n",
    "        \"\"\"Get extraction templates for specific domain and document type\"\"\"\n",
    "        if domain not in self.domain_templates:\n",
    "            return self.domain_templates.get(\"finance\", {}).get(\"invoice\", [])  # Fallback\n",
    "        \n",
    "        domain_data = self.domain_templates[domain]\n",
    "        \n",
    "        if document_type and document_type in domain_data:\n",
    "            return domain_data[document_type]\n",
    "        \n",
    "        # Return all questions for the domain if no specific document type\n",
    "        all_questions = []\n",
    "        for doc_type, questions in domain_data.items():\n",
    "            all_questions.extend(questions)\n",
    "        \n",
    "        return all_questions\n",
    "    \n",
    "    def add_custom_domain(self, domain_name: str, templates: Dict[str, List[str]]):\n",
    "        \"\"\"Add new domain with custom templates\"\"\"\n",
    "        self.domain_templates[domain_name] = templates\n",
    "        print(f\"✅ Added custom domain: {domain_name}\")\n",
    "    \n",
    "    def detect_domain_and_type(self, text: str) -> tuple:\n",
    "        \"\"\"Auto-detect domain and document type from text content\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Healthcare keywords\n",
    "        if any(word in text_lower for word in ['patient', 'diagnosis', 'prescription', 'medical', 'doctor', 'hospital']):\n",
    "            if any(word in text_lower for word in ['prescription', 'medication', 'dosage']):\n",
    "                return 'healthcare', 'prescription'\n",
    "            elif any(word in text_lower for word in ['lab', 'test', 'result', 'specimen']):\n",
    "                return 'healthcare', 'lab_report'\n",
    "            else:\n",
    "                return 'healthcare', 'medical_record'\n",
    "        \n",
    "        # Legal keywords\n",
    "        elif any(word in text_lower for word in ['court', 'plaintiff', 'defendant', 'lawsuit', 'legal']):\n",
    "            if any(word in text_lower for word in ['case number', 'filing', 'court']):\n",
    "                return 'legal', 'court_document'\n",
    "            elif any(word in text_lower for word in ['notice', 'demand', 'cease']):\n",
    "                return 'legal', 'legal_notice'\n",
    "            else:\n",
    "                return 'legal', 'contract'\n",
    "        \n",
    "        # HR keywords\n",
    "        elif any(word in text_lower for word in ['employee', 'resume', 'candidate', 'performance']):\n",
    "            if any(word in text_lower for word in ['resume', 'cv', 'experience', 'education']):\n",
    "                return 'hr', 'resume'\n",
    "            elif any(word in text_lower for word in ['performance', 'review', 'rating']):\n",
    "                return 'hr', 'performance_review'\n",
    "            else:\n",
    "                return 'hr', 'employee_record'\n",
    "        \n",
    "        # Education keywords\n",
    "        elif any(word in text_lower for word in ['student', 'grade', 'transcript', 'university', 'research']):\n",
    "            if any(word in text_lower for word in ['transcript', 'gpa', 'courses']):\n",
    "                return 'education', 'transcript'\n",
    "            else:\n",
    "                return 'education', 'research_paper'\n",
    "        \n",
    "        # Retail keywords\n",
    "        elif any(word in text_lower for word in ['receipt', 'purchase', 'inventory', 'store', 'items']):\n",
    "            if any(word in text_lower for word in ['inventory', 'stock', 'reorder']):\n",
    "                return 'retail', 'inventory_report'\n",
    "            else:\n",
    "                return 'retail', 'receipt'\n",
    "        \n",
    "        # Finance keywords (including existing invoice detection)\n",
    "        elif any(word in text_lower for word in ['invoice', 'bill', 'payment', 'financial', 'revenue']):\n",
    "            if any(word in text_lower for word in ['revenue', 'income', 'assets', 'liabilities']):\n",
    "                return 'finance', 'financial_statement'\n",
    "            elif any(word in text_lower for word in ['contract', 'agreement', 'terms']):\n",
    "                return 'finance', 'contract'\n",
    "            else:\n",
    "                return 'finance', 'invoice'\n",
    "        \n",
    "        # Default fallback\n",
    "        return 'finance', 'invoice'\n",
    "\n",
    "# Initialize domain manager\n",
    "domain_manager = DomainTemplateManager()\n",
    "print(\"✅ Domain Template Manager initialized with 6 domains and 15+ document types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8df5e9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Domain Extractor ready for 6 industries\n"
     ]
    }
   ],
   "source": [
    "# Phase 5B: Multi-Domain Extractor\n",
    "\n",
    "class MultiDomainExtractor(QABasedExtractor):\n",
    "    \"\"\"Extended QA extractor with domain-specific capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"deepset/xlm-roberta-large-squad2\", local_dir=\"./lora_fine_tuned_model\"):\n",
    "        super().__init__(model_name, local_dir)\n",
    "        self.domain_manager = DomainTemplateManager()\n",
    "        \n",
    "        # Add domain-specific preprocessing patterns\n",
    "        self.domain_patterns = {\n",
    "            'finance': {\n",
    "                'currency': r'[\\$€£¥][\\d,\\.]+',\n",
    "                'dates': r'\\d{1,2}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{2,4}',\n",
    "                'invoice_numbers': r'(?:INV|inv|Invoice|INVOICE)[#\\-\\s]*([A-Z0-9\\-]+)'\n",
    "            },\n",
    "            'healthcare': {\n",
    "                'medications': r'(?:mg|ml|tablets?|capsules?)\\s*\\d+',\n",
    "                'vital_signs': r'(?:BP|Blood Pressure)[:\\s]*\\d+\\/\\d+',\n",
    "                'patient_ids': r'(?:Patient ID|ID)[:\\s]*([A-Z0-9\\-]+)'\n",
    "            },\n",
    "            'legal': {\n",
    "                'case_numbers': r'(?:Case|No\\.)[:\\s]*([A-Z0-9\\-\\/]+)',\n",
    "                'dates': r'\\d{1,2}(?:st|nd|rd|th)?\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}',\n",
    "                'parties': r'(?:Plaintiff|Defendant)[:\\s]*([A-Za-z\\s,\\.]+)'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def preprocess_domain_text(self, text: str, domain: str) -> str:\n",
    "        \"\"\"Enhanced preprocessing based on detected domain\"\"\"\n",
    "        text = super().preprocess_text(text)\n",
    "        \n",
    "        if domain in self.domain_patterns:\n",
    "            patterns = self.domain_patterns[domain]\n",
    "            \n",
    "            # Add domain-specific context markers\n",
    "            for pattern_type, pattern in patterns.items():\n",
    "                matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    if isinstance(match, tuple):\n",
    "                        match = match[0] if match else \"\"\n",
    "                    text = text.replace(str(match), f\"IMPORTANT_{pattern_type.upper()}: {match}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_multi_domain(self, text: str, domain: str = None, document_type: str = None) -> Dict:\n",
    "        \"\"\"Extract information using domain-specific templates\"\"\"\n",
    "        \n",
    "        # Auto-detect domain if not provided\n",
    "        if not domain:\n",
    "            domain, document_type = self.domain_manager.detect_domain_and_type(text)\n",
    "            print(f\" Auto-detected: {domain}/{document_type}\")\n",
    "        \n",
    "        # Get domain-specific questions\n",
    "        questions = self.domain_manager.get_domain_templates(domain, document_type)\n",
    "        \n",
    "        # Use domain-specific preprocessing\n",
    "        processed_text = self.preprocess_domain_text(text, domain)\n",
    "        \n",
    "        # Extract with domain-specific questions\n",
    "        qa_results = self.extract_with_questions(processed_text, questions)\n",
    "        \n",
    "        # Get entities\n",
    "        ner_results = self.extract_entities_with_ner(processed_text)\n",
    "        \n",
    "        successful = sum(1 for r in qa_results.values() if r.get('extracted'))\n",
    "        \n",
    "        return {\n",
    "            'domain': domain,\n",
    "            'document_type': document_type,\n",
    "            'extraction_timestamp': datetime.now().isoformat(),\n",
    "            'model_used': self.model_name,\n",
    "            'total_questions': len(questions),\n",
    "            'successful_extractions': successful,\n",
    "            'success_rate': successful / len(questions) if questions else 0,\n",
    "            'extractions': qa_results,\n",
    "            'entities': ner_results,\n",
    "            'confidence_distribution': self._analyze_confidence_distribution(qa_results)\n",
    "        }\n",
    "    \n",
    "    def _analyze_confidence_distribution(self, qa_results: Dict) -> Dict:\n",
    "        \"\"\"Analyze confidence score distribution for quality assessment\"\"\"\n",
    "        confidences = [r.get('confidence', 0) for r in qa_results.values()]\n",
    "        \n",
    "        if not confidences:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'mean_confidence': sum(confidences) / len(confidences),\n",
    "            'high_confidence_count': sum(1 for c in confidences if c >= 0.7),\n",
    "            'medium_confidence_count': sum(1 for c in confidences if 0.3 <= c < 0.7),\n",
    "            'low_confidence_count': sum(1 for c in confidences if c < 0.3),\n",
    "            'confidence_std': np.std(confidences) if len(confidences) > 1 else 0\n",
    "        }\n",
    "\n",
    "# Create multi-domain extractor instance\n",
    "multi_extractor = MultiDomainExtractor()\n",
    "print(\"✅ Multi-Domain Extractor ready for 6 industries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df052528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registered module: table_extractor\n",
      "✅ Registered module: email_extractor\n",
      "✅ Registered module: datetime_extractor\n",
      "✅ Modular Extraction System initialized with 3 default modules\n"
     ]
    }
   ],
   "source": [
    "# Phase 5C: Plug-and-Play Module System\n",
    "\n",
    "class ExtractorModule:\n",
    "    \"\"\"Base class for extraction modules\"\"\"\n",
    "    \n",
    "    def __init__(self, module_name: str, supported_formats: List[str]):\n",
    "        self.module_name = module_name\n",
    "        self.supported_formats = supported_formats\n",
    "        self.is_active = True\n",
    "    \n",
    "    def can_process(self, file_path: str, content: str) -> bool:\n",
    "        \"\"\"Check if this module can process the given file\"\"\"\n",
    "        file_ext = os.path.splitext(file_path)[1].lower()\n",
    "        return file_ext in self.supported_formats\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        \"\"\"Override this method in subclasses\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement extract method\")\n",
    "\n",
    "class TableExtractionModule(ExtractorModule):\n",
    "    \"\"\"Specialized module for table extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"table_extractor\", [\".pdf\", \".xlsx\", \".csv\"])\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        # Simulate table extraction\n",
    "        tables = []\n",
    "        \n",
    "        # Look for table-like patterns\n",
    "        lines = content.split('\\n')\n",
    "        potential_tables = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            # lines with multiple numbers/currencies\n",
    "            if len(re.findall(r'\\d+[,\\.]?\\d*', line)) >= 3:\n",
    "                potential_tables.append({\n",
    "                    'line_number': i + 1,\n",
    "                    'content': line.strip(),\n",
    "                    'confidence': 0.8\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'module': self.module_name,\n",
    "            'tables_found': len(potential_tables),\n",
    "            'tables': potential_tables[:5],  # Limit to first 5\n",
    "            'extraction_type': 'tabular_data'\n",
    "        }\n",
    "\n",
    "class EmailExtractionModule(ExtractorModule):\n",
    "    \"\"\"Specialized module for email extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"email_extractor\", [\".eml\", \".txt\", \".pdf\"])\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        phone_pattern = r'(?:\\+\\d{1,3}[\\s-]?)?\\(?[0-9]{3}\\)?[\\s-]?[0-9]{3}[\\s-]?[0-9]{4}'\n",
    "        \n",
    "        emails = re.findall(email_pattern, content)\n",
    "        phones = re.findall(phone_pattern, content)\n",
    "        \n",
    "        return {\n",
    "            'module': self.module_name,\n",
    "            'emails_found': emails,\n",
    "            'phones_found': phones,\n",
    "            'contact_count': len(emails) + len(phones),\n",
    "            'extraction_type': 'contact_information'\n",
    "        }\n",
    "\n",
    "class DateTimeExtractionModule(ExtractorModule):\n",
    "    \"\"\"Specialized module for date/time extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"datetime_extractor\", [\".pdf\", \".txt\", \".docx\"])\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        # Various date patterns\n",
    "        date_patterns = [\n",
    "            r'\\d{1,2}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{2,4}',  # MM/DD/YYYY\n",
    "            r'\\d{4}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{1,2}',    # YYYY/MM/DD\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{4}\\b',\n",
    "            r'\\d{1,2}(?:st|nd|rd|th)?\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}'\n",
    "        ]\n",
    "        \n",
    "        found_dates = []\n",
    "        for pattern in date_patterns:\n",
    "            matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "            found_dates.extend(matches)\n",
    "        \n",
    "        return {\n",
    "            'module': self.module_name,\n",
    "            'dates_found': found_dates,\n",
    "            'date_count': len(found_dates),\n",
    "            'extraction_type': 'temporal_information'\n",
    "        }\n",
    "\n",
    "class ModularExtractionSystem:\n",
    "    \"\"\"Plug-and-play system for managing extraction modules\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.modules = {}\n",
    "        self.processing_order = []\n",
    "        \n",
    "        # Register default modules\n",
    "        self._register_default_modules()\n",
    "    \n",
    "    def _register_default_modules(self):\n",
    "        \"\"\"Register built-in extraction modules\"\"\"\n",
    "        default_modules = [\n",
    "            TableExtractionModule(),\n",
    "            EmailExtractionModule(),\n",
    "            DateTimeExtractionModule()\n",
    "        ]\n",
    "        \n",
    "        for module in default_modules:\n",
    "            self.register_module(module)\n",
    "    \n",
    "    def register_module(self, module: ExtractorModule):\n",
    "        \"\"\"Register a new extraction module\"\"\"\n",
    "        self.modules[module.module_name] = module\n",
    "        if module.module_name not in self.processing_order:\n",
    "            self.processing_order.append(module.module_name)\n",
    "        print(f\"✅ Registered module: {module.module_name}\")\n",
    "    \n",
    "    def unregister_module(self, module_name: str):\n",
    "        \"\"\"Remove an extraction module\"\"\"\n",
    "        if module_name in self.modules:\n",
    "            del self.modules[module_name]\n",
    "            if module_name in self.processing_order:\n",
    "                self.processing_order.remove(module_name)\n",
    "            print(f\"❌ Unregistered module: {module_name}\")\n",
    "    \n",
    "    def get_compatible_modules(self, file_path: str, content: str) -> List[ExtractorModule]:\n",
    "        \"\"\"Get modules that can process the given file\"\"\"\n",
    "        compatible = []\n",
    "        for module in self.modules.values():\n",
    "            if module.is_active and module.can_process(file_path, content):\n",
    "                compatible.append(module)\n",
    "        return compatible\n",
    "    \n",
    "    def extract_with_modules(self, file_path: str, content: str) -> Dict:\n",
    "        \"\"\"Run all compatible modules on the content\"\"\"\n",
    "        compatible_modules = self.get_compatible_modules(file_path, content)\n",
    "        \n",
    "        results = {\n",
    "            'file_path': file_path,\n",
    "            'modules_used': [m.module_name for m in compatible_modules],\n",
    "            'module_results': {},\n",
    "            'total_modules': len(compatible_modules),\n",
    "            'extraction_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        for module in compatible_modules:\n",
    "            try:\n",
    "                module_result = module.extract(content)\n",
    "                results['module_results'][module.module_name] = module_result\n",
    "                print(f\"✅ {module.module_name}: {module_result.get('extraction_type', 'extracted')}\")\n",
    "            except Exception as e:\n",
    "                results['module_results'][module.module_name] = {\n",
    "                    'error': str(e),\n",
    "                    'extraction_type': 'failed'\n",
    "                }\n",
    "                print(f\" {module.module_name}: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize modular system\n",
    "modular_system = ModularExtractionSystem()\n",
    "print(\"✅ Modular Extraction System initialized with 3 default modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e13cc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5D: Unified Scalable Pipeline\n",
    "\n",
    "def enhanced_multi_domain_extraction(file_path: str, domain: str = None, \n",
    "                                   document_type: str = None, \n",
    "                                   use_modules: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Ultimate extraction pipeline combining:\n",
    "    - Multi-domain QA extraction\n",
    "    - Plug-and-play modules\n",
    "    - Original normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Load and normalize (existing system)\n",
    "    document_data = load_and_normalize(file_path)\n",
    "    \n",
    "    # Step 2: Combine text for processing\n",
    "    all_text = \"\"\n",
    "    for content_item in document_data['content']:\n",
    "        all_text += content_item['text'] + \" \"\n",
    "    \n",
    "    # Step 3: Multi-domain QA extraction\n",
    "    qa_results = multi_extractor.extract_multi_domain(all_text, domain, document_type)\n",
    "    \n",
    "    # Step 4: Modular extraction (if enabled)\n",
    "    module_results = {}\n",
    "    if use_modules:\n",
    "        module_results = modular_system.extract_with_modules(file_path, all_text)\n",
    "    \n",
    "    # Step 5: Combine all results\n",
    "    enhanced_document = {\n",
    "        **document_data,  # Original normalized data\n",
    "        'multi_domain_extraction': qa_results,\n",
    "        'modular_extraction': module_results,\n",
    "        'scalability_features': {\n",
    "            'domain_detected': qa_results.get('domain', 'unknown'),\n",
    "            'document_type_detected': qa_results.get('document_type', 'unknown'),\n",
    "            'modules_used': module_results.get('modules_used', []),\n",
    "            'total_extraction_methods': 1 + len(module_results.get('modules_used', [])),\n",
    "            'confidence_analysis': qa_results.get('confidence_distribution', {}),\n",
    "            'processing_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return enhanced_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4221c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING MULTI-DOMAIN SCALABILITY\n",
      "\n",
      " Testing: Internship_Agreement\n",
      "Processing pdf file: UNPAID_Internship_Agreement_Aslican_Alacal.pdf\n",
      " Auto-detected: legal/court_document\n",
      "✅ table_extractor: tabular_data\n",
      "✅ email_extractor: contact_information\n",
      "✅ datetime_extractor: temporal_information\n",
      "✅ Domain: legal\n",
      "✅ Document Type: court_document\n",
      "✅ Success Rate: 0.0%\n",
      "✅ Modules Used: 3\n",
      "✅ Mean Confidence: 0.000\n",
      "\n",
      " Testing: CV\n",
      "Processing pdf file: Aslican_Alacal_CV.pdf\n",
      " Auto-detected: education/research_paper\n",
      "✅ table_extractor: tabular_data\n",
      "✅ email_extractor: contact_information\n",
      "✅ datetime_extractor: temporal_information\n",
      "✅ Domain: education\n",
      "✅ Document Type: research_paper\n",
      "✅ Success Rate: 0.0%\n",
      "✅ Modules Used: 3\n",
      "✅ Mean Confidence: 0.009\n",
      "\n",
      " Testing: invoice\n",
      "Processing pdf file: SWME_Rechnung_07122020.pdf\n",
      " Auto-detected: finance/invoice\n",
      "✅ table_extractor: tabular_data\n",
      "✅ email_extractor: contact_information\n",
      "✅ datetime_extractor: temporal_information\n",
      "✅ Domain: finance\n",
      "✅ Document Type: invoice\n",
      "✅ Success Rate: 50.0%\n",
      "✅ Modules Used: 3\n",
      "✅ Mean Confidence: 0.195\n",
      "   Q: What is the invoice number?...\n",
      "   A:  100260250 (conf: 0.531)\n",
      "   Q: What is the due date?...\n",
      "   A:  21.12.2020 (conf: 0.758)\n"
     ]
    }
   ],
   "source": [
    "# Phase 5D: Test scalability with different domains\n",
    "\n",
    "print(\"TESTING MULTI-DOMAIN SCALABILITY\")\n",
    "\n",
    "# Test files for different domains \n",
    "test_scenarios = [\n",
    "    {\n",
    "        'name': 'Internship_Agreement',\n",
    "        'file': r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\UNPAID_Internship_Agreement_Aslican_Alacal.pdf\",\n",
    "        'expected_domain': 'hr'\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'name': 'CV',\n",
    "        'file': r\"C:\\Users\\aslia\\Downloads\\Aslican_Alacal_CV.pdf\",\n",
    "        'expected_domain': 'hr'\n",
    "    },\n",
    "      {\n",
    "        'name': 'invoice',\n",
    "        'file': r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\\SWME_Rechnung_07122020.pdf\",\n",
    "        'expected_domain': 'finance'\n",
    "    }\n",
    "    # add more test files \n",
    "]\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    if os.path.exists(scenario['file']):\n",
    "        print(f\"\\n Testing: {scenario['name']}\")\n",
    "        \n",
    "        result = enhanced_multi_domain_extraction(scenario['file'])\n",
    "        \n",
    "        scalability = result['scalability_features']\n",
    "        domain_result = result['multi_domain_extraction']\n",
    "        \n",
    "        print(f\"✅ Domain: {scalability['domain_detected']}\")\n",
    "        print(f\"✅ Document Type: {scalability['document_type_detected']}\")\n",
    "        print(f\"✅ Success Rate: {domain_result['success_rate']:.1%}\")\n",
    "        print(f\"✅ Modules Used: {len(scalability['modules_used'])}\")\n",
    "        print(f\"✅ Mean Confidence: {scalability['confidence_analysis'].get('mean_confidence', 0):.3f}\")\n",
    "        \n",
    "        # Show top extractions\n",
    "        extractions = domain_result['extractions']\n",
    "        high_conf_count = 0\n",
    "        for question, answer in extractions.items():\n",
    "            if answer.get('confidence', 0) > 0.5 and high_conf_count < 3:\n",
    "                print(f\"   Q: {question[:60]}...\")\n",
    "                print(f\"   A: {answer['answer']} (conf: {answer['confidence']:.3f})\")\n",
    "                high_conf_count += 1\n",
    "    else:\n",
    "        print(f\"❌ Test file not found: {scenario['file']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52db25",
   "metadata": {},
   "source": [
    "**6. Explainability and Trust**\n",
    "* Integrate explainable AI (XAI) tools (e.g., SHAP, attention visualization) to make extraction results transparent and interpretable for end users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26283b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6A- Explainable AI Dependencies and Setup\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "from typing import Optional, Union, Tuple\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForQuestionAnswering, \n",
    "    pipeline,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DefaultDataCollator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf3e3945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Attention Visualizer ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 6B - Attention Visualization for Transformer Models\n",
    "\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    \"\"\"Visualize attention patterns in transformer models for explainability\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def get_attention_weights(self, question: str, context: str):\n",
    "        \"\"\"Extract attention weights from the model\"\"\"\n",
    "        try:\n",
    "            # Tokenize inputs\n",
    "            inputs = self.tokenizer(\n",
    "                question, context,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Get model outputs with attention\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, output_attentions=True)\n",
    "            \n",
    "            # Extract attention weights (last layer, first head for simplicity)\n",
    "            attention = outputs.attentions[-1][0, 0].cpu().numpy()\n",
    "            \n",
    "            # Get tokens\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "            \n",
    "            return {\n",
    "                'attention_weights': attention,\n",
    "                'tokens': tokens,\n",
    "                'question_length': len(self.tokenizer.tokenize(question)),\n",
    "                'context_start': len(self.tokenizer.tokenize(question)) + 2  # +2 for special tokens\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Attention extraction failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def visualize_attention_heatmap(self, question: str, context: str, save_path: str = None):\n",
    "        \"\"\"Create attention heatmap visualization\"\"\"\n",
    "        attention_data = self.get_attention_weights(question, context)\n",
    "        \n",
    "        if not attention_data:\n",
    "            return None\n",
    "        \n",
    "        attention = attention_data['attention_weights']\n",
    "        tokens = attention_data['tokens']\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(\n",
    "            attention[:len(tokens), :len(tokens)],\n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cmap='Blues',\n",
    "            cbar=True\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Attention Heatmap\\nQ: {question[:50]}...')\n",
    "        plt.xlabel('Target Tokens')\n",
    "        plt.ylabel('Source Tokens')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\" Attention heatmap saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        return attention_data\n",
    "    \n",
    "    def get_top_attended_tokens(self, question: str, context: str, top_k: int = 5):\n",
    "        \"\"\"Get tokens that received highest attention for answer\"\"\"\n",
    "        attention_data = self.get_attention_weights(question, context)\n",
    "        \n",
    "        if not attention_data:\n",
    "            return []\n",
    "        \n",
    "        attention = attention_data['attention_weights']\n",
    "        tokens = attention_data['tokens']\n",
    "        context_start = attention_data['context_start']\n",
    "        \n",
    "        # Focus on context tokens\n",
    "        context_attention = attention[context_start:, context_start:]\n",
    "        context_tokens = tokens[context_start:]\n",
    "        \n",
    "        # Get average attention received by each context token\n",
    "        avg_attention = context_attention.mean(axis=0)\n",
    "        \n",
    "        # Get top attended tokens\n",
    "        top_indices = avg_attention.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        top_tokens = []\n",
    "        for idx in top_indices:\n",
    "            if idx < len(context_tokens):\n",
    "                top_tokens.append({\n",
    "                    'token': context_tokens[idx],\n",
    "                    'attention_score': avg_attention[idx],\n",
    "                    'position': idx\n",
    "                })\n",
    "        \n",
    "        return top_tokens\n",
    "\n",
    "# Initialize attention visualizer\n",
    "print(\"✅ Attention Visualizer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3b794b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LIME Explainer ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 6C - LIME Text Explainer Integration\n",
    "\n",
    "\n",
    "class LIMEExplainer:\n",
    "    \"\"\"LIME-based explainability for QA predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, qa_pipeline):\n",
    "        self.qa_pipeline = qa_pipeline\n",
    "        self.explainer = LimeTextExplainer(class_names=['answer_confidence'])\n",
    "        \n",
    "    def predict_function(self, texts, question):\n",
    "        \"\"\"Prediction function for LIME\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                result = self.qa_pipeline(question=question, context=text)\n",
    "                # Return confidence score as prediction\n",
    "                predictions.append([1 - result['score'], result['score']])\n",
    "            except:\n",
    "                predictions.append([0.5, 0.5])  # Neutral if failed\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def explain_prediction(self, question: str, context: str, num_features: int = 10):\n",
    "        \"\"\"Generate LIME explanation for QA prediction\"\"\"\n",
    "        \n",
    "        # Create prediction function for this specific question\n",
    "        def pred_fn(texts):\n",
    "            return self.predict_function(texts, question)\n",
    "        \n",
    "        try:\n",
    "            # Generate explanation\n",
    "            explanation = self.explainer.explain_instance(\n",
    "                context,\n",
    "                pred_fn,\n",
    "                num_features=num_features,\n",
    "                num_samples=100\n",
    "            )\n",
    "            \n",
    "            # Extract feature importance\n",
    "            feature_importance = explanation.as_list()\n",
    "            \n",
    "            # Get original prediction\n",
    "            original_result = self.qa_pipeline(question=question, context=context)\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'predicted_answer': original_result['answer'],\n",
    "                'confidence': original_result['score'],\n",
    "                'feature_importance': feature_importance,\n",
    "                'explanation_type': 'LIME',\n",
    "                'important_words': [item[0] for item in feature_importance[:5]]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ LIME explanation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def visualize_explanation(self, explanation_data, save_path: str = None):\n",
    "        \"\"\"Visualize LIME explanation\"\"\"\n",
    "        if not explanation_data:\n",
    "            return\n",
    "        \n",
    "        features = explanation_data['feature_importance']\n",
    "        \n",
    "        # Separate positive and negative contributions\n",
    "        words = [item[0] for item in features]\n",
    "        scores = [item[1] for item in features]\n",
    "        \n",
    "        # Create colors based on positive/negative contribution\n",
    "        colors = ['green' if score > 0 else 'red' for score in scores]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.barh(range(len(words)), scores, color=colors, alpha=0.7)\n",
    "        \n",
    "        plt.yticks(range(len(words)), words)\n",
    "        plt.xlabel('Feature Importance (Positive = Supports Answer)')\n",
    "        plt.title(f'LIME Explanation\\nQ: {explanation_data[\"question\"][:50]}...\\nA: {explanation_data[\"predicted_answer\"]} (conf: {explanation_data[\"confidence\"]:.3f})')\n",
    "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, scores):\n",
    "            plt.text(bar.get_width() + (0.01 if score > 0 else -0.01), \n",
    "                    bar.get_y() + bar.get_height()/2,\n",
    "                    f'{score:.3f}', \n",
    "                    ha='left' if score > 0 else 'right', \n",
    "                    va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\" LIME explanation saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "print(\"✅ LIME Explainer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fb90d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded fine-tuned model from ./lora_fine_tuned_model\n",
      "✅ Explainable QA System ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 6D - Comprehensive Explainability Pipeline\n",
    "\n",
    "class ExplainableQASystem:\n",
    "    \"\"\"Complete explainable QA system combining multiple XAI techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"./lora_fine_tuned_model\"):\n",
    "        # Load model and tokenizer\n",
    "        if os.path.exists(model_path):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer\n",
    "            )\n",
    "            print(f\"✅ Loaded fine-tuned model from {model_path}\")\n",
    "        else:\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=\"deepset/xlm-roberta-large-squad2\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"deepset/xlm-roberta-large-squad2\")\n",
    "            self.model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/xlm-roberta-large-squad2\")\n",
    "            print(\"✅ Using base model for explainability\")\n",
    "        \n",
    "        # Initialize explainers\n",
    "        self.attention_viz = AttentionVisualizer(self.model, self.tokenizer)\n",
    "        self.lime_explainer = LIMEExplainer(self.qa_pipeline)\n",
    "        \n",
    "    def explain_prediction(self, question: str, context: str, \n",
    "                         use_attention: bool = True, \n",
    "                         use_lime: bool = True,\n",
    "                         save_visualizations: bool = True) -> Dict:\n",
    "        \"\"\"Generate comprehensive explanation for QA prediction\"\"\"\n",
    "        # TRUNCATE CONTEXT TO PREVENT MEMORY ISSUES\n",
    "        if len(context) > 2000:\n",
    "            context = context[:2000] + \"...\"\n",
    "            print(\" Context truncated for memory optimization\")\n",
    "        \n",
    "        print(f\"Generating explanations for:\")\n",
    "        print(f\"Q: {question[:60]}...\")\n",
    "        print(f\"Context: {context[:100]}...\")\n",
    "        \n",
    "        # Get base prediction\n",
    "        prediction = self.qa_pipeline(question=question, context=context)\n",
    "        \n",
    "        explanations = {\n",
    "            'question': question,\n",
    "            'context': context[:200] + \"...\" if len(context) > 200 else context,\n",
    "            'prediction': {\n",
    "                'answer': prediction['answer'],\n",
    "                'confidence': prediction['score'],\n",
    "                'start_pos': prediction['start'],\n",
    "                'end_pos': prediction['end']\n",
    "            },\n",
    "            'explanations': {}\n",
    "        }\n",
    "        \n",
    "        # Generate attention explanation\n",
    "        if use_attention:\n",
    "            print(\"Generating attention visualization...\")\n",
    "            try:\n",
    "                top_tokens = self.attention_viz.get_top_attended_tokens(question, context)\n",
    "                explanations['explanations']['attention'] = {\n",
    "                    'top_attended_tokens': top_tokens,\n",
    "                    'explanation_type': 'attention_weights'\n",
    "                }\n",
    "                \n",
    "                if save_visualizations:\n",
    "                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                    attention_path = f\"attention_explanation_{timestamp}.png\"\n",
    "                    self.attention_viz.visualize_attention_heatmap(\n",
    "                        question, context, attention_path\n",
    "                    )\n",
    "                    explanations['explanations']['attention']['visualization_path'] = attention_path\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Attention explanation failed: {e}\")\n",
    "        \n",
    "        # Generate LIME explanation\n",
    "        if use_lime:\n",
    "            print(\" Generating LIME explanation...\")\n",
    "            try:\n",
    "                lime_result = self.lime_explainer.explain_prediction(question, context)\n",
    "                if lime_result:\n",
    "                    explanations['explanations']['lime'] = lime_result\n",
    "                    \n",
    "                    if save_visualizations:\n",
    "                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                        lime_path = f\"lime_explanation_{timestamp}.png\"\n",
    "                        self.lime_explainer.visualize_explanation(lime_result, lime_path)\n",
    "                        explanations['explanations']['lime']['visualization_path'] = lime_path\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"❌LIME explanation failed: {e}\")\n",
    "        \n",
    "        # Generate confidence interpretation\n",
    "        explanations['trust_indicators'] = self._generate_trust_indicators(prediction, explanations)\n",
    "        \n",
    "        return explanations\n",
    "    \n",
    "    def _generate_trust_indicators(self, prediction: Dict, explanations: Dict) -> Dict:\n",
    "        \"\"\"Generate trust and reliability indicators\"\"\"\n",
    "        confidence = prediction['score']\n",
    "        \n",
    "        # Confidence level categorization\n",
    "        if confidence >= 0.8:\n",
    "            confidence_level = \"High\"\n",
    "            trust_message = \"High confidence - answer is very reliable\"\n",
    "        elif confidence >= 0.5:\n",
    "            confidence_level = \"Medium\"\n",
    "            trust_message = \"Medium confidence - answer is moderately reliable\"\n",
    "        elif confidence >= 0.3:\n",
    "            confidence_level = \"Low\"\n",
    "            trust_message = \"Low confidence - answer may be uncertain\"\n",
    "        else:\n",
    "            confidence_level = \"Very Low\"\n",
    "            trust_message = \"Very low confidence - answer is unreliable\"\n",
    "        \n",
    "        # Check explanation consistency\n",
    "        explanation_consistency = \"Unknown\"\n",
    "        if 'attention' in explanations.get('explanations', {}):\n",
    "            top_tokens = explanations['explanations']['attention'].get('top_attended_tokens', [])\n",
    "            if top_tokens and len(top_tokens) > 0:\n",
    "                # Check if answer appears in top attended tokens\n",
    "                answer_tokens = prediction['answer'].lower().split()\n",
    "                attended_tokens = [t['token'].lower().replace('##', '') for t in top_tokens]\n",
    "                \n",
    "                overlap = any(token in ' '.join(attended_tokens) for token in answer_tokens)\n",
    "                explanation_consistency = \"High\" if overlap else \"Low\"\n",
    "        \n",
    "        return {\n",
    "            'confidence_score': confidence,\n",
    "            'confidence_level': confidence_level,\n",
    "            'trust_message': trust_message,\n",
    "            'explanation_consistency': explanation_consistency,\n",
    "            'reliability_score': min(confidence * 1.2, 1.0) if explanation_consistency == \"High\" else confidence * 0.8\n",
    "        }\n",
    "    \n",
    "    def batch_explain_extractions(self, extractions_dict: Dict, max_explanations: int = 3) -> Dict:\n",
    "        \"\"\"Generate explanations for multiple extractions\"\"\"\n",
    "        print(f\"Generating explanations for top {max_explanations} extractions...\")\n",
    "        \n",
    "        explained_extractions = {}\n",
    "        count = 0\n",
    "        \n",
    "        for question, result in extractions_dict.items():\n",
    "            if count >= max_explanations:\n",
    "                break\n",
    "                \n",
    "            if result.get('answer') and result.get('confidence', 0) > 0.3:\n",
    "                print(f\"\\n Explaining: {question[:50]}...\")\n",
    "                \n",
    "                # Reconstruct context \n",
    "                context = result.get('context', 'Context not available')\n",
    "                \n",
    "                explanation = self.explain_prediction(\n",
    "                    question=question,\n",
    "                    context=context,\n",
    "                    use_attention=True,\n",
    "                    use_lime=True,\n",
    "                    save_visualizations=True\n",
    "                )\n",
    "                \n",
    "                explained_extractions[question] = explanation\n",
    "                count += 1\n",
    "        \n",
    "        return explained_extractions\n",
    "\n",
    "# Initialize explainable QA system\n",
    "explainable_qa = ExplainableQASystem()\n",
    "print(\"✅ Explainable QA System ready!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0e3d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Explainable Multi-Domain Extraction ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Phase 6E: Integration with Multi-Domain System\n",
    "\n",
    "def enhanced_multi_domain_extraction_with_explanations(file_path: str, \n",
    "                                                      domain: str = None, \n",
    "                                                      document_type: str = None,\n",
    "                                                      explain_top_results: bool = True,\n",
    "                                                      max_explanations: int = 2) -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced extraction with built-in explainability\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1: Run your existing multi-domain extraction\n",
    "    result = enhanced_multi_domain_extraction(file_path, domain, document_type)\n",
    "    \n",
    "    # 2: Add explainability for high-confidence results\n",
    "    if explain_top_results:\n",
    "        print(f\"\\n Adding explainability to extraction results...\")\n",
    "        \n",
    "        # Get text content for explanations\n",
    "        all_text = \"\"\n",
    "        for content_item in result['content']:\n",
    "            all_text += content_item['text'] + \" \"\n",
    "        \n",
    "        # Get high-confidence extractions\n",
    "        extractions = result['multi_domain_extraction']['extractions']\n",
    "        high_conf_extractions = {}\n",
    "        \n",
    "        count = 0\n",
    "        for question, answer_data in extractions.items():\n",
    "            if (answer_data.get('confidence', 0) > 0.4 and \n",
    "                answer_data.get('answer') and \n",
    "                count < max_explanations):\n",
    "                \n",
    "                high_conf_extractions[question] = {\n",
    "                    **answer_data,\n",
    "                    'context': all_text  # Add context for explanation\n",
    "                }\n",
    "                count += 1\n",
    "        \n",
    "        # Generate explanations\n",
    "        if high_conf_extractions:\n",
    "            explanations = {}\n",
    "            for question, data in high_conf_extractions.items():\n",
    "                print(f\" Explaining: {question[:40]}...\")\n",
    "                \n",
    "                explanation = explainable_qa.explain_prediction(\n",
    "                    question=question,\n",
    "                    context=data['context'],\n",
    "                    use_attention=True,\n",
    "                    use_lime=True,\n",
    "                    save_visualizations=True\n",
    "                )\n",
    "                \n",
    "                explanations[question] = explanation\n",
    "            \n",
    "            # Add explanations to result\n",
    "            result['explainability'] = {\n",
    "                'explained_extractions': explanations,\n",
    "                'explanation_count': len(explanations),\n",
    "                'explainability_methods': ['attention_visualization', 'lime_text_explanation'],\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            }\n",
    "        else:\n",
    "            result['explainability'] = {\n",
    "                'message': 'No high-confidence extractions found for explanation',\n",
    "                'min_confidence_threshold': 0.4\n",
    "            }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✅ Explainable Multi-Domain Extraction ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5a27ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing explainability with: invoice_10451.pdf\n",
      "Processing pdf file: invoice_10451.pdf\n",
      " Auto-detected: finance/invoice\n",
      "✅ table_extractor: tabular_data\n",
      "✅ email_extractor: contact_information\n",
      "✅ datetime_extractor: temporal_information\n",
      "\n",
      " Adding explainability to extraction results...\n",
      "\n",
      "✅ EXPLAINABLE EXTRACTION COMPLETE!\n",
      "Domain: finance\n",
      "Success Rate: 62.5%\n",
      "❌ Explanation message: No high-confidence extractions found for explanation\n"
     ]
    }
   ],
   "source": [
    "# Phase 6F: Test Explainable System\n",
    "\n",
    "def get_available_test_file():\n",
    "    \"\"\"Get any available test file for explainability\"\"\"\n",
    "    \n",
    "    test_files = []\n",
    "    \n",
    "    # Check large dataset folder\n",
    "    if os.path.exists(\"./large_scale_invoice_dataset\"):\n",
    "        dataset_files = list(Path(\"./large_scale_invoice_dataset\").glob(\"*.pdf\"))\n",
    "        test_files.extend([str(f) for f in dataset_files[:2]])\n",
    "    \n",
    "    # Return first available file\n",
    "    for file_path in test_files:\n",
    "        if os.path.exists(file_path):\n",
    "            return file_path\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def test_explainable_system():\n",
    "    \"\"\"Test explainability with fallback options\"\"\"\n",
    "    \n",
    "    # Get best available test file\n",
    "    test_file = get_available_test_file()\n",
    "    \n",
    "    if not test_file:\n",
    "        print(\"❌ No test files available, check your dataset paths\")\n",
    "        return\n",
    "    \n",
    "    print(f\" Testing explainability with: {os.path.basename(test_file)}\")\n",
    "    \n",
    "    try:\n",
    "        # Run with error handling\n",
    "        explainable_result = enhanced_multi_domain_extraction_with_explanations(\n",
    "            test_file,\n",
    "            explain_top_results=True,\n",
    "            max_explanations=1  # Reduced to prevent timeout\n",
    "        )\n",
    "        \n",
    "        # Display results with error checking\n",
    "        print(f\"\\n✅ EXPLAINABLE EXTRACTION COMPLETE!\")\n",
    "        \n",
    "        scalability = explainable_result.get('scalability_features', {})\n",
    "        multi_domain = explainable_result.get('multi_domain_extraction', {})\n",
    "        \n",
    "        print(f\"Domain: {scalability.get('domain_detected', 'unknown')}\")\n",
    "        print(f\"Success Rate: {multi_domain.get('success_rate', 0):.1%}\")\n",
    "        \n",
    "        # Check if explanations were generated\n",
    "        explainability = explainable_result.get('explainability', {})\n",
    "        if 'explained_extractions' in explainability:\n",
    "            print(f\" Explanations Generated: {explainability.get('explanation_count', 0)}\")\n",
    "            print(\" EXPLAINABLE AI INTEGRATION SUCCESSFUL!\")\n",
    "        else:\n",
    "            print(f\"❌ Explanation message: {explainability.get('message', 'No explanations')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌Explainability test failed: {e}\")\n",
    "        print(\" This might be due to model/memory limitations\")\n",
    "\n",
    "# Run the robust test\n",
    "test_explainable_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a84d37",
   "metadata": {},
   "source": [
    "**7.\tSecurity and Offline Operability**\n",
    "* Ensure the system runs in secure environments without requiring external cloud APIs or persistent internet access.\n",
    "* Design for deployment in edge devices or private infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae458979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keyring in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (25.6.0)\n",
      "Requirement already satisfied: pywin32-ctypes>=0.2.0 in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from keyring) (0.2.3)\n",
      "Requirement already satisfied: jaraco.classes in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from keyring) (3.4.0)\n",
      "Requirement already satisfied: jaraco.functools in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from keyring) (4.3.0)\n",
      "Requirement already satisfied: jaraco.context in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from keyring) (6.0.1)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\aslia\\anaconda3\\envs\\selflearn\\lib\\site-packages (from jaraco.classes->keyring) (10.8.0)\n",
      "✅ Secure Data Handler ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 7A: Security Framework and Encryption\n",
    "\n",
    "!pip install keyring\n",
    "import keyring\n",
    "import hashlib\n",
    "import secrets\n",
    "import base64\n",
    "from cryptography.fernet import Fernet\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "\n",
    "class SecureDataHandler:\n",
    "    \"\"\"Handles encryption/decryption of sensitive data and model files\"\"\"\n",
    "    \n",
    "    def __init__(self, password: str = None):\n",
    "        self.password = password or self._get_secure_password()\n",
    "        self.key = self._derive_key(self.password)\n",
    "        self.cipher = Fernet(self.key)\n",
    "        \n",
    "    def _get_secure_password(self):\n",
    "        \"\"\"Get password securely (in production, use proper key management)\"\"\"\n",
    "        try:\n",
    "            # Try to get from system keyring first\n",
    "            password = keyring.get_password(\"qa_extractor\", \"main_key\")\n",
    "            if not password:\n",
    "                password = getpass.getpass(\"Enter encryption password for secure storage: \")\n",
    "                keyring.set_password(\"qa_extractor\", \"main_key\", password)\n",
    "            return password\n",
    "        except:\n",
    "            # Fallback to environment or prompt\n",
    "            return os.environ.get(\"QA_EXTRACTOR_KEY\", \"secure_default_key_2024\")\n",
    "    \n",
    "    def _derive_key(self, password: str):\n",
    "        \"\"\"Derive encryption key from password\"\"\"\n",
    "        password_bytes = password.encode()\n",
    "        salt = b\"qa_extractor_salt_2024\"  # In production, use random salt per user\n",
    "        kdf = PBKDF2HMAC(\n",
    "            algorithm=hashes.SHA256(),\n",
    "            length=32,\n",
    "            salt=salt,\n",
    "            iterations=100000,\n",
    "        )\n",
    "        key = base64.urlsafe_b64encode(kdf.derive(password_bytes))\n",
    "        return key\n",
    "    \n",
    "    def encrypt_file(self, file_path: str, output_path: str = None):\n",
    "        \"\"\"Encrypt a file and save to output path\"\"\"\n",
    "        if not output_path:\n",
    "            output_path = file_path + \".encrypted\"\n",
    "            \n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        encrypted_data = self.cipher.encrypt(data)\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(encrypted_data)\n",
    "        \n",
    "        print(f\"Encrypted: {file_path} -> {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def decrypt_file(self, encrypted_path: str, output_path: str = None):\n",
    "        \"\"\"Decrypt a file and save to output path\"\"\"\n",
    "        if not output_path:\n",
    "            output_path = encrypted_path.replace(\".encrypted\", \"\")\n",
    "            \n",
    "        with open(encrypted_path, 'rb') as f:\n",
    "            encrypted_data = f.read()\n",
    "        \n",
    "        decrypted_data = self.cipher.decrypt(encrypted_data)\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(decrypted_data)\n",
    "        \n",
    "        print(f\"Decrypted: {encrypted_path} -> {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def encrypt_text(self, text: str) -> str:\n",
    "        \"\"\"Encrypt text data\"\"\"\n",
    "        return self.cipher.encrypt(text.encode()).decode()\n",
    "    \n",
    "    def decrypt_text(self, encrypted_text: str) -> str:\n",
    "        \"\"\"Decrypt text data\"\"\"\n",
    "        return self.cipher.decrypt(encrypted_text.encode()).decode()\n",
    "\n",
    "print(\"✅ Secure Data Handler ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0046ae13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Offline Model Manager ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 7B: Offline Model Manager\n",
    "\n",
    "class OfflineModelManager:\n",
    "    \"\"\"Manages model storage, loading, and security for offline operation\"\"\"\n",
    "    \n",
    "    def __init__(self, secure_storage_dir=\"./secure_models\"):\n",
    "        self.storage_dir = Path(secure_storage_dir)\n",
    "        self.storage_dir.mkdir(exist_ok=True)\n",
    "        self.security_handler = SecureDataHandler()\n",
    "        self.model_registry = self._load_model_registry()\n",
    "        \n",
    "    def _load_model_registry(self):\n",
    "        \"\"\"Load encrypted model registry\"\"\"\n",
    "        registry_path = self.storage_dir / \"model_registry.encrypted\"\n",
    "        if registry_path.exists():\n",
    "            try:\n",
    "                decrypted_data = self.security_handler.decrypt_text(\n",
    "                    registry_path.read_text()\n",
    "                )\n",
    "                return json.loads(decrypted_data)\n",
    "            except:\n",
    "                print(\" Could not decrypt model registry, creating new one\")\n",
    "        \n",
    "        return {\"models\": {}, \"active_model\": None, \"created_at\": datetime.now().isoformat()}\n",
    "    \n",
    "    def _save_model_registry(self):\n",
    "        \"\"\"Save encrypted model registry\"\"\"\n",
    "        registry_path = self.storage_dir / \"model_registry.encrypted\"\n",
    "        encrypted_data = self.security_handler.encrypt_text(\n",
    "            json.dumps(self.model_registry, indent=2)\n",
    "        )\n",
    "        registry_path.write_text(encrypted_data)\n",
    "    \n",
    "    def store_model_securely(self, model_path: str, model_name: str, description: str = \"\"):\n",
    "        \"\"\"Store model files with encryption\"\"\"\n",
    "        model_dir = self.storage_dir / model_name\n",
    "        model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Encrypt and store all model files\n",
    "        secured_files = {}\n",
    "        \n",
    "        for file_path in Path(model_path).glob(\"*\"):\n",
    "            if file_path.is_file():\n",
    "                encrypted_path = model_dir / f\"{file_path.name}.encrypted\"\n",
    "                self.security_handler.encrypt_file(str(file_path), str(encrypted_path))\n",
    "                secured_files[file_path.name] = str(encrypted_path)\n",
    "        \n",
    "        # Update registry\n",
    "        self.model_registry[\"models\"][model_name] = {\n",
    "            \"description\": description,\n",
    "            \"stored_at\": datetime.now().isoformat(),\n",
    "            \"files\": secured_files,\n",
    "            \"original_path\": str(model_path),\n",
    "            \"secure_path\": str(model_dir)\n",
    "        }\n",
    "        \n",
    "        self._save_model_registry()\n",
    "        print(f\" Model '{model_name}' stored securely with {len(secured_files)} files\")\n",
    "        return model_dir\n",
    "    \n",
    "    def load_model_securely(self, model_name: str, temp_dir: str = None):\n",
    "        \"\"\"Decrypt and load model for use\"\"\"\n",
    "        if model_name not in self.model_registry[\"models\"]:\n",
    "            raise ValueError(f\"Model '{model_name}' not found in secure storage\")\n",
    "        \n",
    "        model_info = self.model_registry[\"models\"][model_name]\n",
    "        \n",
    "        # Create temporary directory for decrypted files\n",
    "        if not temp_dir:\n",
    "            temp_dir = tempfile.mkdtemp(prefix=\"qa_model_\")\n",
    "        else:\n",
    "            Path(temp_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "        # Decrypt all model files to temp directory\n",
    "        for original_name, encrypted_path in model_info[\"files\"].items():\n",
    "            output_path = Path(temp_dir) / original_name\n",
    "            self.security_handler.decrypt_file(encrypted_path, str(output_path))\n",
    "        \n",
    "        print(f\" Model '{model_name}' loaded to temporary directory: {temp_dir}\")\n",
    "        return temp_dir\n",
    "    \n",
    "    def list_secure_models(self):\n",
    "        \"\"\"List all securely stored models\"\"\"\n",
    "        print(\"\\n SECURE MODEL REGISTRY:\")\n",
    "        for name, info in self.model_registry[\"models\"].items():\n",
    "            print(f\"  • {name}: {info['description']}\")\n",
    "            print(f\"    Stored: {info['stored_at']}\")\n",
    "            print(f\"    Files: {len(info['files'])}\")\n",
    "        \n",
    "        if self.model_registry[\"active_model\"]:\n",
    "            print(f\"\\n Active Model: {self.model_registry['active_model']}\")\n",
    "    \n",
    "    def cleanup_temp_files(self, temp_dir: str):\n",
    "        \"\"\"Securely delete temporary decrypted files\"\"\"\n",
    "        if Path(temp_dir).exists():\n",
    "            shutil.rmtree(temp_dir)\n",
    "            print(f\" Cleaned up temporary files: {temp_dir}\")\n",
    "\n",
    "print(\"✅ Offline Model Manager ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5295a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Air-Gapped Extractor ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 7C: Air-Gapped Environment Support\n",
    "\n",
    "class AirGappedExtractor(QABasedExtractor):\n",
    "    \"\"\"QA Extractor designed for completely offline/air-gapped environments\"\"\"\n",
    "    \n",
    "    def __init__(self, secure_model_name: str = None):\n",
    "        self.model_manager = OfflineModelManager()\n",
    "        self.temp_model_dir = None\n",
    "        self.model_name = secure_model_name or \"production_model\"\n",
    "        \n",
    "        # Load model from secure storage\n",
    "        self._load_secure_model()\n",
    "        \n",
    "    def _load_secure_model(self):\n",
    "        \"\"\"Load model from encrypted storage\"\"\"\n",
    "        try:\n",
    "            # Try to load from secure storage first\n",
    "            self.temp_model_dir = self.model_manager.load_model_securely(self.model_name)\n",
    "            \n",
    "            # Initialize QA pipeline with decrypted model\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=self.temp_model_dir,\n",
    "                tokenizer=self.temp_model_dir,\n",
    "                device=-1  # Force CPU for better compatibility\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Loaded secure model: {self.model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Could not load secure model, using base model: {e}\")\n",
    "            # Fallback to base model\n",
    "            super().__init__(model_name=\"deepset/xlm-roberta-large-squad2\")\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up temporary files when object is destroyed\"\"\"\n",
    "        if self.temp_model_dir:\n",
    "            self.model_manager.cleanup_temp_files(self.temp_model_dir)\n",
    "    \n",
    "    def extract_with_security_audit(self, text: str, questions: List[str]) -> Dict:\n",
    "        \"\"\"Extract with full security auditing\"\"\"\n",
    "        # Process document completely offline\n",
    "        # Log every action for security audit\n",
    "        # Add digital fingerprint to results\n",
    "        \n",
    "        \n",
    "        # Generate audit ID\n",
    "        audit_id = hashlib.sha256(\n",
    "            (text[:100] + str(datetime.now())).encode()\n",
    "        ).hexdigest()[:12]\n",
    "        \n",
    "        # Regular extraction\n",
    "        results = self.extract_with_questions(text, questions)\n",
    "        \n",
    "        # Add security metadata\n",
    "        security_info = {\n",
    "            'audit_id': audit_id,\n",
    "            'processing_mode': 'air_gapped',\n",
    "            'model_source': 'secure_storage' if self.temp_model_dir else 'fallback',\n",
    "            'data_hash': hashlib.sha256(text.encode()).hexdigest(),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'no_external_calls': True,\n",
    "            'encryption_used': True\n",
    "        }\n",
    "        \n",
    "        results['security_audit'] = security_info\n",
    "        return results\n",
    "\n",
    "print(\"✅ Air-Gapped Extractor ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0da0724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Edge-Optimized Extractor ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 7D: Edge Device Optimization\n",
    "\n",
    "class EdgeOptimizedExtractor:\n",
    "    \"\"\"Lightweight extractor optimized for edge devices and limited resources\"\"\"\n",
    "    \n",
    "    def __init__(self, model_size=\"small\", max_memory_mb=512):\n",
    "        self.max_memory_mb = max_memory_mb\n",
    "        self.model_size = model_size\n",
    "        \n",
    "        # Choose model based on resource constraints\n",
    "        if model_size == \"tiny\":\n",
    "            model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "        elif model_size == \"small\":\n",
    "            model_name = \"deepset/minilm-uncased-squad2\"\n",
    "        else:\n",
    "            model_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "        \n",
    "        # Initialize with memory optimization\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=model_name,\n",
    "            device=-1,  # CPU only for edge devices\n",
    "            batch_size=1,  # Process one at a time\n",
    "            max_length=256  # Reduced context length\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Edge-optimized extractor ready (model: {model_size})\")\n",
    "    \n",
    "    def extract_lightweight(self, text: str, max_questions: int = 5) -> Dict:\n",
    "        \"\"\"Memory-efficient extraction with limited questions\"\"\"\n",
    "        \n",
    "        # Truncate text if too long\n",
    "        if len(text) > 1000:\n",
    "            text = text[:1000] + \"...\"\n",
    "        \n",
    "        # Use only most important questions\n",
    "        important_questions = [\n",
    "            \"What is the main amount or total?\",\n",
    "            \"What is the document number?\",\n",
    "            \"What company or organization is mentioned?\",\n",
    "            \"What date is mentioned?\",\n",
    "            \"Who is the person or contact mentioned?\"\n",
    "        ][:max_questions]\n",
    "        \n",
    "        results = {}\n",
    "        for question in important_questions:\n",
    "            try:\n",
    "                result = self.qa_pipeline(\n",
    "                    question=question,\n",
    "                    context=text,\n",
    "                    max_answer_len=50  # Short answers only\n",
    "                )\n",
    "                \n",
    "                results[question] = {\n",
    "                    'answer': result['answer'],\n",
    "                    'confidence': result['score'],\n",
    "                    'extracted': result['score'] > 0.1  # Lower threshold for edge\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[question] = {'error': str(e), 'extracted': False}\n",
    "        \n",
    "        return {\n",
    "            'extractions': results,\n",
    "            'model_size': self.model_size,\n",
    "            'memory_optimized': True,\n",
    "            'edge_compatible': True\n",
    "        }\n",
    "\n",
    "print(\"✅ Edge-Optimized Extractor ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a38cb7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ❌ Could not load secure model, using base model: Model 'production_model' not found in secure storage\n",
      "Loading LoRA fine-tuned model from ./lora_fine_tuned_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Secure Offline System ready!\n"
     ]
    }
   ],
   "source": [
    "# Phase 7E: Secure Pipeline Integration\n",
    "\n",
    "class SecureOfflineSystem:\n",
    "    \"\"\"Combines everything into one complet secure, offline-capable extraction system\"\"\"\n",
    "    \n",
    "    def __init__(self, security_level=\"high\"):\n",
    "        self.security_level = security_level\n",
    "        self.security_handler = SecureDataHandler()\n",
    "        self.model_manager = OfflineModelManager()\n",
    "        \n",
    "        # Initialize appropriate extractor based on security level\n",
    "        if security_level == \"high\":\n",
    "            self.extractor = AirGappedExtractor()\n",
    "        elif security_level == \"edge\":\n",
    "            self.extractor = EdgeOptimizedExtractor()\n",
    "        else:\n",
    "            self.extractor = QABasedExtractor()\n",
    "        \n",
    "        # Security audit log\n",
    "        self.audit_log = []\n",
    "        \n",
    "    def process_document_securely(self, file_path: str, delete_after_processing: bool = True):\n",
    "        \"\"\"Process document with full security measures\"\"\"\n",
    "        # Generate unique ID for this processing\n",
    "        # Extract data with chosen security level\n",
    "        # Log all security events\n",
    "        # Optionally delete original file\n",
    "        # Encrypt results before saving\n",
    "        \n",
    "        \n",
    "        # Generate processing ID\n",
    "        process_id = secrets.token_hex(8)\n",
    "        \n",
    "        try:\n",
    "            # Log start of processing\n",
    "            self._log_security_event(\"PROCESSING_START\", {\n",
    "                'process_id': process_id,\n",
    "                'file_name': Path(file_path).name,\n",
    "                'file_size': Path(file_path).stat().st_size,\n",
    "                'security_level': self.security_level\n",
    "            })\n",
    "            \n",
    "            # Load and normalize document\n",
    "            document_data = load_and_normalize(file_path)\n",
    "            \n",
    "            # Extract text content\n",
    "            all_text = \"\"\n",
    "            for content_item in document_data['content']:\n",
    "                all_text += content_item['text'] + \" \"\n",
    "            \n",
    "            # Secure extraction\n",
    "            if hasattr(self.extractor, 'extract_with_security_audit'):\n",
    "                extraction_results = self.extractor.extract_with_security_audit(\n",
    "                    all_text, \n",
    "                    self.extractor.extraction_templates['german_invoice']\n",
    "                )\n",
    "            else:\n",
    "                extraction_results = self.extractor.extract_information(all_text)\n",
    "            \n",
    "            # Add security metadata\n",
    "            extraction_results['security_info'] = {\n",
    "                'process_id': process_id,\n",
    "                'security_level': self.security_level,\n",
    "                'offline_mode': True,\n",
    "                'encryption_available': True,\n",
    "                'audit_trail': len(self.audit_log)\n",
    "            }\n",
    "            \n",
    "            # Log successful processing\n",
    "            self._log_security_event(\"PROCESSING_SUCCESS\", {\n",
    "                'process_id': process_id,\n",
    "                'extractions_found': extraction_results.get('successful_extractions', 0)\n",
    "            })\n",
    "            \n",
    "            # Optionally delete source file for security\n",
    "            if delete_after_processing:\n",
    "                os.remove(file_path)\n",
    "                self._log_security_event(\"FILE_DELETED\", {'process_id': process_id})\n",
    "            \n",
    "            return extraction_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._log_security_event(\"PROCESSING_ERROR\", {\n",
    "                'process_id': process_id,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            raise\n",
    "    \n",
    "    def _log_security_event(self, event_type: str, details: dict):\n",
    "        \"\"\"Log security events for audit trail\"\"\"\n",
    "        event = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'event_type': event_type,\n",
    "            'details': details,\n",
    "            'system_user': os.getenv('USERNAME', 'unknown')\n",
    "        }\n",
    "        \n",
    "        self.audit_log.append(event)\n",
    "        \n",
    "        # Also log to secure file\n",
    "        log_file = Path(\"security_audit.log\")\n",
    "        with open(log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(event) + \"\\n\")\n",
    "    \n",
    "    def export_secure_results(self, results: dict, output_path: str):\n",
    "        \"\"\"Export results with encryption\"\"\"\n",
    "        \n",
    "        # Convert results to JSON\n",
    "        results_json = json.dumps(results, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Encrypt the results\n",
    "        encrypted_path = output_path + \".encrypted\"\n",
    "        encrypted_data = self.security_handler.encrypt_text(results_json)\n",
    "        \n",
    "        with open(encrypted_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(encrypted_data)\n",
    "        \n",
    "        print(f\" ✅ Results exported securely to: {encrypted_path}\")\n",
    "        return encrypted_path\n",
    "    \n",
    "    def get_security_audit_report(self):\n",
    "        \"\"\"Generate security audit report\"\"\"\n",
    "        report = {\n",
    "            'total_events': len(self.audit_log),\n",
    "            'security_level': self.security_level,\n",
    "            'recent_events': self.audit_log[-10:],  # Last 10 events\n",
    "            'event_types': {},\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Count event types\n",
    "        for event in self.audit_log:\n",
    "            event_type = event['event_type']\n",
    "            report['event_types'][event_type] = report['event_types'].get(event_type, 0) + 1\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize secure system\n",
    "secure_system = SecureOfflineSystem(security_level=\"high\")\n",
    "print(\"✅ Secure Offline System ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b730d",
   "metadata": {},
   "source": [
    "❌ Could not load secure model, using base model... = It's refusing to load a model that doesn't exist in secure storage and falling back safely to a known model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ad1b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TESTING SECURE OFFLINE SYSTEM\n",
      "==================================================\n",
      "Processing pdf file: test_secure_processing.pdf\n",
      "\n",
      "✅ SECURE PROCESSING RESULTS:\n",
      "   Security Level: high\n",
      "   Process ID: 6eeb6402390dd29c\n",
      "   Offline Mode: True\n",
      "   Extractions Found: 0\n",
      " ✅ Results exported securely to: secure_extraction_results.json.encrypted\n",
      "\n",
      " SECURITY AUDIT:\n",
      "   Total Events: 3\n",
      "   Event Types: {'PROCESSING_START': 1, 'PROCESSING_SUCCESS': 1, 'FILE_DELETED': 1}\n",
      "\n",
      " ✅ SECURE OFFLINE SYSTEM TEST COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# 7F: Test Complete Secure System\n",
    "\n",
    "def test_secure_offline_system():\n",
    "    \"\"\"Test the complete secure, offline system\"\"\"\n",
    "    \n",
    "    print(\" TESTING SECURE OFFLINE SYSTEM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test file\n",
    "    test_file = r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\\SWME_Rechnung_07122020.pdf\"\n",
    "    \n",
    "    if os.path.exists(test_file):\n",
    "        # Make a copy for testing (don't delete original)\n",
    "        test_copy = \"test_secure_processing.pdf\"\n",
    "        shutil.copy2(test_file, test_copy)\n",
    "        \n",
    "        try:\n",
    "            # Process with secure system\n",
    "            results = secure_system.process_document_securely(\n",
    "                test_copy, \n",
    "                delete_after_processing=True  # Will delete the copy\n",
    "            )\n",
    "            \n",
    "            print(\"\\n✅ SECURE PROCESSING RESULTS:\")\n",
    "            print(f\"   Security Level: {results['security_info']['security_level']}\")\n",
    "            print(f\"   Process ID: {results['security_info']['process_id']}\")\n",
    "            print(f\"   Offline Mode: {results['security_info']['offline_mode']}\")\n",
    "            print(f\"   Extractions Found: {results.get('successful_extractions', 0)}\")\n",
    "            \n",
    "            # Show some extractions\n",
    "            if 'extractions' in results:\n",
    "                print(\"\\n SAMPLE EXTRACTIONS:\")\n",
    "                count = 0\n",
    "                for question, result in results['extractions'].items():\n",
    "                    if result.get('answer') and count < 3:\n",
    "                        print(f\"   Q: {question[:50]}...\")\n",
    "                        print(f\"   A: {result['answer']} (conf: {result['confidence']:.3f})\")\n",
    "                        count += 1\n",
    "            \n",
    "            # Export results securely\n",
    "            encrypted_output = secure_system.export_secure_results(\n",
    "                results, \n",
    "                \"secure_extraction_results.json\"\n",
    "            )\n",
    "            \n",
    "            # Show security audit\n",
    "            audit_report = secure_system.get_security_audit_report()\n",
    "            print(f\"\\n SECURITY AUDIT:\")\n",
    "            print(f\"   Total Events: {audit_report['total_events']}\")\n",
    "            print(f\"   Event Types: {audit_report['event_types']}\")\n",
    "            \n",
    "            print(\"\\n ✅ SECURE OFFLINE SYSTEM TEST COMPLETE!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in secure processing: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ Test file not found - update path for testing\")\n",
    "\n",
    "# Run the test\n",
    "test_secure_offline_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selflearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
