{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6a0788",
   "metadata": {},
   "source": [
    "## Self-Learning Data Extraction and Auto Fine-Tuning System for Structured and Unstructured Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f61df0",
   "metadata": {},
   "source": [
    "**1. Unified Data Handling**\n",
    "* Develop techniques to process and normalize structured (e.g., CSV, SQL, PDFs), unstructured (e.g., text, images), and streaming data from diverse sources.\n",
    "* Enable seamless integration with offline files, legacy datasets, and live data pipelines (e.g., via data federation or APIs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all installs\n",
    "\n",
    "!pip install pdfplumber\n",
    "!pip install xlrd \n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install pytesseract Pillow \n",
    "!pip install matplotlib seaborn\n",
    "!pip install shap\n",
    "!pip install lime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necesary libraries\n",
    "\n",
    "# Fix protobufs:\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\" \n",
    "os.environ[\"TRANSFORMERS_NO_SLOW_TOKENIZER\"] = \"1\"\n",
    "\n",
    "# Tesseract\n",
    "try:\n",
    "    import pytesseract\n",
    "    pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "except ImportError:\n",
    "    print(\"pytesseract not installed yet\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import sqlite3 \n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import torch, numpy, spacy\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from difflib import SequenceMatcher \n",
    "\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e38b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety switches for Run All\n",
    "\n",
    "SAFE_RUN = True       # Skip heavy/long steps (fine-tune) on first full run\n",
    "INTERACTIVE = False   # Avoid input() prompts during Run All\n",
    "print(\"SAFE_RUN =\", SAFE_RUN, \"| INTERACTIVE =\", INTERACTIVE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75315e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Simple type detection function \n",
    "\n",
    "# This function determines the file type based on the file extension\n",
    "\n",
    "def detect_file_type(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        return \"csv\"\n",
    "    elif ext == \".pdf\":\n",
    "        return \"pdf\"\n",
    "    elif ext == \".txt\":\n",
    "        return \"text\"\n",
    "    elif ext in [\".xlsx\", \".xls\"]:  \n",
    "        return \"excel\"\n",
    "    elif ext == \".db\": \n",
    "        return \"sqlite\"\n",
    "    elif ext in [\".png\", \".jpg\", \".jpeg\", \".tiff\", \".bmp\"]: \n",
    "        return \"image\"\n",
    "    else:\n",
    "        return \"unsupported\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File loading functions for different data types \n",
    "# These functions handle the actual data extraction from supported file formats\n",
    "# Each loader returns data in a standardized format for downstream processing\n",
    "\n",
    "def load_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.to_dict(orient=\"records\")  # returns list of row dictionaries\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    data = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                data.append(text)\n",
    "    return {\"pages\": data}\n",
    "\n",
    "def load_excel(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    return df.to_dict(orient=\"records\")\n",
    "\n",
    "def load_sqlite(file_path, table_name):\n",
    "    conn = sqlite3.connect(file_path)\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    return df.to_dict(orient=\"records\")\n",
    "\n",
    "def load_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return {\"text\": f.read()}\n",
    "    \n",
    "def load_image_with_ocr(file_path):\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        import pytesseract\n",
    "        \n",
    "        # Set Tesseract path for Windows\n",
    "        pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "        \n",
    "        image = Image.open(file_path)\n",
    "        # Enhanced OCR \n",
    "        text = pytesseract.image_to_string(\n",
    "            image, \n",
    "            config='--oem 3 --psm 6 -l eng+deu'  # Multi-language support\n",
    "        )\n",
    "        return {\"extracted_text\": text.strip(), \"source\": \"ocr\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"OCR failed: {str(e)}\", \"extracted_text\": \"\"}\n",
    "\n",
    "\n",
    "def load_file(file_path):\n",
    "    file_type = detect_file_type(file_path)\n",
    "    if file_type == \"csv\":\n",
    "        return load_csv(file_path)\n",
    "    elif file_type == \"pdf\":\n",
    "        return load_pdf(file_path)\n",
    "    elif file_type == \"text\":\n",
    "        return load_text(file_path)\n",
    "    elif file_type == \"excel\":  \n",
    "        return load_excel(file_path)  \n",
    "    elif file_type == \"sqlite\": \n",
    "        return load_sqlite(file_path) \n",
    "    elif file_type == \"image\":\n",
    "        return load_image_with_ocr(file_path)\n",
    "    else: \n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing text file\n",
    "\n",
    "file_path = \"example.txt\"\n",
    "data = load_file(r\"C:\\Users\\aslia\\OneDrive\\Desktop\\github\\Predicting-Train-Delays\\README.txt\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d267886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_live_data(api_url, headers=None, timeout=10):\n",
    "    try:\n",
    "        response = requests.get(api_url, headers=headers, timeout=timeout)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Handle different content types\n",
    "            content_type = response.headers.get('content-type', '')\n",
    "            \n",
    "            if 'json' in content_type:\n",
    "                data = response.json()\n",
    "            else:\n",
    "                data = response.text\n",
    "                \n",
    "            return [{\n",
    "                \"file_name\": f\"api_data_{int(time.time())}\",  # Unique timestamp\n",
    "                \"file_type\": \"json\" if 'json' in content_type else \"text\",\n",
    "                \"source\": \"live_api\",\n",
    "                \"content\": [{\n",
    "                    \"section_id\": 0,\n",
    "                    \"text\": str(data),\n",
    "                    \"metadata\": {\n",
    "                        \"source\": api_url,\n",
    "                        \"timestamp\": time.time(),\n",
    "                        \"status_code\": response.status_code\n",
    "                    }\n",
    "                }]\n",
    "            }]\n",
    "        else:\n",
    "            raise Exception(f\"API returned status code: {response.status_code}\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to fetch live data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Extracted Content\n",
    "\n",
    "# These functions convert extracted data into a standardized format for consistent processing\n",
    "# All functions return a list of dictionaries with 'section_id', 'text', and 'metadata' fields\n",
    "\"\"\"Detects file type cals the appropriate normalization function and\n",
    "Returns unified document structure\"\"\"\n",
    "\n",
    "def normalize_csv(file_path):  # Normalizes CSV data by converting each row into a standardized content block.\n",
    "    df = pd.read_csv(file_path)\n",
    "    content = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content.append({ \"section_id\":idx, \n",
    "                        \"text\":str(row.to_dict()), \"metadata\": {\"row\": idx} # Convert row data to string representation\n",
    "                        }\n",
    "                       )\n",
    "    return content\n",
    "\n",
    "def normalize_pdf(file_path): # All functions return a list of dictionaries with 'section_id', 'text', and 'metadata' fields\n",
    "    content = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:   # Only include pages with extractable text\n",
    "                content.append({\n",
    "                    \"section_id\": i,\n",
    "                    \"text\": text,\n",
    "                    \"metadata\": {\"page\": i + 1}\n",
    "                })\n",
    "    return content\n",
    "\n",
    "def normalize_text(file_path): # Normalizes text files by treating each non-empty line as a separate content section.\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    content = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():  # skip blank lines\n",
    "            content.append({\n",
    "                \"section_id\": i,\n",
    "                \"text\": line.strip(),\n",
    "                \"metadata\": {\"line\": i + 1}\n",
    "            })\n",
    "    return content\n",
    "\n",
    "def normalize_excel(file_path): # Excel loader legacy dataset\n",
    "    df = pd.read_excel(file_path)\n",
    "    content = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content.append({\n",
    "            \"section_id\": idx,\n",
    "            \"text\": str(row.to_dict()),\n",
    "            \"metadata\": {\"row\": idx}\n",
    "        })\n",
    "    return content\n",
    "\n",
    "\n",
    "# SQLite support  dataset\n",
    "\n",
    "def normalize_sqlite(db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    content = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content.append({\n",
    "            \"section_id\": idx,\n",
    "            \"text\": str(row.to_dict()),\n",
    "            \"metadata\": {\"row\": idx}\n",
    "        })\n",
    "    return content\n",
    "\n",
    "\n",
    "# image normalization function\n",
    "\n",
    "def normalize_image(file_path):\n",
    "    ocr_result = load_image_with_ocr(file_path)\n",
    "    \n",
    "    if ocr_result.get(\"error\"):\n",
    "        return [{\"section_id\": 0, \"text\": \"\", \"metadata\": {\"error\": ocr_result[\"error\"]}}]\n",
    "    \n",
    "    text = ocr_result[\"extracted_text\"]\n",
    "    if not text:\n",
    "        return [{\"section_id\": 0, \"text\": \"\", \"metadata\": {\"error\": \"No text found\"}}]\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    content = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():\n",
    "            content.append({\n",
    "                \"section_id\": i,\n",
    "                \"text\": line.strip(),\n",
    "                \"metadata\": {\"line\": i + 1, \"source\": \"ocr\"}\n",
    "            })\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Master function that loads and normalizes any supported file type.\n",
    "\n",
    "# Creates a unified data structure regardles of input file format.\n",
    "    \n",
    "def load_and_normalize(file_path, table_name=None):\n",
    "    \n",
    "    try:  # Check if file exists\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError (f\"File not found: {file_path}\")\n",
    "    \n",
    "        file_type = detect_file_type(file_path)\n",
    "        file_name = os.path.basename(file_path) # Extract filename without path\n",
    "        print(f\"Processing {file_type} file: {file_name}\")\n",
    "        \n",
    "# appropriatee normalization function based on file type\n",
    "\n",
    "        if file_type == 'csv':\n",
    "            content = normalize_csv(file_path)\n",
    "        elif file_type == 'pdf':\n",
    "            content = normalize_pdf(file_path)\n",
    "        elif file_type == 'text':\n",
    "            content = normalize_text(file_path)\n",
    "        elif file_type == 'excel':\n",
    "            content = normalize_excel(file_path)\n",
    "        elif file_type == 'sqlite':\n",
    "            if not table_name:\n",
    "                raise ValueError(\"sqlite files required a table_name\")\n",
    "            content = normalize_sqlite(file_path, table_name)\n",
    "        elif file_type == 'image': \n",
    "            content = normalize_image(file_path)\n",
    "    \n",
    "        elif file_type == 'unsupported':\n",
    "            raise ValueError(f\"Unsupported file type for: {file_name}\")\n",
    "    \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type:{file_type}\")\n",
    "    \n",
    "# Return standardized document structure                        \n",
    "        return { \n",
    "                \"file_name\": file_name,\n",
    "                \"file_type\": file_type,\n",
    "                \"source\": \"offline\",\n",
    "                \"content\": content,\n",
    "                \"processed_at\": time.time() # Adds timestamp\n",
    "            }\n",
    "    except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "data = load_and_normalize(r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\Rechnung.pdf\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9c8be",
   "metadata": {},
   "source": [
    "**2. Intelligent Information Extraction**\n",
    "* Design or adapt machine learning and NLP models to automatically extract key entities, values, and patterns from heterogeneous data.\n",
    "* Support document parsing, table recognition, entity linking, and contextual extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d86690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding missing imports\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a55320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QABasedExtractor\n",
    "\n",
    "class QABasedExtractor:\n",
    "    def __init__(self, model_name=\"deepset/xlm-roberta-large-squad2\", local_dir=\"./fine_tuned_model\"):\n",
    "        \n",
    "       \n",
    "        # Always define to avoid AttributeError\n",
    "        self.ner_pipeline = None\n",
    "        \n",
    "        # Prefer local fine-tuned model if available\n",
    "        model_to_use = local_dir if (os.path.isdir(local_dir) and os.path.exists(os.path.join(local_dir, \"config.json\"))) else model_name\n",
    "        print(f\"Loading model: {model_to_use}\")\n",
    "\n",
    "        self.model_name = model_to_use\n",
    "        try:\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=model_to_use,\n",
    "                tokenizer=model_to_use,\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            \n",
    "            # NER pipeline for entity extraction\n",
    "            #self.ner_pipeline = pipeline(\n",
    "                #\"ner\",\n",
    "                #model=\"xlm-roberta-large-finetuned-conll03-multilingual\",\n",
    "                #aggregation_strategy=\"simple\"\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {e}\")\n",
    "            # Fallback to your original model if needed\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=\"distilbert-base-cased-distilled-squad\"\n",
    "            )\n",
    "            self.ner_pipeline = None\n",
    "        \n",
    "        # Updated question templates with better targeting\n",
    "        \n",
    "        self.extraction_templates = {\n",
    "            \"invoice\": [\n",
    "                \"What number appears after 'Invoice Number' or 'Rechnungsnummer'?\",\n",
    "                \"What amount appears after 'Total' or 'Gesamtbetrag' or 'Endbetrag'?\",\n",
    "                \"What company name appears at the top of the document?\",\n",
    "                \"What date appears after 'Invoice Date' or 'Rechnungsdatum'?\",\n",
    "                \"What is the largest monetary amount mentioned?\",\n",
    "                \"What customer name appears on the invoice?\",\n",
    "                \"What tax amount is mentioned?\",\n",
    "                \"What is the net amount before tax?\"\n",
    "            ],\n",
    "            \"german_invoice\": [\n",
    "                \"Welche Nummer steht nach 'Rechnungsnummer'?\",\n",
    "                \"Welcher Betrag steht nach 'Gesamtbetrag' oder 'Endbetrag'?\",\n",
    "                \"Wie heiÃŸt die Firma auf der Rechnung?\",\n",
    "                \"Welches Datum steht nach 'Rechnungsdatum'?\",\n",
    "                \"What number appears after 'Rechnungsnummer'?\",\n",
    "                \"What amount appears after 'Gesamtbetrag' or 'Total'?\",\n",
    "                \"What company name is mentioned?\",\n",
    "                \"What date appears after 'Rechnungsdatum'?\",\n",
    "                \"What is the highest amount in Euro mentioned?\",\n",
    "                \"What is the MwSt or USt amount?\",\n",
    "                \"What services or products are listed?\",\n",
    "                \"What is the invoice number?\",\n",
    "                \"What is the Rechnungsnummer?\",\n",
    "                \"What is the total amount?\",\n",
    "                \"What is the Gesamtbetrag?\", \n",
    "                \"What is the Endbetrag?\",\n",
    "                \"What is the vendor name?\",\n",
    "                \"What is the company name?\",\n",
    "                \"What is the Firmenname?\",\n",
    "                \"What is the invoice date?\",\n",
    "                \"What is the Rechnungsdatum?\",\n",
    "                \"What is the billing date?\",\n",
    "                \"Who is the customer?\",\n",
    "                \"What is the Kunde?\",\n",
    "                \"What services were provided?\",\n",
    "                \"What is the Leistung?\",\n",
    "                \"What is the tax amount?\",\n",
    "                \"What is the Mehrwertsteuer?\",\n",
    "                \"What is the USt?\",\n",
    "                \"What is the net amount?\",\n",
    "                \"What is the Nettobetrag?\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"What are the most important numbers in this document?\",\n",
    "                \"What companies or organizations are mentioned?\",\n",
    "                \"What dates are mentioned?\",\n",
    "                \"What monetary amounts are mentioned?\",\n",
    "                \"What are the key facts in this document?\",\n",
    "                \"What is the main topic or subject of this document?\",\n",
    "                \"What names of people are mentioned?\",\n",
    "                \"What locations or addresses are mentioned?\",\n",
    "                \"What email addresses or contact information is provided?\",\n",
    "                \"What phone numbers are listed?\",\n",
    "                \"What percentages or statistics are mentioned?\",\n",
    "                \"What products or services are described?\",\n",
    "                \"What deadlines or time periods are mentioned?\",\n",
    "                \"What requirements or specifications are listed?\",\n",
    "                \"What actions or tasks are described?\",\n",
    "                \"What problems or issues are identified?\",\n",
    "                \"What solutions or recommendations are provided?\",\n",
    "                \"What project names or codes are mentioned?\",\n",
    "                \"What versions or releases are referenced?\",\n",
    "                \"What technologies or tools are discussed?\",\n",
    "                \"What departments or teams are mentioned?\",\n",
    "                \"What metrics or measurements are provided?\",\n",
    "                \"What goals or objectives are stated?\",\n",
    "                \"What risks or concerns are identified?\",\n",
    "                \"What benefits or advantages are highlighted?\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Enhanced preprocessing for invoices\"\"\"\n",
    "        # basic cleaning\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        # invoice-specific preprocessing\n",
    "        # context markers to help AI understand\n",
    "        text = re.sub(r'(Rechnungsnummer|Invoice Number)[\\s:]*([A-Z0-9\\-]+)', \n",
    "                      r'The invoice number is \\2. Rechnungsnummer: \\2', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        text = re.sub(r'(Gesamtbetrag|Total|Endbetrag)[\\s:]*([â‚¬$]?\\s*[\\d,\\.]+)', \n",
    "                      r'The total amount is \\2. Gesamtbetrag: \\2', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # limit length\n",
    "        if len(text) > 2000:\n",
    "            text = text[:2000] + \"...\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_entities_with_ner(self, text: str) -> Dict:\n",
    "        \"\"\"Enhanced NER extraction\"\"\"\n",
    "        ner = getattr(self, \"ner_pipeline\", None)\n",
    "        if not ner:\n",
    "            return {\n",
    "                'organizations': [],\n",
    "                'money': [],\n",
    "                'dates': [],\n",
    "                'persons': []\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            entities = ner(text)\n",
    "            processed = {'organizations': [], 'money': [], 'dates': [], 'persons': []}\n",
    "            for entity in entities:\n",
    "                entity_type = entity.get('entity_group', '')\n",
    "                word = entity.get('word', '').strip()\n",
    "                if entity_type == 'ORG' and len(word) > 2:\n",
    "                    processed['organizations'].append(word)\n",
    "                elif entity_type == 'PER' and len(word) > 2:\n",
    "                    processed['persons'].append(word)\n",
    "            return processed\n",
    "        except Exception:\n",
    "            return {}\n",
    "        \n",
    "    def extract_with_questions(self, text: str, questions: List[str], \n",
    "                             confidence_threshold: float = 0.05) -> Dict:  # Lower threshold\n",
    "        text = self.preprocess_text(text)\n",
    "        results = {}\n",
    "        \n",
    "        for question in questions:\n",
    "            try:\n",
    "                qa_result = self.qa_pipeline(\n",
    "                    question=question,\n",
    "                    context=text,\n",
    "                    max_answer_len=150  # Longer answers\n",
    "                )\n",
    "                \n",
    "                results[question] = {\n",
    "                    'answer': qa_result['answer'] if qa_result['score'] >= confidence_threshold else None,\n",
    "                    'confidence': qa_result['score'],\n",
    "                    'start_pos': qa_result.get('start', 0),\n",
    "                    'end_pos': qa_result.get('end', 0),\n",
    "                    'extracted': qa_result['score'] >= confidence_threshold\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[question] = {\n",
    "                    'answer': None,\n",
    "                    'confidence': 0.0,\n",
    "                    'error': str(e),\n",
    "                    'extracted': False\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def auto_detect_document_type(self, text: str) -> str:\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Better German detection\n",
    "        german_keywords = ['rechnung', 'rechnungsnummer', 'mehrwertsteuer', 'ust', 'gesamtbetrag', 'firmenname']\n",
    "        if any(keyword in text_lower for keyword in german_keywords):\n",
    "            return 'german_invoice'\n",
    "        \n",
    "        # English invoice detection\n",
    "        if any(word in text_lower for word in ['invoice', 'bill to', 'invoice number']):\n",
    "            return 'invoice'\n",
    "        \n",
    "        return 'general'\n",
    "    \n",
    "    def extract_information(self, text: str, document_type: Optional[str] = None, \n",
    "                          custom_questions: Optional[List[str]] = None) -> Dict:\n",
    "        \n",
    "        # Autodetect if not provided\n",
    "        if document_type is None:\n",
    "            document_type = self.auto_detect_document_type(text)\n",
    "        \n",
    "        # Choose questions\n",
    "        if custom_questions:\n",
    "            questions = custom_questions\n",
    "        else:\n",
    "            questions = self.extraction_templates.get(document_type, self.extraction_templates['general'])\n",
    "        \n",
    "        # Extract with QA\n",
    "        qa_results = self.extract_with_questions(text, questions)\n",
    "        \n",
    "        # Extract entities with NER if available\n",
    "        ner_results = self.extract_entities_with_ner(text)\n",
    "        \n",
    "        # Combine results\n",
    "        successful = sum(1 for r in qa_results.values() if r.get('extracted'))\n",
    "        \n",
    "        return {\n",
    "            'document_type': document_type,\n",
    "            'extraction_timestamp': datetime.now().isoformat(),\n",
    "            'model_used': self.model_name,\n",
    "            'total_questions': len(questions),\n",
    "            'successful_extractions': successful,\n",
    "            'success_rate': successful / len(questions) if questions else 0,\n",
    "            'extractions': qa_results,\n",
    "            'entities': ner_results\n",
    "        }\n",
    "        \n",
    "    # Only uses high-confidence predictions for further training\n",
    "    # This prevents error propagation\n",
    "    def get_high_confidence_extractions(self, results: Dict, min_confidence: float = 0.3) -> Dict:\n",
    "        \"\"\"Lower confidence threshold for invoice extraction\"\"\"\n",
    "        high_conf = {}\n",
    "        \n",
    "        for question, result in results['extractions'].items():\n",
    "            if result.get('confidence', 0) >= min_confidence and result.get('answer'):\n",
    "                high_conf[question] = result\n",
    "        \n",
    "        return {\n",
    "            'document_type': results['document_type'],\n",
    "            'high_confidence_extractions': high_conf,\n",
    "            'total_high_confidence': len(high_conf)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7785e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data processing pipeline with QA extraction\n",
    "\n",
    "def process_document_with_qa(document_data: Dict, custom_questions: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a normalized document using QA-based extraction.\n",
    "    \"\"\"\n",
    "    extractor = QABasedExtractor()\n",
    "    \n",
    "    # Combine all text content from the document\n",
    "    all_text = \"\"\n",
    "    for content_item in document_data['content']:\n",
    "        all_text += content_item['text'] + \" \"\n",
    "    \n",
    "    \"\"\"Takes your normalized document (which has text split by pages/sections)\n",
    "Combines everything into one big text string\n",
    "This gives the AI the full context to answer questions\"\"\"\n",
    "\n",
    "    # Extract information using QA\n",
    "    extraction_results = extractor.extract_information(\n",
    "        text=all_text,\n",
    "        custom_questions=custom_questions\n",
    "    )\n",
    "    \"\"\"Passes the combined text to your AI extractor\n",
    "Uses custom questions if provided (like your German questions)\n",
    "Gets back structured answers with confidence scores\"\"\"\n",
    "\n",
    "    # extraction results to document data\n",
    "    \n",
    "    document_data['qa_extraction'] = extraction_results\n",
    "    document_data['high_confidence_extractions'] = extractor.get_high_confidence_extractions(\n",
    "        extraction_results\n",
    "    )\n",
    "    \"\"\"Adds AI results to  original document structure\n",
    "Creates two versions: all results + high-confidence only\n",
    "Preserves original data while adding AI insights\"\"\"\n",
    "    \n",
    "    return document_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration with existing pipeline\n",
    "def enhanced_load_and_normalize_with_qa(file_path: str, table_name: Optional[str] = None, \n",
    "                                       custom_questions: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced version of load_and_normalize that includes QA extraction.\n",
    "    \"\"\"\n",
    "    # Use existing normalization function\n",
    "    document_data = load_and_normalize(file_path, table_name)\n",
    "    \n",
    "    # QA-based extraction\n",
    "    enhanced_data = process_document_with_qa(document_data, custom_questions)\n",
    "    \"\"\"Takes the normalized document\n",
    "Runs AI question-answering\n",
    "Adds intelligent extraction results\n",
    "Returns enhanced document with AI insights\"\"\"\n",
    "    return enhanced_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a80b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-trigger fine-tuning when enough new feedback exists\n",
    "try:\n",
    "    if not SAFE_RUN:\n",
    "        print(\"Checking if fine-tuning should run from feedback...\")\n",
    "        maybe_finetune(threshold_new_rows=5)\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"SAFE_RUN=True: skipping fine-tune auto-trigger\")\n",
    "except NameError:\n",
    "    print(\"Skipping auto-trigger: maybe_finetune not defined yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1328736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with German PDF\n",
    "german_path = r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\Rechnung.pdf\"\n",
    "\n",
    "# Load and process\n",
    "result = enhanced_load_and_normalize_with_qa(german_path)\n",
    "\n",
    "# Show improved results\n",
    "print(f\"âœ… Document: {result['file_name']}\")\n",
    "print(f\"âœ… Type: {result['qa_extraction']['document_type']}\")\n",
    "print(f\"âœ… Success Rate: {result['qa_extraction']['success_rate']:.1%}\")\n",
    "print(f\"âœ… Extractions: {result['qa_extraction']['successful_extractions']}/{result['qa_extraction']['total_questions']}\")\n",
    "\n",
    "print(\"\\n=== High Confidence Results ===\")\n",
    "for question, answer in result['high_confidence_extractions']['high_confidence_extractions'].items():\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer['answer']} (confidence: {answer['confidence']:.3f})\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Show NER entities if found\n",
    "if result['qa_extraction'].get('entities'):\n",
    "    print(\"\\n=== Named Entities Found ===\")\n",
    "    for entity_type, entities in result['qa_extraction']['entities'].items():\n",
    "        if entities:\n",
    "            print(f\"{entity_type}: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of  multi-format system (extra .txt file)\n",
    "\n",
    "print(\"Testing Multi-Format Document Extraction\")\n",
    "\n",
    "test_files = [\n",
    "    r\"C:\\Users\\aslia\\OneDrive\\Desktop\\github\\Predicting-Train-Delays\\README.txt\"]\n",
    "\n",
    "\n",
    "for file_path in test_files:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"\\n Testing: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            result = enhanced_load_and_normalize_with_qa(file_path)\n",
    "            print(f\"âœ… Type: {result['qa_extraction']['document_type']}\")\n",
    "            print(f\"âœ… Success Rate: {result['qa_extraction']['success_rate']:.1%}\")\n",
    "            \n",
    "            # Show top 3 extractions\n",
    "            count = 0\n",
    "            for question, answer in result['qa_extraction']['extractions'].items():\n",
    "                if answer.get('answer') and count < 3:\n",
    "                    print(f\"   Q: {question[:50]}...\")\n",
    "                    print(f\"   A: {answer['answer']}\")\n",
    "                    count += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "    else:\n",
    "        print(f\" File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with OCR + QA system\n",
    "\n",
    "print(\"=== Testing OCR Integration with YOUR System ===\")\n",
    "\n",
    "# Test image path\n",
    "image_path = r\"C:\\Users\\aslia\\Downloads\\invoice_sample.png\"\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    \n",
    "    # Use  existing pipeline\n",
    "    result = enhanced_load_and_normalize_with_qa(image_path)\n",
    "    \n",
    "    print(f\"âœ… OCR + QA Success!\")\n",
    "    print(f\"âœ… Document: {result['file_name']}\")\n",
    "    print(f\"âœ… Type: {result['qa_extraction']['document_type']}\")\n",
    "    print(f\"âœ… AI Extractions: {result['qa_extraction']['successful_extractions']}\")\n",
    "    \n",
    "    # Show results using system\n",
    "    for question, answer in result['high_confidence_extractions']['high_confidence_extractions'].items():\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {answer['answer']} (confidence: {answer['confidence']:.3f})\")\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"Add your image path to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca90862",
   "metadata": {},
   "source": [
    "**3. Self-Learning and Auto Fine-Tuning**\n",
    "* Build a feedback loop that captures user corrections, validation mismatches, or annotation logs.\n",
    "* Implement automatic model fine-tuning or retraining using this feedback without manual intervention.\n",
    "* Ensure continuous performance improvement while avoiding overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedback Collection\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Path to store feedback logs\n",
    "feedback_file = Path(\"feedback_log.csv\")\n",
    "\n",
    "# Create file with headers if it doesn't exist\n",
    "\n",
    "if not feedback_file.exists():\n",
    "    with open(feedback_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"timestamp\", \"question\", \"context\", \"predicted_answer\", \"correct_answer\"])\n",
    "\n",
    "# Captures human feedback for reinforcement\n",
    "def log_feedback(question, context, predicted_answer, correct_answer=None):\n",
    "    \"\"\"\n",
    "    Logs model output and user-provided corrections to a CSV file.\n",
    "    \"\"\"\n",
    "    with open(feedback_file, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            datetime.now().isoformat(),\n",
    "            question,\n",
    "            context,\n",
    "            predicted_answer,\n",
    "            correct_answer if correct_answer else \"\"\n",
    "        ])\n",
    "\n",
    "# Example usage after prediction step:\n",
    "\n",
    "question = \"What is the capital of Germany?\"\n",
    "context = \"Berlin is the capital and largest city of Germany.\"\n",
    "predicted_answer = \"Berlin\"\n",
    "\n",
    "# Suppose user confirms or corrects:\n",
    "\n",
    "if INTERACTIVE:\n",
    "    correct_answer = input(f\"Predicted answer: {predicted_answer}\\nIf wrong, type correct answer (or press Enter to confirm): \")\n",
    "else:\n",
    "    print(\"INTERACTIVE=False: auto-confirming predicted answer\")\n",
    "    correct_answer = predicted_answer\n",
    "\n",
    "print(\"Feedback logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd6f35",
   "metadata": {},
   "source": [
    "ðŸ”¹feedback_log.csv is created the first time you run it. After each prediction, the system:\n",
    "   Shows the predicted answer.\n",
    "   Lets the user correct it or press Enter to confirm.\n",
    "\n",
    "Appends a new row with timestamp, Q, context, prediction, and correction.\n",
    "\n",
    "ðŸ”¹This creates your personal dataset for fine-tuning.\n",
    " It ensures the model improves based on your domain-specific corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309852af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Feedback for Fine-Tuning\n",
    "\n",
    "# Load feedback\n",
    "feedback_file = \"feedback_log.csv\"\n",
    "\n",
    "# Prepare SQuAD-style dataset(current model deepset/xlm-roberta-large-squad2 uses this format)\n",
    "data = {\n",
    "    \"version\": \"v2.0\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"feedback_data\",\n",
    "            \"paragraphs\": []\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "import hashlib \n",
    "with open(feedback_file, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        context = row[\"context\"]\n",
    "        question = row[\"question\"]\n",
    "        answer_text = row[\"correct_answer\"] or row[\"predicted_answer\"]\n",
    "\n",
    "        # Find start index of answer in context (required for SQuAD format)\n",
    "        start_index = context.find(answer_text)\n",
    "        if start_index == -1:\n",
    "            continue  # skip if answer not found in context\n",
    "\n",
    "        # Force string types and stable ID\n",
    "        safe_q = str(question)\n",
    "        safe_ctx = str(context)\n",
    "        qid = hashlib.sha1((safe_q + \"||\" + safe_ctx).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        paragraph = {\n",
    "            \"context\": safe_ctx,\n",
    "            \"qas\": [\n",
    "                {\n",
    "                    \"id\": qid,\n",
    "                    \"question\": safe_q,\n",
    "                    \"answers\": [\n",
    "                        {\n",
    "                            \"text\": answer_text,\n",
    "                            \"answer_start\": start_index\n",
    "                        }\n",
    "                    ],\n",
    "                    \"is_impossible\": False\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        data[\"data\"][0][\"paragraphs\"].append(paragraph)\n",
    "\n",
    "# Save dataset\n",
    "with open(\"feedback_dataset.json\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(data, out_file, ensure_ascii=False, indent=2)\n",
    "print(\"feedback_dataset.json created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b74aa",
   "metadata": {},
   "source": [
    "* Read feedback_log.csv ( manually collected corrections).\n",
    "\n",
    "* Convert it to SQuAD-style JSON : required for most extractive QA fine-tuning (current model deepset/xlm-roberta-large-squad2 uses this format).\n",
    "\n",
    "* Save it as feedback_dataset.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be651aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the feedback_dataset.json into a Hugging Face Dataset object.\n",
    "\n",
    "if os.path.exists(\"feedback_dataset.json\"):\n",
    "    with open(\"feedback_dataset.json\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"feedback_dataset.json loaded: {len(data['data'][0]['paragraphs'])} paragraphs\")\n",
    "else:\n",
    "    print(\"feedback_dataset.json not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ad8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal fine-tuning path without `datasets`/`pyarrow`/`spacy`\n",
    "\n",
    "\n",
    "import os, json, torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    TrainingArguments, Trainer, DefaultDataCollator\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load your SQuAD-style feedback JSON\n",
    "\n",
    "with open(\"feedback_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "examples = []\n",
    "for article in data[\"data\"]:\n",
    "    for para in article[\"paragraphs\"]:\n",
    "        ctx = para[\"context\"]\n",
    "        for qa in para[\"qas\"]:\n",
    "            ans = qa[\"answers\"][0] if qa.get(\"answers\") else {\"text\": \"\", \"answer_start\": 0}\n",
    "            examples.append({\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"context\": ctx,\n",
    "                \"answer_text\": ans.get(\"text\", \"\"),\n",
    "                \"answer_start\": ans.get(\"answer_start\", 0),\n",
    "            })\n",
    "\n",
    "def make_features(ex):\n",
    "    tok = tokenizer(\n",
    "        ex[\"question\"],\n",
    "        ex[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    offsets = tok.pop(\"offset_mapping\")\n",
    "    seq_ids = tok.sequence_ids()\n",
    "\n",
    "    # Context token span\n",
    "    start_ctx = 0\n",
    "    while start_ctx < len(seq_ids) and seq_ids[start_ctx] != 1:\n",
    "        start_ctx += 1\n",
    "    end_ctx = len(seq_ids) - 1\n",
    "    while end_ctx >= 0 and seq_ids[end_ctx] != 1:\n",
    "        end_ctx -= 1\n",
    "\n",
    "    start_char = ex[\"answer_start\"]\n",
    "    end_char = start_char + len(ex[\"answer_text\"])\n",
    "\n",
    "    if ex[\"answer_text\"] == \"\":\n",
    "        start_pos = end_pos = start_ctx\n",
    "    elif not (offsets[start_ctx][0] <= start_char and offsets[end_ctx][1] >= end_char):\n",
    "        start_pos = end_pos = start_ctx\n",
    "    else:\n",
    "        s = start_ctx\n",
    "        while s < len(offsets) and offsets[s][0] <= start_char:\n",
    "            s += 1\n",
    "        start_pos = s - 1\n",
    "        e = end_ctx\n",
    "        while e > 0 and offsets[e][1] >= end_char:\n",
    "            e -= 1\n",
    "        end_pos = e + 1\n",
    "\n",
    "    tok[\"start_positions\"] = start_pos\n",
    "    tok[\"end_positions\"] = end_pos\n",
    "    return tok\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.features = [make_features(ex) for ex in examples]\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, i):\n",
    "        return {k: torch.tensor(v) for k, v in self.features[i].items()}\n",
    "\n",
    "train_dataset = QADataset(examples)\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    report_to=None,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DefaultDataCollator(),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"Model saved to './fine_tuned_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d387ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell creates an automated system that can fine-tune AI model \n",
    "#based on user feedback without manual intervention.\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    TrainingArguments, Trainer, DefaultDataCollator\n",
    ")\n",
    "\n",
    "#1. File Management\n",
    "\n",
    "STATE_FILE = Path(\".self_tune_state.json\") # Tracks training history\n",
    "FEEDBACK_CSV = Path(\"feedback_log.csv\")  # User corrections\n",
    "FEEDBACK_JSON = Path(\"feedback_dataset.json\") # Training-ready format\n",
    "OUTPUT_DIR = Path(\"./fine_tuned_model\") # Where improved model is saved\n",
    "\n",
    "BASE_MODEL = \"deepset/xlm-roberta-large-squad2\"\n",
    "\n",
    "\n",
    "#2. State Tracking Functions\n",
    "\n",
    "def _load_state(): # + save?state Remember how many feedback rows were used for training\n",
    "    if STATE_FILE.exists():\n",
    "        return json.loads(STATE_FILE.read_text(encoding=\"utf-8\"))\n",
    "    return {\"last_trained_count\": 0, \"last_trained_at\": 0}\n",
    "\n",
    "def _save_state(state):\n",
    "    STATE_FILE.write_text(json.dumps(state, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _feedback_count():   # Count new user corrections since last training\n",
    "    if not FEEDBACK_CSV.exists():\n",
    "        return 0\n",
    "    with FEEDBACK_CSV.open(newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = list(reader)\n",
    "        return max(0, len(rows) - 1)  # minus header\n",
    "\n",
    "#3. Data Conversion\n",
    "\n",
    "def _build_squad_from_csv(csv_path=FEEDBACK_CSV, out_json=FEEDBACK_JSON): #cnverts feedbacks CSV into SQuAD format (the training format AI models expect)\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(\"feedback_log.csv not found\")\n",
    "    import hashlib \n",
    "    data = {\"version\": \"v2.0\", \"data\": [{\"title\": \"feedback_data\", \"paragraphs\": []}]}\n",
    "    with csv_path.open(newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            ctx = row[\"context\"]\n",
    "            q = row[\"question\"]\n",
    "            ans_text = row[\"correct_answer\"] or row[\"predicted_answer\"]\n",
    "            start = ctx.find(ans_text)\n",
    "            if start == -1:\n",
    "                continue\n",
    "            safe_q = str(q)\n",
    "            safe_ctx = str(ctx)\n",
    "            qid = hashlib.sha1((safe_q + \"||\" + safe_ctx).encode(\"utf-8\")).hexdigest()\n",
    "            data[\"data\"][0][\"paragraphs\"].append({\n",
    "                \"context\": safe_ctx,\n",
    "                \"qas\": [{\n",
    "                    \"id\": qid,\n",
    "                    \"question\": safe_q,\n",
    "                    \"answers\": [{\"text\": ans_text, \"answer_start\": start}],\n",
    "                    \"is_impossible\": False\n",
    "                }]\n",
    "            })\n",
    "    out_json.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return out_json\n",
    "\n",
    "\n",
    "def _load_examples(feedback_json=FEEDBACK_JSON):\n",
    "    d = json.loads(Path(feedback_json).read_text(encoding=\"utf-8\"))\n",
    "    examples = []\n",
    "    for article in d[\"data\"]:\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            ctx = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                ans = qa[\"answers\"][0] if qa.get(\"answers\") else {\"text\": \"\", \"answer_start\": 0}\n",
    "                examples.append({\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"context\": ctx,\n",
    "                    \"answer_text\": ans.get(\"text\", \"\"),\n",
    "                    \"answer_start\": ans.get(\"answer_start\", 0),\n",
    "                })\n",
    "    return examples\n",
    "\n",
    "#4. Training Pipeline\n",
    "\"\"\"Prepares data for the AI model\n",
    "Tokenizes questions and contexts\n",
    "Maps answer positions in text\n",
    "Creates PyTorch dataset\"\"\"\n",
    "\n",
    "\"\"\"train_from_feedback(): The actual fine-tuning\n",
    "\n",
    "Loads your feedback data\n",
    "Creates a new model based on your corrections\n",
    "Saves improved model to ./fine_tuned_model\"\"\"\n",
    "\n",
    "def _features_maker(tokenizer): \n",
    "    def make_features(ex):\n",
    "        tok = tokenizer(\n",
    "            ex[\"question\"], ex[\"context\"],\n",
    "            truncation=\"only_second\", max_length=384, stride=128,\n",
    "            return_offsets_mapping=True, padding=\"max_length\",\n",
    "        )\n",
    "        offsets = tok.pop(\"offset_mapping\")\n",
    "        seq_ids = tok.sequence_ids()\n",
    "        start_ctx = 0\n",
    "        while start_ctx < len(seq_ids) and seq_ids[start_ctx] != 1:\n",
    "            start_ctx += 1\n",
    "        end_ctx = len(seq_ids) - 1\n",
    "        while end_ctx >= 0 and seq_ids[end_ctx] != 1:\n",
    "            end_ctx -= 1\n",
    "        start_char = ex[\"answer_start\"]\n",
    "        end_char = start_char + len(ex[\"answer_text\"])\n",
    "        if ex[\"answer_text\"] == \"\":\n",
    "            start_pos = end_pos = start_ctx\n",
    "        elif not (offsets[start_ctx][0] <= start_char and offsets[end_ctx][1] >= end_char):\n",
    "            start_pos = end_pos = start_ctx\n",
    "        else:\n",
    "            s = start_ctx\n",
    "            while s < len(offsets) and offsets[s][0] <= start_char:\n",
    "                s += 1\n",
    "            start_pos = s - 1\n",
    "            e = end_ctx\n",
    "            while e > 0 and offsets[e][1] >= end_char:\n",
    "                e -= 1\n",
    "            end_pos = e + 1\n",
    "        tok[\"start_positions\"] = start_pos\n",
    "        tok[\"end_positions\"] = end_pos\n",
    "        return tok\n",
    "    return make_features\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer):\n",
    "        maker = _features_maker(tokenizer)\n",
    "        self.features = [maker(ex) for ex in examples]\n",
    "    def __len__(self): return len(self.features)\n",
    "    def __getitem__(self, i): return {k: torch.tensor(v) for k, v in self.features[i].items()}\n",
    "\n",
    "def train_from_feedback(base_model=None, output_dir=OUTPUT_DIR, epochs=3, lr=2e-5, batch_size=2):\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    base = base_model or (str(output_dir) if (output_dir / \"config.json\").exists() else BASE_MODEL)\n",
    "    _build_squad_from_csv(FEEDBACK_CSV, FEEDBACK_JSON)\n",
    "    examples = _load_examples(FEEDBACK_JSON)\n",
    "    if not examples:\n",
    "        print(\"No examples to train.\")\n",
    "        return False\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base)\n",
    "    dataset = QADataset(examples, tokenizer)\n",
    "    print(f\"Train samples: {len(dataset)} (base={base})\")\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(base)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        evaluation_strategy=\"no\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "        report_to=None,\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=DefaultDataCollator(),\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(str(output_dir))\n",
    "    tokenizer.save_pretrained(str(output_dir))\n",
    "    print(f\"Saved to {output_dir}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# 5. Auto-Trigger automatically retrains when enough feedback collected actuaÅŸl \"self-training\" part\n",
    "\n",
    "\n",
    "\"\"\"maybe_finetune(): Smart automation\n",
    "\n",
    "Checks if you have enough new feedback (default: 5 corrections)\n",
    "Automatically starts training when threshold is met\n",
    "Updates tracking so it doesn't retrain on same data\"\"\"\n",
    "\n",
    "def maybe_finetune(threshold_new_rows=5):\n",
    "    state = _load_state()\n",
    "    total = _feedback_count()\n",
    "    new = total - state.get(\"last_trained_count\", 0)\n",
    "    print(f\"Feedback rows: {total} (new since last train: {new})\")\n",
    "    if new >= threshold_new_rows:\n",
    "        ok = train_from_feedback()\n",
    "        if ok:\n",
    "            state[\"last_trained_count\"] = total\n",
    "            state[\"last_trained_at\"] = int(time.time())\n",
    "            _save_state(state)\n",
    "            print(\"Fine-tuned and state updated.\")\n",
    "    else:\n",
    "        print(f\"Not enough new feedback yet (need {threshold_new_rows}).\")\n",
    "        \n",
    "        \"\"\"How It Works in Practice:\n",
    "You use the system â†’ AI makes predictions\n",
    "You correct wrong answers â†’ Logged to feedback_log.csv\n",
    "Background check â†’ maybe_finetune() counts new corrections\n",
    "Auto-training â†’ When you have 5+ new corrections, it automatically fine-tunes\n",
    "Improved model â†’ Next time you use the system, it's smarter\n",
    "Why This Is Powerful:\n",
    "Zero manual work: No need to manually retrain\n",
    "Continuous improvement: Gets better with each correction you make\n",
    "Domain-specific: Learns YOUR specific documents and terminology\n",
    "Memory: Never forgets previous corrections\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aea508",
   "metadata": {},
   "source": [
    "**4.\tModel Evaluation and Versioning**\n",
    "* Define evaluation metrics for accuracy, confidence, and error detection.\n",
    "* Track model versions and performance history, and allow safe promotion of improved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Versioning System\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, metrics_file=\"model_metrics.json\"):\n",
    "        # Creates a tracker that saves results to a JSON file\n",
    "        self.metrics_file = Path(metrics_file)\n",
    "        self.metrics_history = self._load_metrics() # Loads previous results\n",
    "        \n",
    "    def _load_metrics(self):\n",
    "        if self.metrics_file.exists():\n",
    "            return json.loads(self.metrics_file.read_text())\n",
    "        return {\"evaluations\": []}\n",
    "    \n",
    "    def _save_metrics(self):\n",
    "        self.metrics_file.write_text(json.dumps(self.metrics_history, indent=2))\n",
    "    \n",
    "    def create_ground_truth_template(self, invoice_files): # invoice_files is a LIST of paths\n",
    "        \"\"\"Create template for manual ground truth annotation\"\"\" \n",
    "        template = {}\n",
    "        for file_path in invoice_files:\n",
    "            filename = Path(file_path).name  # Extract just filename (eg \"invoice1.pdf\")\n",
    "            template[filename] = {\n",
    "                \"invoice_number\": \"\",\n",
    "                \"total_amount\": \"\",\n",
    "                \"company_name\": \"\",\n",
    "                \"invoice_date\": \"\",\n",
    "                \"tax_amount\": \"\",\n",
    "                \"net_amount\": \"\",\n",
    "                \"customer_name\": \"\",\n",
    "                \"notes\": \"Manual annotation needed\"\n",
    "            }\n",
    "        \n",
    "        # Save template\n",
    "        with open(\"ground_truth_template.json\", \"w\") as f:\n",
    "            json.dump(template, f, indent=2)\n",
    "        \n",
    "        print(\"Ground truth template created: ground_truth_template.json\")\n",
    "        print(\"Please fill in the correct values manually\")\n",
    "        return template\n",
    "    \n",
    "    def evaluate_extraction_results(self, results_dict, ground_truth=None, document_name=\"\"):\n",
    "        \"\"\"Evaluate extraction quality with multiple metrics\"\"\"\n",
    "        evaluation = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"document_name\": document_name,\n",
    "            \"model_version\": results_dict.get('qa_extraction', {}).get('model_used', 'unknown'),\n",
    "            \"document_type\": results_dict.get('qa_extraction', {}).get('document_type', 'unknown'),\n",
    "            \"total_questions\": results_dict.get('qa_extraction', {}).get('total_questions', 0),\n",
    "            \"successful_extractions\": results_dict.get('qa_extraction', {}).get('successful_extractions', 0),\n",
    "            \"success_rate\": results_dict.get('qa_extraction', {}).get('success_rate', 0),\n",
    "            \"avg_confidence\": self._calculate_avg_confidence(results_dict),\n",
    "            \"high_confidence_count\": len(results_dict.get('high_confidence_extractions', {}).get('high_confidence_extractions', {}))\n",
    "        } \n",
    "        \n",
    "        # Add ground truth comparison if available\n",
    "        if ground_truth:\n",
    "            evaluation.update(self._compare_with_ground_truth(results_dict, ground_truth))\n",
    "        \n",
    "        self.metrics_history[\"evaluations\"].append(evaluation)\n",
    "        self._save_metrics()\n",
    "        return evaluation\n",
    "    \n",
    "    def _calculate_avg_confidence(self, results_dict):\n",
    "        extractions = results_dict.get('qa_extraction', {}).get('extractions', {})\n",
    "        confidences = [result.get('confidence', 0) for result in extractions.values() \n",
    "                      if result.get('confidence') is not None]\n",
    "        return sum(confidences) / len(confidences) if confidences else 0\n",
    "    \n",
    "    def _compare_with_ground_truth(self, results_dict, ground_truth):\n",
    "        \"\"\"Compare extractions with ground truth for accuracy metrics\"\"\"\n",
    "        extractions = results_dict.get('qa_extraction', {}).get('extractions', {})\n",
    "        \n",
    "        # Key extraction mapping\n",
    "        key_mappings = {\n",
    "            \"invoice_number\": [\"rechnungsnummer\", \"invoice number\", \"nummer\"],\n",
    "            \"total_amount\": [\"gesamtbetrag\", \"total\", \"endbetrag\", \"amount\"],\n",
    "            \"company_name\": [\"company\", \"firma\", \"firmenname\", \"vendor\"],\n",
    "            \"invoice_date\": [\"datum\", \"date\", \"rechnungsdatum\"],\n",
    "            \"tax_amount\": [\"mwst\", \"ust\", \"tax\", \"mehrwertsteuer\"],\n",
    "            \"net_amount\": [\"netto\", \"net\", \"nettobetrag\"]\n",
    "        }\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        detailed_results = {}\n",
    "        \n",
    "        for gt_key, gt_value in ground_truth.items():\n",
    "            if not gt_value:  # Skip empty ground truth values\n",
    "                continue\n",
    "                \n",
    "            best_match = None\n",
    "            best_similarity = 0\n",
    "            \n",
    "            # Find best matching extraction\n",
    "            for question, result in extractions.items():\n",
    "                if result.get('answer') and any(keyword in question.lower() for keyword in key_mappings.get(gt_key, [])):\n",
    "                    similarity = self._answers_match_score(result['answer'], gt_value)\n",
    "                    if similarity > best_similarity:\n",
    "                        best_similarity = similarity\n",
    "                        best_match = result['answer']\n",
    "            \n",
    "            total += 1\n",
    "            is_correct = best_similarity >= 0.8\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            \n",
    "            detailed_results[gt_key] = {\n",
    "                \"ground_truth\": gt_value,\n",
    "                \"predicted\": best_match,\n",
    "                \"similarity\": best_similarity,\n",
    "                \"correct\": is_correct\n",
    "            }\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        return {\n",
    "            \"ground_truth_accuracy\": accuracy,\n",
    "            \"correct_answers\": correct,\n",
    "            \"total_compared\": total,\n",
    "            \"detailed_results\": detailed_results\n",
    "        }\n",
    "    \n",
    "    def _answers_match_score(self, predicted, actual):\n",
    "        \"\"\"Calculate similarity score between predicted and actual answers\"\"\"\n",
    "        if not predicted or not actual:\n",
    "            return 0\n",
    "        \n",
    "        # Clean and normalize\n",
    "        pred_clean = str(predicted).lower().strip()\n",
    "        actual_clean = str(actual).lower().strip()\n",
    "        \n",
    "        # Exact match\n",
    "        if pred_clean == actual_clean:\n",
    "            return 1.0\n",
    "        \n",
    "        # Fuzzy matching\n",
    "        similarity = SequenceMatcher(None, pred_clean, actual_clean).ratio()\n",
    "        \n",
    "        # Bonus for number matching (important for invoices)\n",
    "    \n",
    "        pred_numbers = re.findall(r'\\d+[\\.,]?\\d*', pred_clean)\n",
    "        actual_numbers = re.findall(r'\\d+[\\.,]?\\d*', actual_clean)\n",
    "        \n",
    "        if pred_numbers and actual_numbers:\n",
    "            # Normalize numbers for comparison\n",
    "            pred_num = pred_numbers[0].replace(',', '.')\n",
    "            actual_num = actual_numbers[0].replace(',', '.')\n",
    "            try:\n",
    "                if float(pred_num) == float(actual_num):\n",
    "                    similarity = max(similarity, 0.9)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def batch_evaluate_invoices(self, invoice_folder, ground_truth_file=None):\n",
    "        \"\"\" Runs your AI on multiple invoices to generates performance report.\"\"\"\n",
    "        invoice_folder = Path(invoice_folder)\n",
    "        results = []\n",
    "        \n",
    "        # Load ground truth\n",
    "        ground_truth_data = {}\n",
    "        if ground_truth_file and Path(ground_truth_file).exists():\n",
    "            with open(ground_truth_file) as f:\n",
    "                ground_truth_data = json.load(f)\n",
    "        \n",
    "        # Process each file type separately to fix the glob() error\n",
    "        supported_patterns = [\"*.pdf\", \"*.png\", \"*.jpg\", \"*.jpeg\", \"*.tiff\", \"*.bmp\"]\n",
    "        \n",
    "        for pattern in supported_patterns:\n",
    "            for invoice_file in invoice_folder.glob(pattern):  # ONE pattern at a time\n",
    "                print(f\"Processing: {invoice_file.name}\")\n",
    "                \n",
    "                try:\n",
    "                    # Extract using your existing pipeline\n",
    "                    result = enhanced_load_and_normalize_with_qa(str(invoice_file))\n",
    "                    \n",
    "                    # Get ground truth for this file\n",
    "                    gt = ground_truth_data.get(invoice_file.name, {})\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    evaluation = self.evaluate_extraction_results(result, gt, invoice_file.name)\n",
    "                    results.append(evaluation)\n",
    "                    \n",
    "                    print(f\"âœ… Success rate: {evaluation['success_rate']:.1%}\")\n",
    "                    if gt:\n",
    "                        print(f\"âœ… Accuracy: {evaluation.get('ground_truth_accuracy', 0):.1%}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error processing {invoice_file.name}: {e}\")\n",
    "        \n",
    "        # Generate summary report\n",
    "        if results:\n",
    "            self._generate_evaluation_report(results)\n",
    "        else:\n",
    "            print(\"âŒ No files were processed successfully!\")\n",
    "            print(\" Check if files exist in:\", str(invoice_folder.absolute()))\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _generate_evaluation_report(self, results):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        if not results:\n",
    "            print(\"No results to report\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "=== INVOICE EXTRACTION EVALUATION REPORT ===\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "    OVERALL PERFORMANCE:\n",
    "â€¢ Total invoices processed: {len(results)}\n",
    "â€¢ Average success rate: {df['success_rate'].mean():.1%}\n",
    "â€¢ Average confidence: {df['avg_confidence'].mean():.3f}\n",
    "â€¢ High confidence extractions: {df['high_confidence_count'].mean():.1f}/question\n",
    "\n",
    "    ACCURACY METRICS:\n",
    "\"\"\"\n",
    "        \n",
    "        if 'ground_truth_accuracy' in df.columns:\n",
    "            accuracy_data = df.dropna(subset=['ground_truth_accuracy'])\n",
    "            if not accuracy_data.empty:\n",
    "                report += f\"\"\"â€¢ Ground truth accuracy: {accuracy_data['ground_truth_accuracy'].mean():.1%}\n",
    "â€¢ Correct answers: {accuracy_data['correct_answers'].sum()}/{accuracy_data['total_compared'].sum()}\n",
    "â€¢ Best performing invoice: {accuracy_data.loc[accuracy_data['ground_truth_accuracy'].idxmax(), 'document_name']}\n",
    "â€¢ Worst performing invoice: {accuracy_data.loc[accuracy_data['ground_truth_accuracy'].idxmin(), 'document_name']}\n",
    "\"\"\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    " PERFORMANCE DISTRIBUTION:\n",
    "â€¢ Success rate std dev: {df['success_rate'].std():.3f}\n",
    "â€¢ Confidence std dev: {df['avg_confidence'].std():.3f}\n",
    "\n",
    "    RECOMMENDATIONS:\n",
    "\"\"\"\n",
    "        \n",
    "        avg_success = df['success_rate'].mean()\n",
    "        if avg_success < 0.7:\n",
    "            report += \"â€¢ Consider expanding question templates\\n\"\n",
    "        if avg_success > 0.8:\n",
    "            report += \"â€¢ System performing well - ready for production\\n\"\n",
    "        \n",
    "        avg_confidence = df['avg_confidence'].mean()\n",
    "        if avg_confidence < 0.5:\n",
    "            report += \"â€¢ Low confidence scores - may need more training data\\n\"\n",
    "        \n",
    "        print(report)\n",
    "        \n",
    "        # Save report\n",
    "        with open(f\"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\", \"w\") as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Model Version Manager :  Tracks different model versions and safely promotes improvements\n",
    "class ModelVersionManager:\n",
    "    def __init__(self, versions_file=\"model_versions.json\"):\n",
    "        self.versions_file = Path(versions_file)\n",
    "        self.versions = self._load_versions()\n",
    "    \n",
    "    def _load_versions(self):\n",
    "        if self.versions_file.exists():\n",
    "            return json.loads(self.versions_file.read_text())\n",
    "        return {\"versions\": [], \"current_version\": None}\n",
    "    \n",
    "    def _save_versions(self):\n",
    "        self.versions_file.write_text(json.dumps(self.versions, indent=2))\n",
    "    \n",
    "    def register_new_version(self, model_path, performance_metrics, description=\"\"):\n",
    "        \"\"\"Registers each new fine-tuned model with its performance scores\"\"\"\n",
    "        version_info = {\n",
    "            \"version_id\": f\"v{len(self.versions['versions']) + 1}\", #v1,v2,v3\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_path\": str(model_path),  # ./fine_tuned_model\n",
    "            \"performance\": performance_metrics,  # Success rates, accuracy\n",
    "            \"description\": description,\n",
    "            \"is_active\": False       # Not active yet\n",
    "        }\n",
    "        \n",
    "        self.versions[\"versions\"].append(version_info)\n",
    "        self._save_versions()\n",
    "        return version_info[\"version_id\"]\n",
    "    \n",
    "    def promote_version(self, version_id, min_success_rate=0.7, min_accuracy=0.6):\n",
    "        \"\"\" Safety checks before making model active,\n",
    "        Only promotes models that perform better than thresholds (prevents regression)\"\"\"\n",
    "        version = self._find_version(version_id)\n",
    "        if not version:\n",
    "            return False, \"Version not found\"\n",
    "        \n",
    "        # Check performance criteria\n",
    "        success_rate = version[\"performance\"].get(\"avg_success_rate\", 0)\n",
    "        accuracy = version[\"performance\"].get(\"avg_accuracy\", 0)\n",
    "        \n",
    "        if success_rate < min_success_rate:\n",
    "            return False, f\"Success rate {success_rate:.2%} below threshold {min_success_rate:.2%}\"\n",
    "        \n",
    "        if accuracy > 0 and accuracy < min_accuracy:\n",
    "            return False, f\"Accuracy {accuracy:.2%} below threshold {min_accuracy:.2%}\"\n",
    "        \n",
    "        # Deactivate current version\n",
    "        for v in self.versions[\"versions\"]:\n",
    "            v[\"is_active\"] = False\n",
    "        \n",
    "        # Activate new version\n",
    "        version[\"is_active\"] = True\n",
    "        self.versions[\"current_version\"] = version_id\n",
    "        self._save_versions()\n",
    "        \n",
    "        return True, f\"Version {version_id} promoted successfully\"\n",
    "    \n",
    "    def _find_version(self, version_id):\n",
    "        for version in self.versions[\"versions\"]:\n",
    "            if version[\"version_id\"] == version_id:\n",
    "                return version\n",
    "        return None\n",
    "    \n",
    "    \"\"\"Research Paper Benefits:\n",
    "Quantitative results showing improvement over time\n",
    "Comparison metrics before/after self-learning\n",
    "Performance graphs demonstrating ReST effectiveness\n",
    "Statistical validation of your approach\"\"\"\n",
    "\n",
    "# Model Evaluation and Versioning System ready after this next 10-20 invoices for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4A: Large-Scale Dataset Setup for 1000+ Invoice Evaluation\n",
    "\n",
    "\n",
    "def setup_large_scale_invoice_dataset():\n",
    "    \"\"\"\n",
    "    Dataset setup for large-scale evaluation\n",
    "    Automatically discovers and organizes invoices from multiple source folders\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration for dataset sources\n",
    "    DATASET_SOURCES = [\n",
    "        \n",
    "        # downloaded dataset\n",
    "        r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\new invoices dataset\",\n",
    "\n",
    "        # Original test files\n",
    "        r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\",\n",
    "        \n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Supported file extensions\n",
    "    SUPPORTED_EXTENSIONS = ['.pdf', '.png', '.jpg', '.jpeg', '.tiff', '.bmp']\n",
    "\n",
    "    \n",
    "    # Create main evaluation directory\n",
    "    EVAL_DIR = Path(\"./large_scale_invoice_dataset\")\n",
    "    EVAL_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {'total_copied': 0, 'errors': []}\n",
    "    \n",
    "    # Scan all source directories\n",
    "    for source_dir in DATASET_SOURCES:\n",
    "        source_path = Path(source_dir)\n",
    "        \n",
    "        if not source_path.exists():\n",
    "            print(f\" Source not found: {source_dir}\")\n",
    "            stats['errors'].append(f\"Directory not found: {source_dir}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n Scanning: {source_dir}\")\n",
    "        folder_stats = {'pdf': 0, 'image': 0, 'other': 0, 'total': 0}\n",
    "        \n",
    "        # Scan all source directories\n",
    "    for source_dir in DATASET_SOURCES:\n",
    "        source_path = Path(source_dir)\n",
    "        \n",
    "        if not source_path.exists():\n",
    "            print(f\" Source not found: {source_dir}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\" Processing: {source_path.name}\")\n",
    "        folder_count = 0\n",
    "        \n",
    "        # Find and copy all supported files\n",
    "        for ext in SUPPORTED_EXTENSIONS:\n",
    "            pattern = f\"**/*{ext}\"\n",
    "            for file_path in source_path.glob(pattern):\n",
    "                if file_path.is_file():\n",
    "                    # Create unique filename\n",
    "                    counter = 1\n",
    "                    dest_name = file_path.name\n",
    "                    dest_path = EVAL_DIR / dest_name\n",
    "                    \n",
    "                    while dest_path.exists():\n",
    "                        stem = file_path.stem\n",
    "                        suffix = file_path.suffix\n",
    "                        dest_name = f\"{stem}_{counter}{suffix}\"\n",
    "                        dest_path = EVAL_DIR / dest_name\n",
    "                        counter += 1\n",
    "                    \n",
    "                    try:\n",
    "                        shutil.copy2(file_path, dest_path)\n",
    "                        stats['total_copied'] += 1\n",
    "                        folder_count += 1\n",
    "                        \n",
    "                        # Progress indicator\n",
    "                        if stats['total_copied'] % 100 == 0:\n",
    "                            print(f\" Copied {stats['total_copied']} files...\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        stats['errors'].append(f\"Failed: {file_path.name}\")\n",
    "        \n",
    "        print(f\" Copied {folder_count} files from this folder\")\n",
    "        \n",
    "# Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\" DATASET SETUP COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\" Total invoices ready: {stats['total_copied']}\")\n",
    "    print(f\" Location: {EVAL_DIR.absolute()}\")\n",
    "    print(f\" Errors: {len(stats['errors'])}\")\n",
    "    if stats['total_copied'] >= 1000:\n",
    "        print(\"âœ… 1000+ REQUIREMENT MET - READY FOR LARGE-SCALE TESTING!\")\n",
    "    else:\n",
    "        print(f\" Only {stats['total_copied']} files found - may need more datasets\")\n",
    "        \n",
    "    # Save simple report\n",
    "    with open(\"dataset_summary.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            'total_files': stats['total_copied'],\n",
    "            'setup_completed': datetime.now().isoformat(),\n",
    "            'dataset_location': str(EVAL_DIR.absolute()),\n",
    "            'ready_for_evaluation': stats['total_copied'] >= 1000\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Execute the setup\n",
    "print(\" STARTING LARGE-SCALE DATASET SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "setup_stats = setup_large_scale_invoice_dataset()\n",
    "\n",
    "if setup_stats['total_copied'] > 0:\n",
    "    # Create evaluator for large-scale testing\n",
    "    evaluator = ModelEvaluator()\n",
    "    print(f\"\\nâœ… Ready for large-scale evaluation with {setup_stats['total_copied']} documents\")\n",
    "    print(\" Files location: ./large_scale_invoice_dataset/\")\n",
    "    print(\" Next: Run your evaluation system on this dataset\")\n",
    "else:\n",
    "    print(\"\\n No files copied - please check your source paths\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4B: Smart Automated Batch Evaluation\n",
    "\n",
    "class SmartAutomatedEvaluator:\n",
    "    \"\"\"Combines smart validation with full automation for zero human intervention\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluator = ModelEvaluator()\n",
    "        self.confidence_patterns = {\n",
    "            \"invoice_number\": [r\"[A-Z0-9\\-]{5,15}\", r\"\\d{6,12}\"],\n",
    "            \"total_amount\": [r\"[\\d,\\.]+\\s*â‚¬\", r\"â‚¬\\s*[\\d,\\.]+\", r\"\\d+[,\\.]\\d{2}\"],\n",
    "            \"company_name\": [r\"[A-Z][a-zA-Z\\s&,\\.]{3,30}\"],\n",
    "            \"invoice_date\": [r\"\\d{1,2}[./\\-]\\d{1,2}[./\\-]\\d{2,4}\"]\n",
    "        }\n",
    "    \n",
    "    def create_smart_ground_truth(self, invoice_folder=\"./large_scale_invoice_dataset\"):\n",
    "        \"\"\"Creates validated ground truth automatically\"\"\"\n",
    "        \n",
    "        smart_gt = {}\n",
    "        validation_scores = {}\n",
    "        \n",
    "        # Process each invoice with multiple validation layers\n",
    "        for pattern in [\"*.pdf\", \"*.png\", \"*.jpg\", \"*.jpeg\"]:\n",
    "            for invoice_file in Path(invoice_folder).glob(pattern):\n",
    "                try:\n",
    "                    print(f\" Processing: {invoice_file.name}\")\n",
    "                    \n",
    "                    # Extract with existing system\n",
    "                    result = enhanced_load_and_normalize_with_qa(str(invoice_file))\n",
    "                    extractions = result['qa_extraction']['extractions']\n",
    "                    \n",
    "                    # Smart validation for each extraction\n",
    "                    validated_data = {}\n",
    "                    file_validation_score = 0\n",
    "                    total_validations = 0\n",
    "                    \n",
    "                    for question, answer_data in extractions.items():\n",
    "                        confidence = answer_data.get('confidence', 0)\n",
    "                        answer = answer_data.get('answer', '')\n",
    "                        \n",
    "                        # null checking\n",
    "                        if answer is None:\n",
    "                            answer = ''\n",
    "                        else:\n",
    "                            answer = str(answer).strip()\n",
    "                        \n",
    "                        if not answer or len(answer) < 2:\n",
    "                            continue\n",
    "                        \n",
    "                        # Multi-criteria validation\n",
    "                        validation_score = 0\n",
    "                        field = self._map_question_to_field(question)\n",
    "                        \n",
    "                        # Confidence threshold\n",
    "                        if confidence >= 0.3:  # Lowered threshold for more data\n",
    "                            validation_score += 0.3\n",
    "                        \n",
    "                        # Pattern matching\n",
    "                        if field and self._validate_pattern(field, answer):\n",
    "                            validation_score += 0.4\n",
    "                        \n",
    "                        # Length and format check\n",
    "                        if 2 <= len(answer) <= 50 and not answer.isspace():\n",
    "                            validation_score += 0.2\n",
    "                        \n",
    "                        # Cross-reference check\n",
    "                        if self._cross_validate_answer(answer, extractions, field):\n",
    "                            validation_score += 0.1\n",
    "                        \n",
    "                        # Accept if validation score >= 0.6\n",
    "                        if validation_score >= 0.6 and field:\n",
    "                            validated_data[field] = answer\n",
    "                            file_validation_score += validation_score\n",
    "                            total_validations += 1\n",
    "                    \n",
    "                    # Only include files with sufficient validated extractions\n",
    "                    if len(validated_data) >= 1:  # Lowered threshold\n",
    "                        smart_gt[invoice_file.name] = validated_data\n",
    "                        validation_scores[invoice_file.name] = file_validation_score / total_validations if total_validations > 0 else 0\n",
    "                        print(f\"   âœ… Validated {len(validated_data)} fields (score: {validation_scores[invoice_file.name]:.2f})\")\n",
    "                    else:\n",
    "                        print(f\"   âŒInsufficient validation - skipped\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Error: {e}\")\n",
    "        \n",
    "        # Save smart ground truth\n",
    "        with open(\"smart_auto_ground_truth.json\", \"w\") as f:\n",
    "            json.dump(smart_gt, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n Smart ground truth created for {len(smart_gt)} files\")\n",
    "        return smart_gt\n",
    "    \n",
    "    def _validate_pattern(self, field, value):\n",
    "        \"\"\"Validate using regex patterns with null checking\"\"\"\n",
    "        if not value or field not in self.confidence_patterns:\n",
    "            return True\n",
    "        \n",
    "        patterns = self.confidence_patterns[field]\n",
    "        return any(re.search(pattern, str(value), re.IGNORECASE) for pattern in patterns)\n",
    "    \n",
    "    def _cross_validate_answer(self, answer, all_extractions, field):\n",
    "        \"\"\"Cross-validate with other answers\"\"\"\n",
    "        if not answer:\n",
    "            return False\n",
    "            \n",
    "        similar_count = 0\n",
    "        for q, data in all_extractions.items():\n",
    "            other_answer = data.get('answer', '')\n",
    "            if other_answer and self._similarity_score(answer, other_answer) > 0.7:\n",
    "                similar_count += 1\n",
    "        return similar_count >= 1\n",
    "    \n",
    "    def _similarity_score(self, text1, text2):\n",
    "        \"\"\"Calculate similarity between two text strings\"\"\"\n",
    "        if not text1 or not text2:\n",
    "            return 0\n",
    "        return SequenceMatcher(None, str(text1).lower(), str(text2).lower()).ratio()\n",
    "    \n",
    "    def _map_question_to_field(self, question):\n",
    "        \"\"\"Map questions to standard ground truth fields\"\"\"\n",
    "        if not question:\n",
    "            return None\n",
    "            \n",
    "        q_lower = question.lower()\n",
    "        if any(kw in q_lower for kw in [\"rechnungsnummer\", \"invoice number\", \"nummer\"]):\n",
    "            return \"invoice_number\"\n",
    "        elif any(kw in q_lower for kw in [\"gesamtbetrag\", \"total\", \"amount\", \"endbetrag\"]):\n",
    "            return \"total_amount\"\n",
    "        elif any(kw in q_lower for kw in [\"company\", \"firma\", \"vendor\", \"firmenname\"]):\n",
    "            return \"company_name\"\n",
    "        elif any(kw in q_lower for kw in [\"datum\", \"date\", \"rechnungsdatum\"]):\n",
    "            return \"invoice_date\"\n",
    "        return None\n",
    "    \n",
    "    def run_fully_automated_evaluation(self):\n",
    "        \"\"\"Complete automated pipeline with zero human intervention\"\"\"\n",
    "        \n",
    "        print(\" STARTING FULLY AUTOMATED EVALUATION PIPELINE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Create smart ground truth\n",
    "        print(\"\\n Step 1: Creating smart ground truth...\")\n",
    "        smart_gt = self.create_smart_ground_truth()\n",
    "        \n",
    "        # Step 2: Only proceed if we have some validated data\n",
    "        if not smart_gt:\n",
    "            print(\"âŒ No valid ground truth data created - check your invoice files\")\n",
    "            return {}, {}, []\n",
    "        \n",
    "        # Step 3: Run evaluation with smart ground truth\n",
    "        print(\"\\n Step 2: Running automated evaluation...\")\n",
    "        results = self.evaluator.batch_evaluate_invoices(\n",
    "            \"./large_scale_invoice_dataset\",\n",
    "            ground_truth_file=\"smart_auto_ground_truth.json\"\n",
    "        )\n",
    "        \n",
    "        # Step 4: Generate comprehensive report\n",
    "        automation_report = {\n",
    "            \"pipeline_completed_at\": datetime.now().isoformat(),\n",
    "            \"smart_ground_truth\": {\n",
    "                \"files_validated\": len(smart_gt),\n",
    "                \"validation_method\": \"multi_criteria_smart_validation\"\n",
    "            },\n",
    "            \"evaluation_results\": {\n",
    "                \"total_processed\": len(results),\n",
    "                \"avg_success_rate\": sum(r['success_rate'] for r in results) / len(results) if results else 0,\n",
    "                \"avg_confidence\": sum(r['avg_confidence'] for r in results) / len(results) if results else 0\n",
    "            },\n",
    "            \"automation_level\": \"full_zero_intervention\"\n",
    "        }\n",
    "        \n",
    "        with open(\"automation_report.json\", \"w\") as f:\n",
    "            json.dump(automation_report, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n FULLY AUTOMATED EVALUATION COMPLETE!\")\n",
    "        print(f\"   Ground truth files: {len(smart_gt)}\")\n",
    "        print(f\"   Evaluation results: {len(results)} invoices\")\n",
    "        if results:\n",
    "            print(f\" Average success rate: {automation_report['evaluation_results']['avg_success_rate']:.1%}\")\n",
    "        \n",
    "        return automation_report, smart_gt, results\n",
    "\n",
    "# Initialize and run the smart automated evaluator with error handling\n",
    "if os.path.exists(\"./large_scale_invoice_dataset\"):\n",
    "    pdf_files = list(Path(\"./large_scale_invoice_dataset\").glob(\"*.pdf\"))\n",
    "    image_files = list(Path(\"./large_scale_invoice_dataset\").glob(\"*.png\")) + list(Path(\"./large_scale_invoice_dataset\").glob(\"*.jpg\"))\n",
    "    \n",
    "    total_files = len(pdf_files) + len(image_files)\n",
    "    \n",
    "    if total_files > 0:\n",
    "        print(f\" Running SMART AUTOMATED evaluation on {total_files} files...\")\n",
    "        print(f\"    PDFs: {len(pdf_files)}\")\n",
    "        print(f\"   Images: {len(image_files)}\")\n",
    "        \n",
    "        smart_evaluator = SmartAutomatedEvaluator()\n",
    "        \n",
    "        # ONE-LINE FULLY AUTOMATED EVALUATION\n",
    "        automation_report, ground_truth, evaluation_results = smart_evaluator.run_fully_automated_evaluation()\n",
    "        \n",
    "        if ground_truth:\n",
    "            print(\"ZERO-INTERVENTION EVALUATION COMPLETE!\")\n",
    "        else:\n",
    "            print(\" Evaluation completed but no valid ground truth generated\")\n",
    "    else:\n",
    "        print(\"âŒ No supported files (PDF/PNG/JPG) found in ./large_scale_invoice_dataset\")\n",
    "else:\n",
    "    print(\"âŒ ./large_scale_invoice_dataset folder not found\")\n",
    "    print(\"   Please ensure Step 4A copied the files correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9148c",
   "metadata": {},
   "source": [
    "**IMPORTANT** 100% accuracy ABOVE is artificial because the system created its own \"answer key\" and then compared against it. but the success rates (67-100%) are the real performance indicators showing how many questions the AI could confidently answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10994fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO-SELECT files with lowest performance from recent evaluation\n",
    "def get_low_performance_files(min_files=3, max_files=5):\n",
    "    \"\"\"Automatically select files that need feedback based on performance\"\"\"\n",
    "    \n",
    "    # Check if we have evaluation results\n",
    "    if os.path.exists(\"model_metrics.json\"):\n",
    "        with open(\"model_metrics.json\", 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        # Get recent evaluations and sort by success rate\n",
    "        evaluations = metrics.get(\"evaluations\", [])\n",
    "        if evaluations:\n",
    "            # Sort by success rate lowest first\n",
    "            sorted_evals = sorted(evaluations, key=lambda x: x.get('success_rate', 0))\n",
    "            \n",
    "            # Get lowest performing files\n",
    "            low_perf_files = []\n",
    "            for eval_data in sorted_evals[:max_files]:\n",
    "                filename = eval_data.get('document_name', '')\n",
    "                if filename and os.path.exists(f\"./large_scale_invoice_dataset/{filename}\"):\n",
    "                    low_perf_files.append(filename)\n",
    "            \n",
    "            if len(low_perf_files) >= min_files:\n",
    "                return low_perf_files[:max_files]\n",
    "    \n",
    "    # Fallback: get any available files\n",
    "    available_files = []\n",
    "    if os.path.exists(\"./large_scale_invoice_dataset\"):\n",
    "        for file_ext in [\"*.pdf\", \"*.png\", \"*.jpg\", \"*.jpeg\"]:\n",
    "            available_files.extend([f.name for f in Path(\"./large_scale_invoice_dataset\").glob(file_ext)])\n",
    "    \n",
    "    return available_files[:max_files] if available_files else []\n",
    "\n",
    "# Get files dynamically\n",
    "low_performance_files = get_low_performance_files(min_files=2, max_files=4)\n",
    "\n",
    "if not low_performance_files:\n",
    "    print(\"âŒ No files found for feedback collection!\")\n",
    "    print(\" Make sure you have files in ./large_scale_invoice_dataset/ or run evaluation first\")\n",
    "else:\n",
    "    print(f\" Selected {len(low_performance_files)} files for feedback:\")\n",
    "    for filename in low_performance_files:\n",
    "        print(f\"   â€¢ {filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda7c7b",
   "metadata": {},
   "source": [
    "**5. Scalability and Adaptability**\n",
    "* Ensure the system scales across multiple domains (finance, healthcare, legal, etc.) and data types.\n",
    "* Support plug-and-play modularity to integrate new extraction modules or data sources easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8034dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5A: Domain-Specific Question Templates and Extraction Modules\n",
    "\n",
    "class DomainTemplateManager:\n",
    "    \"\"\"Manages extraction templates for different industries and document types\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.domain_templates = {\n",
    "            \"finance\": {\n",
    "                \"invoice\": [\n",
    "                    \"What is the invoice number?\",\n",
    "                    \"What is the total amount?\",\n",
    "                    \"What is the due date?\",\n",
    "                    \"Who is the vendor?\",\n",
    "                    \"What is the tax amount?\",\n",
    "                    \"What is the net amount?\",\n",
    "                    \"What payment terms are specified?\",\n",
    "                    \"What is the purchase order number?\"\n",
    "                ],\n",
    "                \"financial_statement\": [\n",
    "                    \"What is the total revenue?\",\n",
    "                    \"What is the net income?\",\n",
    "                    \"What is the reporting period?\",\n",
    "                    \"What are the total assets?\",\n",
    "                    \"What are the total liabilities?\",\n",
    "                    \"What is the cash flow from operations?\",\n",
    "                    \"What is the debt-to-equity ratio?\",\n",
    "                    \"What is the earnings per share?\"\n",
    "                ],\n",
    "                \"contract\": [\n",
    "                    \"What is the contract value?\",\n",
    "                    \"What is the contract duration?\",\n",
    "                    \"Who are the contracting parties?\",\n",
    "                    \"What is the effective date?\",\n",
    "                    \"What is the termination date?\",\n",
    "                    \"What are the payment terms?\",\n",
    "                    \"What penalties are specified?\",\n",
    "                    \"What deliverables are mentioned?\"\n",
    "                ]\n",
    "            },\n",
    "            \"healthcare\": {\n",
    "                \"medical_record\": [\n",
    "                    \"What is the patient name?\",\n",
    "                    \"What is the patient ID?\",\n",
    "                    \"What is the diagnosis?\",\n",
    "                    \"What medications are prescribed?\",\n",
    "                    \"What is the treatment plan?\",\n",
    "                    \"What are the vital signs?\",\n",
    "                    \"What allergies are documented?\",\n",
    "                    \"What is the next appointment date?\"\n",
    "                ],\n",
    "                \"lab_report\": [\n",
    "                    \"What tests were performed?\",\n",
    "                    \"What are the test results?\",\n",
    "                    \"What is the reference range?\",\n",
    "                    \"What is the specimen type?\",\n",
    "                    \"When was the sample collected?\",\n",
    "                    \"Who is the ordering physician?\",\n",
    "                    \"Are any results abnormal?\",\n",
    "                    \"What follow-up is recommended?\"\n",
    "                ],\n",
    "                \"prescription\": [\n",
    "                    \"What medication is prescribed?\",\n",
    "                    \"What is the dosage?\",\n",
    "                    \"What is the frequency?\",\n",
    "                    \"How long is the treatment duration?\",\n",
    "                    \"Who is the prescribing doctor?\",\n",
    "                    \"What is the patient name?\",\n",
    "                    \"Are there any warnings?\",\n",
    "                    \"How many refills are allowed?\"\n",
    "                ]\n",
    "            },\n",
    "            \"legal\": {\n",
    "                \"contract\":[\n",
    "                    \"What is the contract value?\",\n",
    "                    \"What is the contract duration?\", \n",
    "                    \"Who are the contracting parties?\",\n",
    "                    \"What is the effective date?\",\n",
    "                    \"What is the termination date?\",\n",
    "                    \"What are the payment terms?\",\n",
    "                    \"What penalties are specified?\",\n",
    "                    \"What deliverables are mentioned?\"\n",
    "                ],\n",
    "                \n",
    "                \"court_document\": [\n",
    "                    \"What is the case number?\",\n",
    "                    \"Who are the plaintiff and defendant?\",\n",
    "                    \"What court is handling the case?\",\n",
    "                    \"What is the filing date?\",\n",
    "                    \"What relief is sought?\",\n",
    "                    \"What are the key facts?\",\n",
    "                    \"What laws are cited?\",\n",
    "                    \"What is the next hearing date?\"\n",
    "                ],\n",
    "                \"legal_notice\": [\n",
    "                    \"Who is the sender?\",\n",
    "                    \"Who is the recipient?\",\n",
    "                    \"What is the subject matter?\",\n",
    "                    \"What action is demanded?\",\n",
    "                    \"What is the deadline for response?\",\n",
    "                    \"What legal basis is cited?\",\n",
    "                    \"What consequences are threatened?\",\n",
    "                    \"Is legal representation mentioned?\"\n",
    "                ]\n",
    "            },\n",
    "            \"hr\": {\n",
    "                \"resume\": [\n",
    "                    \"What is the full name of the candidate?\",\n",
    "                    \"What email address is provided for contact?\",\n",
    "                    \"What phone number is listed?\",\n",
    "                    \"What is the most recent job title?\",\n",
    "                    \"What company does the candidate currently work for?\",\n",
    "                    \"How many years of total experience are mentioned?\",\n",
    "                    \"What degree or education is mentioned?\",\n",
    "                    \"What programming languages are listed?\",\n",
    "                    \"What technical skills are mentioned?\",\n",
    "                    \"What university or school is mentioned?\",\n",
    "                    \"What certifications are listed?\",\n",
    "                    \"What projects are described?\",\n",
    "                    \"What achievements are highlighted?\",\n",
    "                    \"What software tools are mentioned?\",\n",
    "                    \"What languages does the candidate speak?\"\n",
    "                    ],\n",
    "                \"employee_record\": [\n",
    "                    \"What is the employee ID?\",\n",
    "                    \"What is the employee name?\",\n",
    "                    \"What is their department?\",\n",
    "                    \"What is their position?\",\n",
    "                    \"What is their salary?\",\n",
    "                    \"When was their hire date?\",\n",
    "                    \"Who is their manager?\",\n",
    "                    \"What benefits are they enrolled in?\"\n",
    "                ],\n",
    "                \"performance_review\": [\n",
    "                    \"What is the review period?\",\n",
    "                    \"What is the overall rating?\",\n",
    "                    \"What are the key achievements?\",\n",
    "                    \"What areas need improvement?\",\n",
    "                    \"What goals are set for next period?\",\n",
    "                    \"Is a promotion recommended?\",\n",
    "                    \"What training is suggested?\",\n",
    "                    \"What is the salary recommendation?\"\n",
    "                ]\n",
    "            },\n",
    "            \"education\": {\n",
    "                \"transcript\": [\n",
    "                    \"What is the student name?\",\n",
    "                    \"What is the student ID?\",\n",
    "                    \"What degree program?\",\n",
    "                    \"What is the GPA?\",\n",
    "                    \"What courses were completed?\",\n",
    "                    \"What grades were received?\",\n",
    "                    \"What is the graduation date?\",\n",
    "                    \"Are there any honors or distinctions?\"\n",
    "                ],\n",
    "                \"research_paper\": [\n",
    "                    \"What is the title?\",\n",
    "                    \"Who are the authors?\",\n",
    "                    \"What is the abstract?\",\n",
    "                    \"What methodology is used?\",\n",
    "                    \"What are the key findings?\",\n",
    "                    \"What conclusions are drawn?\",\n",
    "                    \"What future work is suggested?\",\n",
    "                    \"What references are cited?\"\n",
    "                ]\n",
    "            },\n",
    "            \"retail\": {\n",
    "                \"receipt\": [\n",
    "                    \"What store issued this receipt?\",\n",
    "                    \"What is the transaction date?\",\n",
    "                    \"What items were purchased?\",\n",
    "                    \"What are the item prices?\",\n",
    "                    \"What is the subtotal?\",\n",
    "                    \"What taxes were applied?\",\n",
    "                    \"What is the total amount?\",\n",
    "                    \"What payment method was used?\"\n",
    "                ],\n",
    "                \"inventory_report\": [\n",
    "                    \"What products are listed?\",\n",
    "                    \"What are the current stock levels?\",\n",
    "                    \"What is the reorder point?\",\n",
    "                    \"What is the unit cost?\",\n",
    "                    \"What is the total inventory value?\",\n",
    "                    \"Which items are low in stock?\",\n",
    "                    \"What is the turnover rate?\",\n",
    "                    \"When was the last inventory count?\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_domain_templates(self, domain: str, document_type: str = None) -> List[str]:\n",
    "        \"\"\"Get extraction templates for specific domain and document type\"\"\"\n",
    "        if domain not in self.domain_templates:\n",
    "            return self.domain_templates.get(\"finance\", {}).get(\"invoice\", [])  # Fallback\n",
    "        \n",
    "        domain_data = self.domain_templates[domain]\n",
    "        \n",
    "        if document_type and document_type in domain_data:\n",
    "            return domain_data[document_type]\n",
    "        \n",
    "        # Return all questions for the domain if no specific document type\n",
    "        all_questions = []\n",
    "        for doc_type, questions in domain_data.items():\n",
    "            all_questions.extend(questions)\n",
    "        \n",
    "        return all_questions\n",
    "    \n",
    "    def add_custom_domain(self, domain_name: str, templates: Dict[str, List[str]]):\n",
    "        \"\"\"Add new domain with custom templates\"\"\"\n",
    "        self.domain_templates[domain_name] = templates\n",
    "        print(f\"âœ… Added custom domain: {domain_name}\")\n",
    "    \n",
    "    def detect_domain_and_type(self, text: str) -> tuple:\n",
    "        \"\"\"Auto-detect domain and document type from text content\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Healthcare keywords\n",
    "        if any(word in text_lower for word in ['patient', 'diagnosis', 'prescription', 'medical', 'doctor', 'hospital']):\n",
    "            if any(word in text_lower for word in ['prescription', 'medication', 'dosage']):\n",
    "                return 'healthcare', 'prescription'\n",
    "            elif any(word in text_lower for word in ['lab', 'test', 'result', 'specimen']):\n",
    "                return 'healthcare', 'lab_report'\n",
    "            else:\n",
    "                return 'healthcare', 'medical_record'\n",
    "        \n",
    "        # Legal keywords\n",
    "        elif any(word in text_lower for word in ['court', 'plaintiff', 'defendant', 'lawsuit', 'legal']):\n",
    "            if any(word in text_lower for word in ['case number', 'filing', 'court']):\n",
    "                return 'legal', 'court_document'\n",
    "            elif any(word in text_lower for word in ['notice', 'demand', 'cease']):\n",
    "                return 'legal', 'legal_notice'\n",
    "            else:\n",
    "                return 'legal', 'contract'\n",
    "        \n",
    "        # HR keywords\n",
    "        elif any(word in text_lower for word in ['employee', 'resume', 'candidate', 'performance']):\n",
    "            if any(word in text_lower for word in ['resume', 'cv', 'experience', 'education']):\n",
    "                return 'hr', 'resume'\n",
    "            elif any(word in text_lower for word in ['performance', 'review', 'rating']):\n",
    "                return 'hr', 'performance_review'\n",
    "            else:\n",
    "                return 'hr', 'employee_record'\n",
    "        \n",
    "        # Education keywords\n",
    "        elif any(word in text_lower for word in ['student', 'grade', 'transcript', 'university', 'research']):\n",
    "            if any(word in text_lower for word in ['transcript', 'gpa', 'courses']):\n",
    "                return 'education', 'transcript'\n",
    "            else:\n",
    "                return 'education', 'research_paper'\n",
    "        \n",
    "        # Retail keywords\n",
    "        elif any(word in text_lower for word in ['receipt', 'purchase', 'inventory', 'store', 'items']):\n",
    "            if any(word in text_lower for word in ['inventory', 'stock', 'reorder']):\n",
    "                return 'retail', 'inventory_report'\n",
    "            else:\n",
    "                return 'retail', 'receipt'\n",
    "        \n",
    "        # Finance keywords (including existing invoice detection)\n",
    "        elif any(word in text_lower for word in ['invoice', 'bill', 'payment', 'financial', 'revenue']):\n",
    "            if any(word in text_lower for word in ['revenue', 'income', 'assets', 'liabilities']):\n",
    "                return 'finance', 'financial_statement'\n",
    "            elif any(word in text_lower for word in ['contract', 'agreement', 'terms']):\n",
    "                return 'finance', 'contract'\n",
    "            else:\n",
    "                return 'finance', 'invoice'\n",
    "        \n",
    "        # Default fallback\n",
    "        return 'finance', 'invoice'\n",
    "\n",
    "# Initialize domain manager\n",
    "domain_manager = DomainTemplateManager()\n",
    "print(\"âœ… Domain Template Manager initialized with 6 domains and 15+ document types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5B: Multi-Domain Extractor\n",
    "\n",
    "class MultiDomainExtractor(QABasedExtractor):\n",
    "    \"\"\"Extended QA extractor with domain-specific capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"deepset/xlm-roberta-large-squad2\", local_dir=\"./fine_tuned_model\"):\n",
    "        super().__init__(model_name, local_dir)\n",
    "        self.domain_manager = DomainTemplateManager()\n",
    "        \n",
    "        # Add domain-specific preprocessing patterns\n",
    "        self.domain_patterns = {\n",
    "            'finance': {\n",
    "                'currency': r'[\\$â‚¬Â£Â¥][\\d,\\.]+',\n",
    "                'dates': r'\\d{1,2}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{2,4}',\n",
    "                'invoice_numbers': r'(?:INV|inv|Invoice|INVOICE)[#\\-\\s]*([A-Z0-9\\-]+)'\n",
    "            },\n",
    "            'healthcare': {\n",
    "                'medications': r'(?:mg|ml|tablets?|capsules?)\\s*\\d+',\n",
    "                'vital_signs': r'(?:BP|Blood Pressure)[:\\s]*\\d+\\/\\d+',\n",
    "                'patient_ids': r'(?:Patient ID|ID)[:\\s]*([A-Z0-9\\-]+)'\n",
    "            },\n",
    "            'legal': {\n",
    "                'case_numbers': r'(?:Case|No\\.)[:\\s]*([A-Z0-9\\-\\/]+)',\n",
    "                'dates': r'\\d{1,2}(?:st|nd|rd|th)?\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}',\n",
    "                'parties': r'(?:Plaintiff|Defendant)[:\\s]*([A-Za-z\\s,\\.]+)'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def preprocess_domain_text(self, text: str, domain: str) -> str:\n",
    "        \"\"\"Enhanced preprocessing based on detected domain\"\"\"\n",
    "        text = super().preprocess_text(text)\n",
    "        \n",
    "        if domain in self.domain_patterns:\n",
    "            patterns = self.domain_patterns[domain]\n",
    "            \n",
    "            # Add domain-specific context markers\n",
    "            for pattern_type, pattern in patterns.items():\n",
    "                matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    if isinstance(match, tuple):\n",
    "                        match = match[0] if match else \"\"\n",
    "                    text = text.replace(str(match), f\"IMPORTANT_{pattern_type.upper()}: {match}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_multi_domain(self, text: str, domain: str = None, document_type: str = None) -> Dict:\n",
    "        \"\"\"Extract information using domain-specific templates\"\"\"\n",
    "        \n",
    "        # Auto-detect domain if not provided\n",
    "        if not domain:\n",
    "            domain, document_type = self.domain_manager.detect_domain_and_type(text)\n",
    "            print(f\" Auto-detected: {domain}/{document_type}\")\n",
    "        \n",
    "        # Get domain-specific questions\n",
    "        questions = self.domain_manager.get_domain_templates(domain, document_type)\n",
    "        \n",
    "        # Use domain-specific preprocessing\n",
    "        processed_text = self.preprocess_domain_text(text, domain)\n",
    "        \n",
    "        # Extract with domain-specific questions\n",
    "        qa_results = self.extract_with_questions(processed_text, questions)\n",
    "        \n",
    "        # Get entities\n",
    "        ner_results = self.extract_entities_with_ner(processed_text)\n",
    "        \n",
    "        successful = sum(1 for r in qa_results.values() if r.get('extracted'))\n",
    "        \n",
    "        return {\n",
    "            'domain': domain,\n",
    "            'document_type': document_type,\n",
    "            'extraction_timestamp': datetime.now().isoformat(),\n",
    "            'model_used': self.model_name,\n",
    "            'total_questions': len(questions),\n",
    "            'successful_extractions': successful,\n",
    "            'success_rate': successful / len(questions) if questions else 0,\n",
    "            'extractions': qa_results,\n",
    "            'entities': ner_results,\n",
    "            'confidence_distribution': self._analyze_confidence_distribution(qa_results)\n",
    "        }\n",
    "    \n",
    "    def _analyze_confidence_distribution(self, qa_results: Dict) -> Dict:\n",
    "        \"\"\"Analyze confidence score distribution for quality assessment\"\"\"\n",
    "        confidences = [r.get('confidence', 0) for r in qa_results.values()]\n",
    "        \n",
    "        if not confidences:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'mean_confidence': sum(confidences) / len(confidences),\n",
    "            'high_confidence_count': sum(1 for c in confidences if c >= 0.7),\n",
    "            'medium_confidence_count': sum(1 for c in confidences if 0.3 <= c < 0.7),\n",
    "            'low_confidence_count': sum(1 for c in confidences if c < 0.3),\n",
    "            'confidence_std': np.std(confidences) if len(confidences) > 1 else 0\n",
    "        }\n",
    "\n",
    "# Create multi-domain extractor instance\n",
    "multi_extractor = MultiDomainExtractor()\n",
    "print(\"âœ… Multi-Domain Extractor ready for 6 industries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df052528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5C: Plug-and-Play Module System\n",
    "\n",
    "class ExtractorModule:\n",
    "    \"\"\"Base class for extraction modules\"\"\"\n",
    "    \n",
    "    def __init__(self, module_name: str, supported_formats: List[str]):\n",
    "        self.module_name = module_name\n",
    "        self.supported_formats = supported_formats\n",
    "        self.is_active = True\n",
    "    \n",
    "    def can_process(self, file_path: str, content: str) -> bool:\n",
    "        \"\"\"Check if this module can process the given file\"\"\"\n",
    "        file_ext = os.path.splitext(file_path)[1].lower()\n",
    "        return file_ext in self.supported_formats\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        \"\"\"Override this method in subclasses\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement extract method\")\n",
    "\n",
    "class TableExtractionModule(ExtractorModule):\n",
    "    \"\"\"Specialized module for table extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"table_extractor\", [\".pdf\", \".xlsx\", \".csv\"])\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        # Simulate table extraction\n",
    "        tables = []\n",
    "        \n",
    "        # Look for table-like patterns\n",
    "        lines = content.split('\\n')\n",
    "        potential_tables = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            # lines with multiple numbers/currencies\n",
    "            if len(re.findall(r'\\d+[,\\.]?\\d*', line)) >= 3:\n",
    "                potential_tables.append({\n",
    "                    'line_number': i + 1,\n",
    "                    'content': line.strip(),\n",
    "                    'confidence': 0.8\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'module': self.module_name,\n",
    "            'tables_found': len(potential_tables),\n",
    "            'tables': potential_tables[:5],  # Limit to first 5\n",
    "            'extraction_type': 'tabular_data'\n",
    "        }\n",
    "\n",
    "class EmailExtractionModule(ExtractorModule):\n",
    "    \"\"\"Specialized module for email extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"email_extractor\", [\".eml\", \".txt\", \".pdf\"])\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        phone_pattern = r'(?:\\+\\d{1,3}[\\s-]?)?\\(?[0-9]{3}\\)?[\\s-]?[0-9]{3}[\\s-]?[0-9]{4}'\n",
    "        \n",
    "        emails = re.findall(email_pattern, content)\n",
    "        phones = re.findall(phone_pattern, content)\n",
    "        \n",
    "        return {\n",
    "            'module': self.module_name,\n",
    "            'emails_found': emails,\n",
    "            'phones_found': phones,\n",
    "            'contact_count': len(emails) + len(phones),\n",
    "            'extraction_type': 'contact_information'\n",
    "        }\n",
    "\n",
    "class DateTimeExtractionModule(ExtractorModule):\n",
    "    \"\"\"Specialized module for date/time extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"datetime_extractor\", [\".pdf\", \".txt\", \".docx\"])\n",
    "    \n",
    "    def extract(self, content: str, **kwargs) -> Dict:\n",
    "        # Various date patterns\n",
    "        date_patterns = [\n",
    "            r'\\d{1,2}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{2,4}',  # MM/DD/YYYY\n",
    "            r'\\d{4}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{1,2}',    # YYYY/MM/DD\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{4}\\b',\n",
    "            r'\\d{1,2}(?:st|nd|rd|th)?\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}'\n",
    "        ]\n",
    "        \n",
    "        found_dates = []\n",
    "        for pattern in date_patterns:\n",
    "            matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "            found_dates.extend(matches)\n",
    "        \n",
    "        return {\n",
    "            'module': self.module_name,\n",
    "            'dates_found': found_dates,\n",
    "            'date_count': len(found_dates),\n",
    "            'extraction_type': 'temporal_information'\n",
    "        }\n",
    "\n",
    "class ModularExtractionSystem:\n",
    "    \"\"\"Plug-and-play system for managing extraction modules\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.modules = {}\n",
    "        self.processing_order = []\n",
    "        \n",
    "        # Register default modules\n",
    "        self._register_default_modules()\n",
    "    \n",
    "    def _register_default_modules(self):\n",
    "        \"\"\"Register built-in extraction modules\"\"\"\n",
    "        default_modules = [\n",
    "            TableExtractionModule(),\n",
    "            EmailExtractionModule(),\n",
    "            DateTimeExtractionModule()\n",
    "        ]\n",
    "        \n",
    "        for module in default_modules:\n",
    "            self.register_module(module)\n",
    "    \n",
    "    def register_module(self, module: ExtractorModule):\n",
    "        \"\"\"Register a new extraction module\"\"\"\n",
    "        self.modules[module.module_name] = module\n",
    "        if module.module_name not in self.processing_order:\n",
    "            self.processing_order.append(module.module_name)\n",
    "        print(f\"âœ… Registered module: {module.module_name}\")\n",
    "    \n",
    "    def unregister_module(self, module_name: str):\n",
    "        \"\"\"Remove an extraction module\"\"\"\n",
    "        if module_name in self.modules:\n",
    "            del self.modules[module_name]\n",
    "            if module_name in self.processing_order:\n",
    "                self.processing_order.remove(module_name)\n",
    "            print(f\"âŒ Unregistered module: {module_name}\")\n",
    "    \n",
    "    def get_compatible_modules(self, file_path: str, content: str) -> List[ExtractorModule]:\n",
    "        \"\"\"Get modules that can process the given file\"\"\"\n",
    "        compatible = []\n",
    "        for module in self.modules.values():\n",
    "            if module.is_active and module.can_process(file_path, content):\n",
    "                compatible.append(module)\n",
    "        return compatible\n",
    "    \n",
    "    def extract_with_modules(self, file_path: str, content: str) -> Dict:\n",
    "        \"\"\"Run all compatible modules on the content\"\"\"\n",
    "        compatible_modules = self.get_compatible_modules(file_path, content)\n",
    "        \n",
    "        results = {\n",
    "            'file_path': file_path,\n",
    "            'modules_used': [m.module_name for m in compatible_modules],\n",
    "            'module_results': {},\n",
    "            'total_modules': len(compatible_modules),\n",
    "            'extraction_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        for module in compatible_modules:\n",
    "            try:\n",
    "                module_result = module.extract(content)\n",
    "                results['module_results'][module.module_name] = module_result\n",
    "                print(f\"âœ… {module.module_name}: {module_result.get('extraction_type', 'extracted')}\")\n",
    "            except Exception as e:\n",
    "                results['module_results'][module.module_name] = {\n",
    "                    'error': str(e),\n",
    "                    'extraction_type': 'failed'\n",
    "                }\n",
    "                print(f\" {module.module_name}: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize modular system\n",
    "modular_system = ModularExtractionSystem()\n",
    "print(\"âœ… Modular Extraction System initialized with 3 default modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13cc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5D: Unified Scalable Pipeline\n",
    "\n",
    "def enhanced_multi_domain_extraction(file_path: str, domain: str = None, \n",
    "                                   document_type: str = None, \n",
    "                                   use_modules: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Ultimate extraction pipeline combining:\n",
    "    - Multi-domain QA extraction\n",
    "    - Plug-and-play modules\n",
    "    - Original normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Load and normalize (existing system)\n",
    "    document_data = load_and_normalize(file_path)\n",
    "    \n",
    "    # Step 2: Combine text for processing\n",
    "    all_text = \"\"\n",
    "    for content_item in document_data['content']:\n",
    "        all_text += content_item['text'] + \" \"\n",
    "    \n",
    "    # Step 3: Multi-domain QA extraction\n",
    "    qa_results = multi_extractor.extract_multi_domain(all_text, domain, document_type)\n",
    "    \n",
    "    # Step 4: Modular extraction (if enabled)\n",
    "    module_results = {}\n",
    "    if use_modules:\n",
    "        module_results = modular_system.extract_with_modules(file_path, all_text)\n",
    "    \n",
    "    # Step 5: Combine all results\n",
    "    enhanced_document = {\n",
    "        **document_data,  # Original normalized data\n",
    "        'multi_domain_extraction': qa_results,\n",
    "        'modular_extraction': module_results,\n",
    "        'scalability_features': {\n",
    "            'domain_detected': qa_results.get('domain', 'unknown'),\n",
    "            'document_type_detected': qa_results.get('document_type', 'unknown'),\n",
    "            'modules_used': module_results.get('modules_used', []),\n",
    "            'total_extraction_methods': 1 + len(module_results.get('modules_used', [])),\n",
    "            'confidence_analysis': qa_results.get('confidence_distribution', {}),\n",
    "            'processing_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return enhanced_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5D: Test scalability with different domains\n",
    "\n",
    "print(\"TESTING MULTI-DOMAIN SCALABILITY\")\n",
    "\n",
    "# Test files for different domains \n",
    "test_scenarios = [\n",
    "    {\n",
    "        'name': 'Internship_Agreement',\n",
    "        'file': r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\interview\\UNPAID_Internship_Agreement_Aslican_Alacal.pdf\",\n",
    "        'expected_domain': 'hr'\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'name': 'CV',\n",
    "        'file': r\"C:\\Users\\aslia\\Downloads\\Aslican_Alacal_CV.pdf\",\n",
    "        'expected_domain': 'hr'\n",
    "    },\n",
    "      {\n",
    "        'name': 'invoice',\n",
    "        'file': r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\\SWME_Rechnung_07122020.pdf\",\n",
    "        'expected_domain': 'finance'\n",
    "    }\n",
    "    # add more test files \n",
    "]\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    if os.path.exists(scenario['file']):\n",
    "        print(f\"\\n Testing: {scenario['name']}\")\n",
    "        \n",
    "        result = enhanced_multi_domain_extraction(scenario['file'])\n",
    "        \n",
    "        scalability = result['scalability_features']\n",
    "        domain_result = result['multi_domain_extraction']\n",
    "        \n",
    "        print(f\"âœ… Domain: {scalability['domain_detected']}\")\n",
    "        print(f\"âœ… Document Type: {scalability['document_type_detected']}\")\n",
    "        print(f\"âœ… Success Rate: {domain_result['success_rate']:.1%}\")\n",
    "        print(f\"âœ… Modules Used: {len(scalability['modules_used'])}\")\n",
    "        print(f\"âœ… Mean Confidence: {scalability['confidence_analysis'].get('mean_confidence', 0):.3f}\")\n",
    "        \n",
    "        # Show top extractions\n",
    "        extractions = domain_result['extractions']\n",
    "        high_conf_count = 0\n",
    "        for question, answer in extractions.items():\n",
    "            if answer.get('confidence', 0) > 0.5 and high_conf_count < 3:\n",
    "                print(f\"   Q: {question[:60]}...\")\n",
    "                print(f\"   A: {answer['answer']} (conf: {answer['confidence']:.3f})\")\n",
    "                high_conf_count += 1\n",
    "    else:\n",
    "        print(f\"âŒ Test file not found: {scenario['file']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52db25",
   "metadata": {},
   "source": [
    "**6. Explainability and Trust**\n",
    "* Integrate explainable AI (XAI) tools (e.g., SHAP, attention visualization) to make extraction results transparent and interpretable for end users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26283b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6A- Explainable AI Dependencies and Setup\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "from typing import Optional, Union, Tuple\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForQuestionAnswering, \n",
    "    pipeline,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DefaultDataCollator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6B - Attention Visualization for Transformer Models\n",
    "\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    \"\"\"Visualize attention patterns in transformer models for explainability\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def get_attention_weights(self, question: str, context: str):\n",
    "        \"\"\"Extract attention weights from the model\"\"\"\n",
    "        try:\n",
    "            # Tokenize inputs\n",
    "            inputs = self.tokenizer(\n",
    "                question, context,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Get model outputs with attention\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, output_attentions=True)\n",
    "            \n",
    "            # Extract attention weights (last layer, first head for simplicity)\n",
    "            attention = outputs.attentions[-1][0, 0].cpu().numpy()\n",
    "            \n",
    "            # Get tokens\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "            \n",
    "            return {\n",
    "                'attention_weights': attention,\n",
    "                'tokens': tokens,\n",
    "                'question_length': len(self.tokenizer.tokenize(question)),\n",
    "                'context_start': len(self.tokenizer.tokenize(question)) + 2  # +2 for special tokens\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Attention extraction failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def visualize_attention_heatmap(self, question: str, context: str, save_path: str = None):\n",
    "        \"\"\"Create attention heatmap visualization\"\"\"\n",
    "        attention_data = self.get_attention_weights(question, context)\n",
    "        \n",
    "        if not attention_data:\n",
    "            return None\n",
    "        \n",
    "        attention = attention_data['attention_weights']\n",
    "        tokens = attention_data['tokens']\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(\n",
    "            attention[:len(tokens), :len(tokens)],\n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cmap='Blues',\n",
    "            cbar=True\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Attention Heatmap\\nQ: {question[:50]}...')\n",
    "        plt.xlabel('Target Tokens')\n",
    "        plt.ylabel('Source Tokens')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\" Attention heatmap saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        return attention_data\n",
    "    \n",
    "    def get_top_attended_tokens(self, question: str, context: str, top_k: int = 5):\n",
    "        \"\"\"Get tokens that received highest attention for answer\"\"\"\n",
    "        attention_data = self.get_attention_weights(question, context)\n",
    "        \n",
    "        if not attention_data:\n",
    "            return []\n",
    "        \n",
    "        attention = attention_data['attention_weights']\n",
    "        tokens = attention_data['tokens']\n",
    "        context_start = attention_data['context_start']\n",
    "        \n",
    "        # Focus on context tokens\n",
    "        context_attention = attention[context_start:, context_start:]\n",
    "        context_tokens = tokens[context_start:]\n",
    "        \n",
    "        # Get average attention received by each context token\n",
    "        avg_attention = context_attention.mean(axis=0)\n",
    "        \n",
    "        # Get top attended tokens\n",
    "        top_indices = avg_attention.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        top_tokens = []\n",
    "        for idx in top_indices:\n",
    "            if idx < len(context_tokens):\n",
    "                top_tokens.append({\n",
    "                    'token': context_tokens[idx],\n",
    "                    'attention_score': avg_attention[idx],\n",
    "                    'position': idx\n",
    "                })\n",
    "        \n",
    "        return top_tokens\n",
    "\n",
    "# Initialize attention visualizer\n",
    "print(\"âœ… Attention Visualizer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b794b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6C - LIME Text Explainer Integration\n",
    "\n",
    "\n",
    "class LIMEExplainer:\n",
    "    \"\"\"LIME-based explainability for QA predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, qa_pipeline):\n",
    "        self.qa_pipeline = qa_pipeline\n",
    "        self.explainer = LimeTextExplainer(class_names=['answer_confidence'])\n",
    "        \n",
    "    def predict_function(self, texts, question):\n",
    "        \"\"\"Prediction function for LIME\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                result = self.qa_pipeline(question=question, context=text)\n",
    "                # Return confidence score as prediction\n",
    "                predictions.append([1 - result['score'], result['score']])\n",
    "            except:\n",
    "                predictions.append([0.5, 0.5])  # Neutral if failed\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def explain_prediction(self, question: str, context: str, num_features: int = 10):\n",
    "        \"\"\"Generate LIME explanation for QA prediction\"\"\"\n",
    "        \n",
    "        # Create prediction function for this specific question\n",
    "        def pred_fn(texts):\n",
    "            return self.predict_function(texts, question)\n",
    "        \n",
    "        try:\n",
    "            # Generate explanation\n",
    "            explanation = self.explainer.explain_instance(\n",
    "                context,\n",
    "                pred_fn,\n",
    "                num_features=num_features,\n",
    "                num_samples=100\n",
    "            )\n",
    "            \n",
    "            # Extract feature importance\n",
    "            feature_importance = explanation.as_list()\n",
    "            \n",
    "            # Get original prediction\n",
    "            original_result = self.qa_pipeline(question=question, context=context)\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'predicted_answer': original_result['answer'],\n",
    "                'confidence': original_result['score'],\n",
    "                'feature_importance': feature_importance,\n",
    "                'explanation_type': 'LIME',\n",
    "                'important_words': [item[0] for item in feature_importance[:5]]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ LIME explanation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def visualize_explanation(self, explanation_data, save_path: str = None):\n",
    "        \"\"\"Visualize LIME explanation\"\"\"\n",
    "        if not explanation_data:\n",
    "            return\n",
    "        \n",
    "        features = explanation_data['feature_importance']\n",
    "        \n",
    "        # Separate positive and negative contributions\n",
    "        words = [item[0] for item in features]\n",
    "        scores = [item[1] for item in features]\n",
    "        \n",
    "        # Create colors based on positive/negative contribution\n",
    "        colors = ['green' if score > 0 else 'red' for score in scores]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.barh(range(len(words)), scores, color=colors, alpha=0.7)\n",
    "        \n",
    "        plt.yticks(range(len(words)), words)\n",
    "        plt.xlabel('Feature Importance (Positive = Supports Answer)')\n",
    "        plt.title(f'LIME Explanation\\nQ: {explanation_data[\"question\"][:50]}...\\nA: {explanation_data[\"predicted_answer\"]} (conf: {explanation_data[\"confidence\"]:.3f})')\n",
    "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, scores):\n",
    "            plt.text(bar.get_width() + (0.01 if score > 0 else -0.01), \n",
    "                    bar.get_y() + bar.get_height()/2,\n",
    "                    f'{score:.3f}', \n",
    "                    ha='left' if score > 0 else 'right', \n",
    "                    va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\" LIME explanation saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "print(\"âœ… LIME Explainer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb90d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6D - Comprehensive Explainability Pipeline\n",
    "\n",
    "class ExplainableQASystem:\n",
    "    \"\"\"Complete explainable QA system combining multiple XAI techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"./fine_tuned_model\"):\n",
    "        # Load model and tokenizer\n",
    "        if os.path.exists(model_path):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer\n",
    "            )\n",
    "            print(f\"âœ… Loaded fine-tuned model from {model_path}\")\n",
    "        else:\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=\"deepset/xlm-roberta-large-squad2\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"deepset/xlm-roberta-large-squad2\")\n",
    "            self.model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/xlm-roberta-large-squad2\")\n",
    "            print(\"âœ… Using base model for explainability\")\n",
    "        \n",
    "        # Initialize explainers\n",
    "        self.attention_viz = AttentionVisualizer(self.model, self.tokenizer)\n",
    "        self.lime_explainer = LIMEExplainer(self.qa_pipeline)\n",
    "        \n",
    "    def explain_prediction(self, question: str, context: str, \n",
    "                         use_attention: bool = True, \n",
    "                         use_lime: bool = True,\n",
    "                         save_visualizations: bool = True) -> Dict:\n",
    "        \"\"\"Generate comprehensive explanation for QA prediction\"\"\"\n",
    "        # TRUNCATE CONTEXT TO PREVENT MEMORY ISSUES\n",
    "        if len(context) > 2000:\n",
    "            context = context[:2000] + \"...\"\n",
    "            print(\" Context truncated for memory optimization\")\n",
    "        \n",
    "        print(f\"Generating explanations for:\")\n",
    "        print(f\"Q: {question[:60]}...\")\n",
    "        print(f\"Context: {context[:100]}...\")\n",
    "        \n",
    "        # Get base prediction\n",
    "        prediction = self.qa_pipeline(question=question, context=context)\n",
    "        \n",
    "        explanations = {\n",
    "            'question': question,\n",
    "            'context': context[:200] + \"...\" if len(context) > 200 else context,\n",
    "            'prediction': {\n",
    "                'answer': prediction['answer'],\n",
    "                'confidence': prediction['score'],\n",
    "                'start_pos': prediction['start'],\n",
    "                'end_pos': prediction['end']\n",
    "            },\n",
    "            'explanations': {}\n",
    "        }\n",
    "        \n",
    "        # Generate attention explanation\n",
    "        if use_attention:\n",
    "            print(\"Generating attention visualization...\")\n",
    "            try:\n",
    "                top_tokens = self.attention_viz.get_top_attended_tokens(question, context)\n",
    "                explanations['explanations']['attention'] = {\n",
    "                    'top_attended_tokens': top_tokens,\n",
    "                    'explanation_type': 'attention_weights'\n",
    "                }\n",
    "                \n",
    "                if save_visualizations:\n",
    "                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                    attention_path = f\"attention_explanation_{timestamp}.png\"\n",
    "                    self.attention_viz.visualize_attention_heatmap(\n",
    "                        question, context, attention_path\n",
    "                    )\n",
    "                    explanations['explanations']['attention']['visualization_path'] = attention_path\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Attention explanation failed: {e}\")\n",
    "        \n",
    "        # Generate LIME explanation\n",
    "        if use_lime:\n",
    "            print(\" Generating LIME explanation...\")\n",
    "            try:\n",
    "                lime_result = self.lime_explainer.explain_prediction(question, context)\n",
    "                if lime_result:\n",
    "                    explanations['explanations']['lime'] = lime_result\n",
    "                    \n",
    "                    if save_visualizations:\n",
    "                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                        lime_path = f\"lime_explanation_{timestamp}.png\"\n",
    "                        self.lime_explainer.visualize_explanation(lime_result, lime_path)\n",
    "                        explanations['explanations']['lime']['visualization_path'] = lime_path\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"âŒLIME explanation failed: {e}\")\n",
    "        \n",
    "        # Generate confidence interpretation\n",
    "        explanations['trust_indicators'] = self._generate_trust_indicators(prediction, explanations)\n",
    "        \n",
    "        return explanations\n",
    "    \n",
    "    def _generate_trust_indicators(self, prediction: Dict, explanations: Dict) -> Dict:\n",
    "        \"\"\"Generate trust and reliability indicators\"\"\"\n",
    "        confidence = prediction['score']\n",
    "        \n",
    "        # Confidence level categorization\n",
    "        if confidence >= 0.8:\n",
    "            confidence_level = \"High\"\n",
    "            trust_message = \"High confidence - answer is very reliable\"\n",
    "        elif confidence >= 0.5:\n",
    "            confidence_level = \"Medium\"\n",
    "            trust_message = \"Medium confidence - answer is moderately reliable\"\n",
    "        elif confidence >= 0.3:\n",
    "            confidence_level = \"Low\"\n",
    "            trust_message = \"Low confidence - answer may be uncertain\"\n",
    "        else:\n",
    "            confidence_level = \"Very Low\"\n",
    "            trust_message = \"Very low confidence - answer is unreliable\"\n",
    "        \n",
    "        # Check explanation consistency\n",
    "        explanation_consistency = \"Unknown\"\n",
    "        if 'attention' in explanations.get('explanations', {}):\n",
    "            top_tokens = explanations['explanations']['attention'].get('top_attended_tokens', [])\n",
    "            if top_tokens and len(top_tokens) > 0:\n",
    "                # Check if answer appears in top attended tokens\n",
    "                answer_tokens = prediction['answer'].lower().split()\n",
    "                attended_tokens = [t['token'].lower().replace('##', '') for t in top_tokens]\n",
    "                \n",
    "                overlap = any(token in ' '.join(attended_tokens) for token in answer_tokens)\n",
    "                explanation_consistency = \"High\" if overlap else \"Low\"\n",
    "        \n",
    "        return {\n",
    "            'confidence_score': confidence,\n",
    "            'confidence_level': confidence_level,\n",
    "            'trust_message': trust_message,\n",
    "            'explanation_consistency': explanation_consistency,\n",
    "            'reliability_score': min(confidence * 1.2, 1.0) if explanation_consistency == \"High\" else confidence * 0.8\n",
    "        }\n",
    "    \n",
    "    def batch_explain_extractions(self, extractions_dict: Dict, max_explanations: int = 3) -> Dict:\n",
    "        \"\"\"Generate explanations for multiple extractions\"\"\"\n",
    "        print(f\"Generating explanations for top {max_explanations} extractions...\")\n",
    "        \n",
    "        explained_extractions = {}\n",
    "        count = 0\n",
    "        \n",
    "        for question, result in extractions_dict.items():\n",
    "            if count >= max_explanations:\n",
    "                break\n",
    "                \n",
    "            if result.get('answer') and result.get('confidence', 0) > 0.3:\n",
    "                print(f\"\\n Explaining: {question[:50]}...\")\n",
    "                \n",
    "                # Reconstruct context \n",
    "                context = result.get('context', 'Context not available')\n",
    "                \n",
    "                explanation = self.explain_prediction(\n",
    "                    question=question,\n",
    "                    context=context,\n",
    "                    use_attention=True,\n",
    "                    use_lime=True,\n",
    "                    save_visualizations=True\n",
    "                )\n",
    "                \n",
    "                explained_extractions[question] = explanation\n",
    "                count += 1\n",
    "        \n",
    "        return explained_extractions\n",
    "\n",
    "# Initialize explainable QA system\n",
    "explainable_qa = ExplainableQASystem()\n",
    "print(\"âœ… Explainable QA System ready!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Phase 6E: Integration with Multi-Domain System\n",
    "\n",
    "def enhanced_multi_domain_extraction_with_explanations(file_path: str, \n",
    "                                                      domain: str = None, \n",
    "                                                      document_type: str = None,\n",
    "                                                      explain_top_results: bool = True,\n",
    "                                                      max_explanations: int = 2) -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced extraction with built-in explainability\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1: Run your existing multi-domain extraction\n",
    "    result = enhanced_multi_domain_extraction(file_path, domain, document_type)\n",
    "    \n",
    "    # 2: Add explainability for high-confidence results\n",
    "    if explain_top_results:\n",
    "        print(f\"\\n Adding explainability to extraction results...\")\n",
    "        \n",
    "        # Get text content for explanations\n",
    "        all_text = \"\"\n",
    "        for content_item in result['content']:\n",
    "            all_text += content_item['text'] + \" \"\n",
    "        \n",
    "        # Get high-confidence extractions\n",
    "        extractions = result['multi_domain_extraction']['extractions']\n",
    "        high_conf_extractions = {}\n",
    "        \n",
    "        count = 0\n",
    "        for question, answer_data in extractions.items():\n",
    "            if (answer_data.get('confidence', 0) > 0.4 and \n",
    "                answer_data.get('answer') and \n",
    "                count < max_explanations):\n",
    "                \n",
    "                high_conf_extractions[question] = {\n",
    "                    **answer_data,\n",
    "                    'context': all_text  # Add context for explanation\n",
    "                }\n",
    "                count += 1\n",
    "        \n",
    "        # Generate explanations\n",
    "        if high_conf_extractions:\n",
    "            explanations = {}\n",
    "            for question, data in high_conf_extractions.items():\n",
    "                print(f\" Explaining: {question[:40]}...\")\n",
    "                \n",
    "                explanation = explainable_qa.explain_prediction(\n",
    "                    question=question,\n",
    "                    context=data['context'],\n",
    "                    use_attention=True,\n",
    "                    use_lime=True,\n",
    "                    save_visualizations=True\n",
    "                )\n",
    "                \n",
    "                explanations[question] = explanation\n",
    "            \n",
    "            # Add explanations to result\n",
    "            result['explainability'] = {\n",
    "                'explained_extractions': explanations,\n",
    "                'explanation_count': len(explanations),\n",
    "                'explainability_methods': ['attention_visualization', 'lime_text_explanation'],\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            }\n",
    "        else:\n",
    "            result['explainability'] = {\n",
    "                'message': 'No high-confidence extractions found for explanation',\n",
    "                'min_confidence_threshold': 0.4\n",
    "            }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… Explainable Multi-Domain Extraction ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a27ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6F: Test Explainable System\n",
    "\n",
    "def get_available_test_file():\n",
    "    \"\"\"Get any available test file for explainability\"\"\"\n",
    "    \n",
    "    test_files = []\n",
    "    \n",
    "    # Check large dataset folder\n",
    "    if os.path.exists(\"./large_scale_invoice_dataset\"):\n",
    "        dataset_files = list(Path(\"./large_scale_invoice_dataset\").glob(\"*.pdf\"))\n",
    "        test_files.extend([str(f) for f in dataset_files[:2]])\n",
    "    \n",
    "    # Return first available file\n",
    "    for file_path in test_files:\n",
    "        if os.path.exists(file_path):\n",
    "            return file_path\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def test_explainable_system():\n",
    "    \"\"\"Test explainability with fallback options\"\"\"\n",
    "    \n",
    "    # Get best available test file\n",
    "    test_file = get_available_test_file()\n",
    "    \n",
    "    if not test_file:\n",
    "        print(\"âŒ No test files available, check your dataset paths\")\n",
    "        return\n",
    "    \n",
    "    print(f\" Testing explainability with: {os.path.basename(test_file)}\")\n",
    "    \n",
    "    try:\n",
    "        # Run with error handling\n",
    "        explainable_result = ./large_scale_invoice_dataset(\n",
    "            test_file,\n",
    "            explain_top_results=True,\n",
    "            max_explanations=1  # Reduced to prevent timeout\n",
    "        )\n",
    "        \n",
    "        # Display results with error checking\n",
    "        print(f\"\\nâœ… EXPLAINABLE EXTRACTION COMPLETE!\")\n",
    "        \n",
    "        scalability = explainable_result.get('scalability_features', {})\n",
    "        multi_domain = explainable_result.get('multi_domain_extraction', {})\n",
    "        \n",
    "        print(f\"Domain: {scalability.get('domain_detected', 'unknown')}\")\n",
    "        print(f\"Success Rate: {multi_domain.get('success_rate', 0):.1%}\")\n",
    "        \n",
    "        # Check if explanations were generated\n",
    "        explainability = explainable_result.get('explainability', {})\n",
    "        if 'explained_extractions' in explainability:\n",
    "            print(f\" Explanations Generated: {explainability.get('explanation_count', 0)}\")\n",
    "            print(\" EXPLAINABLE AI INTEGRATION SUCCESSFUL!\")\n",
    "        else:\n",
    "            print(f\"âŒ Explanation message: {explainability.get('message', 'No explanations')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒExplainability test failed: {e}\")\n",
    "        print(\" This might be due to model/memory limitations\")\n",
    "\n",
    "# Run the robust test\n",
    "test_explainable_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a84d37",
   "metadata": {},
   "source": [
    "**7.\tSecurity and Offline Operability**\n",
    "* Ensure the system runs in secure environments without requiring external cloud APIs or persistent internet access.\n",
    "* Design for deployment in edge devices or private infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae458979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 7A: Security Framework and Encryption\n",
    "\n",
    "!pip install keyring\n",
    "import keyring\n",
    "import hashlib\n",
    "import secrets\n",
    "import base64\n",
    "from cryptography.fernet import Fernet\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "\n",
    "class SecureDataHandler:\n",
    "    \"\"\"Handles encryption/decryption of sensitive data and model files\"\"\"\n",
    "    \n",
    "    def __init__(self, password: str = None):\n",
    "        self.password = password or self._get_secure_password()\n",
    "        self.key = self._derive_key(self.password)\n",
    "        self.cipher = Fernet(self.key)\n",
    "        \n",
    "    def _get_secure_password(self):\n",
    "        \"\"\"Get password securely (in production, use proper key management)\"\"\"\n",
    "        try:\n",
    "            # Try to get from system keyring first\n",
    "            password = keyring.get_password(\"qa_extractor\", \"main_key\")\n",
    "            if not password:\n",
    "                password = getpass.getpass(\"Enter encryption password for secure storage: \")\n",
    "                keyring.set_password(\"qa_extractor\", \"main_key\", password)\n",
    "            return password\n",
    "        except:\n",
    "            # Fallback to environment or prompt\n",
    "            return os.environ.get(\"QA_EXTRACTOR_KEY\", \"secure_default_key_2024\")\n",
    "    \n",
    "    def _derive_key(self, password: str):\n",
    "        \"\"\"Derive encryption key from password\"\"\"\n",
    "        password_bytes = password.encode()\n",
    "        salt = b\"qa_extractor_salt_2024\"  # In production, use random salt per user\n",
    "        kdf = PBKDF2HMAC(\n",
    "            algorithm=hashes.SHA256(),\n",
    "            length=32,\n",
    "            salt=salt,\n",
    "            iterations=100000,\n",
    "        )\n",
    "        key = base64.urlsafe_b64encode(kdf.derive(password_bytes))\n",
    "        return key\n",
    "    \n",
    "    def encrypt_file(self, file_path: str, output_path: str = None):\n",
    "        \"\"\"Encrypt a file and save to output path\"\"\"\n",
    "        if not output_path:\n",
    "            output_path = file_path + \".encrypted\"\n",
    "            \n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        encrypted_data = self.cipher.encrypt(data)\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(encrypted_data)\n",
    "        \n",
    "        print(f\"Encrypted: {file_path} -> {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def decrypt_file(self, encrypted_path: str, output_path: str = None):\n",
    "        \"\"\"Decrypt a file and save to output path\"\"\"\n",
    "        if not output_path:\n",
    "            output_path = encrypted_path.replace(\".encrypted\", \"\")\n",
    "            \n",
    "        with open(encrypted_path, 'rb') as f:\n",
    "            encrypted_data = f.read()\n",
    "        \n",
    "        decrypted_data = self.cipher.decrypt(encrypted_data)\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(decrypted_data)\n",
    "        \n",
    "        print(f\"Decrypted: {encrypted_path} -> {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def encrypt_text(self, text: str) -> str:\n",
    "        \"\"\"Encrypt text data\"\"\"\n",
    "        return self.cipher.encrypt(text.encode()).decode()\n",
    "    \n",
    "    def decrypt_text(self, encrypted_text: str) -> str:\n",
    "        \"\"\"Decrypt text data\"\"\"\n",
    "        return self.cipher.decrypt(encrypted_text.encode()).decode()\n",
    "\n",
    "print(\"âœ… Secure Data Handler ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 7B: Offline Model Manager\n",
    "\n",
    "class OfflineModelManager:\n",
    "    \"\"\"Manages model storage, loading, and security for offline operation\"\"\"\n",
    "    \n",
    "    def __init__(self, secure_storage_dir=\"./secure_models\"):\n",
    "        self.storage_dir = Path(secure_storage_dir)\n",
    "        self.storage_dir.mkdir(exist_ok=True)\n",
    "        self.security_handler = SecureDataHandler()\n",
    "        self.model_registry = self._load_model_registry()\n",
    "        \n",
    "    def _load_model_registry(self):\n",
    "        \"\"\"Load encrypted model registry\"\"\"\n",
    "        registry_path = self.storage_dir / \"model_registry.encrypted\"\n",
    "        if registry_path.exists():\n",
    "            try:\n",
    "                decrypted_data = self.security_handler.decrypt_text(\n",
    "                    registry_path.read_text()\n",
    "                )\n",
    "                return json.loads(decrypted_data)\n",
    "            except:\n",
    "                print(\" Could not decrypt model registry, creating new one\")\n",
    "        \n",
    "        return {\"models\": {}, \"active_model\": None, \"created_at\": datetime.now().isoformat()}\n",
    "    \n",
    "    def _save_model_registry(self):\n",
    "        \"\"\"Save encrypted model registry\"\"\"\n",
    "        registry_path = self.storage_dir / \"model_registry.encrypted\"\n",
    "        encrypted_data = self.security_handler.encrypt_text(\n",
    "            json.dumps(self.model_registry, indent=2)\n",
    "        )\n",
    "        registry_path.write_text(encrypted_data)\n",
    "    \n",
    "    def store_model_securely(self, model_path: str, model_name: str, description: str = \"\"):\n",
    "        \"\"\"Store model files with encryption\"\"\"\n",
    "        model_dir = self.storage_dir / model_name\n",
    "        model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Encrypt and store all model files\n",
    "        secured_files = {}\n",
    "        \n",
    "        for file_path in Path(model_path).glob(\"*\"):\n",
    "            if file_path.is_file():\n",
    "                encrypted_path = model_dir / f\"{file_path.name}.encrypted\"\n",
    "                self.security_handler.encrypt_file(str(file_path), str(encrypted_path))\n",
    "                secured_files[file_path.name] = str(encrypted_path)\n",
    "        \n",
    "        # Update registry\n",
    "        self.model_registry[\"models\"][model_name] = {\n",
    "            \"description\": description,\n",
    "            \"stored_at\": datetime.now().isoformat(),\n",
    "            \"files\": secured_files,\n",
    "            \"original_path\": str(model_path),\n",
    "            \"secure_path\": str(model_dir)\n",
    "        }\n",
    "        \n",
    "        self._save_model_registry()\n",
    "        print(f\" Model '{model_name}' stored securely with {len(secured_files)} files\")\n",
    "        return model_dir\n",
    "    \n",
    "    def load_model_securely(self, model_name: str, temp_dir: str = None):\n",
    "        \"\"\"Decrypt and load model for use\"\"\"\n",
    "        if model_name not in self.model_registry[\"models\"]:\n",
    "            raise ValueError(f\"Model '{model_name}' not found in secure storage\")\n",
    "        \n",
    "        model_info = self.model_registry[\"models\"][model_name]\n",
    "        \n",
    "        # Create temporary directory for decrypted files\n",
    "        if not temp_dir:\n",
    "            temp_dir = tempfile.mkdtemp(prefix=\"qa_model_\")\n",
    "        else:\n",
    "            Path(temp_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "        # Decrypt all model files to temp directory\n",
    "        for original_name, encrypted_path in model_info[\"files\"].items():\n",
    "            output_path = Path(temp_dir) / original_name\n",
    "            self.security_handler.decrypt_file(encrypted_path, str(output_path))\n",
    "        \n",
    "        print(f\" Model '{model_name}' loaded to temporary directory: {temp_dir}\")\n",
    "        return temp_dir\n",
    "    \n",
    "    def list_secure_models(self):\n",
    "        \"\"\"List all securely stored models\"\"\"\n",
    "        print(\"\\n SECURE MODEL REGISTRY:\")\n",
    "        for name, info in self.model_registry[\"models\"].items():\n",
    "            print(f\"  â€¢ {name}: {info['description']}\")\n",
    "            print(f\"    Stored: {info['stored_at']}\")\n",
    "            print(f\"    Files: {len(info['files'])}\")\n",
    "        \n",
    "        if self.model_registry[\"active_model\"]:\n",
    "            print(f\"\\n Active Model: {self.model_registry['active_model']}\")\n",
    "    \n",
    "    def cleanup_temp_files(self, temp_dir: str):\n",
    "        \"\"\"Securely delete temporary decrypted files\"\"\"\n",
    "        if Path(temp_dir).exists():\n",
    "            shutil.rmtree(temp_dir)\n",
    "            print(f\" Cleaned up temporary files: {temp_dir}\")\n",
    "\n",
    "print(\"âœ… Offline Model Manager ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 7C: Air-Gapped Environment Support\n",
    "\n",
    "class AirGappedExtractor(QABasedExtractor):\n",
    "    \"\"\"QA Extractor designed for completely offline/air-gapped environments\"\"\"\n",
    "    \n",
    "    def __init__(self, secure_model_name: str = None):\n",
    "        self.model_manager = OfflineModelManager()\n",
    "        self.temp_model_dir = None\n",
    "        self.model_name = secure_model_name or \"production_model\"\n",
    "        \n",
    "        # Load model from secure storage\n",
    "        self._load_secure_model()\n",
    "        \n",
    "    def _load_secure_model(self):\n",
    "        \"\"\"Load model from encrypted storage\"\"\"\n",
    "        try:\n",
    "            # Try to load from secure storage first\n",
    "            self.temp_model_dir = self.model_manager.load_model_securely(self.model_name)\n",
    "            \n",
    "            # Initialize QA pipeline with decrypted model\n",
    "            self.qa_pipeline = pipeline(\n",
    "                \"question-answering\",\n",
    "                model=self.temp_model_dir,\n",
    "                tokenizer=self.temp_model_dir,\n",
    "                device=-1  # Force CPU for better compatibility\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… Loaded secure model: {self.model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" âŒ Could not load secure model, using base model: {e}\")\n",
    "            # Fallback to base model\n",
    "            super().__init__(model_name=\"deepset/xlm-roberta-large-squad2\")\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up temporary files when object is destroyed\"\"\"\n",
    "        if self.temp_model_dir:\n",
    "            self.model_manager.cleanup_temp_files(self.temp_model_dir)\n",
    "    \n",
    "    def extract_with_security_audit(self, text: str, questions: List[str]) -> Dict:\n",
    "        \"\"\"Extract with full security auditing\"\"\"\n",
    "        # Process document completely offline\n",
    "        # Log every action for security audit\n",
    "        # Add digital fingerprint to results\n",
    "        \n",
    "        \n",
    "        # Generate audit ID\n",
    "        audit_id = hashlib.sha256(\n",
    "            (text[:100] + str(datetime.now())).encode()\n",
    "        ).hexdigest()[:12]\n",
    "        \n",
    "        # Regular extraction\n",
    "        results = self.extract_with_questions(text, questions)\n",
    "        \n",
    "        # Add security metadata\n",
    "        security_info = {\n",
    "            'audit_id': audit_id,\n",
    "            'processing_mode': 'air_gapped',\n",
    "            'model_source': 'secure_storage' if self.temp_model_dir else 'fallback',\n",
    "            'data_hash': hashlib.sha256(text.encode()).hexdigest(),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'no_external_calls': True,\n",
    "            'encryption_used': True\n",
    "        }\n",
    "        \n",
    "        results['security_audit'] = security_info\n",
    "        return results\n",
    "\n",
    "print(\"âœ… Air-Gapped Extractor ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 7D: Edge Device Optimization\n",
    "\n",
    "class EdgeOptimizedExtractor:\n",
    "    \"\"\"Lightweight extractor optimized for edge devices and limited resources\"\"\"\n",
    "    \n",
    "    def __init__(self, model_size=\"small\", max_memory_mb=512):\n",
    "        self.max_memory_mb = max_memory_mb\n",
    "        self.model_size = model_size\n",
    "        \n",
    "        # Choose model based on resource constraints\n",
    "        if model_size == \"tiny\":\n",
    "            model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "        elif model_size == \"small\":\n",
    "            model_name = \"deepset/minilm-uncased-squad2\"\n",
    "        else:\n",
    "            model_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "        \n",
    "        # Initialize with memory optimization\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=model_name,\n",
    "            device=-1,  # CPU only for edge devices\n",
    "            batch_size=1,  # Process one at a time\n",
    "            max_length=256  # Reduced context length\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Edge-optimized extractor ready (model: {model_size})\")\n",
    "    \n",
    "    def extract_lightweight(self, text: str, max_questions: int = 5) -> Dict:\n",
    "        \"\"\"Memory-efficient extraction with limited questions\"\"\"\n",
    "        \n",
    "        # Truncate text if too long\n",
    "        if len(text) > 1000:\n",
    "            text = text[:1000] + \"...\"\n",
    "        \n",
    "        # Use only most important questions\n",
    "        important_questions = [\n",
    "            \"What is the main amount or total?\",\n",
    "            \"What is the document number?\",\n",
    "            \"What company or organization is mentioned?\",\n",
    "            \"What date is mentioned?\",\n",
    "            \"Who is the person or contact mentioned?\"\n",
    "        ][:max_questions]\n",
    "        \n",
    "        results = {}\n",
    "        for question in important_questions:\n",
    "            try:\n",
    "                result = self.qa_pipeline(\n",
    "                    question=question,\n",
    "                    context=text,\n",
    "                    max_answer_len=50  # Short answers only\n",
    "                )\n",
    "                \n",
    "                results[question] = {\n",
    "                    'answer': result['answer'],\n",
    "                    'confidence': result['score'],\n",
    "                    'extracted': result['score'] > 0.1  # Lower threshold for edge\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[question] = {'error': str(e), 'extracted': False}\n",
    "        \n",
    "        return {\n",
    "            'extractions': results,\n",
    "            'model_size': self.model_size,\n",
    "            'memory_optimized': True,\n",
    "            'edge_compatible': True\n",
    "        }\n",
    "\n",
    "print(\"âœ… Edge-Optimized Extractor ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38cb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 7E: Secure Pipeline Integration\n",
    "\n",
    "class SecureOfflineSystem:\n",
    "    \"\"\"Combines everything into one complet secure, offline-capable extraction system\"\"\"\n",
    "    \n",
    "    def __init__(self, security_level=\"high\"):\n",
    "        self.security_level = security_level\n",
    "        self.security_handler = SecureDataHandler()\n",
    "        self.model_manager = OfflineModelManager()\n",
    "        \n",
    "        # Initialize appropriate extractor based on security level\n",
    "        if security_level == \"high\":\n",
    "            self.extractor = AirGappedExtractor()\n",
    "        elif security_level == \"edge\":\n",
    "            self.extractor = EdgeOptimizedExtractor()\n",
    "        else:\n",
    "            self.extractor = QABasedExtractor()\n",
    "        \n",
    "        # Security audit log\n",
    "        self.audit_log = []\n",
    "        \n",
    "    def process_document_securely(self, file_path: str, delete_after_processing: bool = True):\n",
    "        \"\"\"Process document with full security measures\"\"\"\n",
    "        # Generate unique ID for this processing\n",
    "        # Extract data with chosen security level\n",
    "        # Log all security events\n",
    "        # Optionally delete original file\n",
    "        # Encrypt results before saving\n",
    "        \n",
    "        \n",
    "        # Generate processing ID\n",
    "        process_id = secrets.token_hex(8)\n",
    "        \n",
    "        try:\n",
    "            # Log start of processing\n",
    "            self._log_security_event(\"PROCESSING_START\", {\n",
    "                'process_id': process_id,\n",
    "                'file_name': Path(file_path).name,\n",
    "                'file_size': Path(file_path).stat().st_size,\n",
    "                'security_level': self.security_level\n",
    "            })\n",
    "            \n",
    "            # Load and normalize document\n",
    "            document_data = load_and_normalize(file_path)\n",
    "            \n",
    "            # Extract text content\n",
    "            all_text = \"\"\n",
    "            for content_item in document_data['content']:\n",
    "                all_text += content_item['text'] + \" \"\n",
    "            \n",
    "            # Secure extraction\n",
    "            if hasattr(self.extractor, 'extract_with_security_audit'):\n",
    "                extraction_results = self.extractor.extract_with_security_audit(\n",
    "                    all_text, \n",
    "                    self.extractor.extraction_templates['german_invoice']\n",
    "                )\n",
    "            else:\n",
    "                extraction_results = self.extractor.extract_information(all_text)\n",
    "            \n",
    "            # Add security metadata\n",
    "            extraction_results['security_info'] = {\n",
    "                'process_id': process_id,\n",
    "                'security_level': self.security_level,\n",
    "                'offline_mode': True,\n",
    "                'encryption_available': True,\n",
    "                'audit_trail': len(self.audit_log)\n",
    "            }\n",
    "            \n",
    "            # Log successful processing\n",
    "            self._log_security_event(\"PROCESSING_SUCCESS\", {\n",
    "                'process_id': process_id,\n",
    "                'extractions_found': extraction_results.get('successful_extractions', 0)\n",
    "            })\n",
    "            \n",
    "            # Optionally delete source file for security\n",
    "            if delete_after_processing:\n",
    "                os.remove(file_path)\n",
    "                self._log_security_event(\"FILE_DELETED\", {'process_id': process_id})\n",
    "            \n",
    "            return extraction_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._log_security_event(\"PROCESSING_ERROR\", {\n",
    "                'process_id': process_id,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            raise\n",
    "    \n",
    "    def _log_security_event(self, event_type: str, details: dict):\n",
    "        \"\"\"Log security events for audit trail\"\"\"\n",
    "        event = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'event_type': event_type,\n",
    "            'details': details,\n",
    "            'system_user': os.getenv('USERNAME', 'unknown')\n",
    "        }\n",
    "        \n",
    "        self.audit_log.append(event)\n",
    "        \n",
    "        # Also log to secure file\n",
    "        log_file = Path(\"security_audit.log\")\n",
    "        with open(log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(event) + \"\\n\")\n",
    "    \n",
    "    def export_secure_results(self, results: dict, output_path: str):\n",
    "        \"\"\"Export results with encryption\"\"\"\n",
    "        \n",
    "        # Convert results to JSON\n",
    "        results_json = json.dumps(results, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Encrypt the results\n",
    "        encrypted_path = output_path + \".encrypted\"\n",
    "        encrypted_data = self.security_handler.encrypt_text(results_json)\n",
    "        \n",
    "        with open(encrypted_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(encrypted_data)\n",
    "        \n",
    "        print(f\" âœ… Results exported securely to: {encrypted_path}\")\n",
    "        return encrypted_path\n",
    "    \n",
    "    def get_security_audit_report(self):\n",
    "        \"\"\"Generate security audit report\"\"\"\n",
    "        report = {\n",
    "            'total_events': len(self.audit_log),\n",
    "            'security_level': self.security_level,\n",
    "            'recent_events': self.audit_log[-10:],  # Last 10 events\n",
    "            'event_types': {},\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Count event types\n",
    "        for event in self.audit_log:\n",
    "            event_type = event['event_type']\n",
    "            report['event_types'][event_type] = report['event_types'].get(event_type, 0) + 1\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize secure system\n",
    "secure_system = SecureOfflineSystem(security_level=\"high\")\n",
    "print(\"âœ… Secure Offline System ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b730d",
   "metadata": {},
   "source": [
    "âŒ Could not load secure model, using base model... = It's refusing to load a model that doesn't exist in secure storage and falling back safely to a known model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7F: Test Complete Secure System\n",
    "\n",
    "def test_secure_offline_system():\n",
    "    \"\"\"Test the complete secure, offline system\"\"\"\n",
    "    \n",
    "    print(\" TESTING SECURE OFFLINE SYSTEM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test file\n",
    "    test_file = r\"C:\\Users\\aslia\\OneDrive\\Desktop\\HOPn\\Self-Learning Data Extraction and Auto Fine-Tuning System\\invoice_dataset\\SWME_Rechnung_07122020.pdf\"\n",
    "    \n",
    "    if os.path.exists(test_file):\n",
    "        # Make a copy for testing (don't delete original)\n",
    "        test_copy = \"test_secure_processing.pdf\"\n",
    "        shutil.copy2(test_file, test_copy)\n",
    "        \n",
    "        try:\n",
    "            # Process with secure system\n",
    "            results = secure_system.process_document_securely(\n",
    "                test_copy, \n",
    "                delete_after_processing=True  # Will delete the copy\n",
    "            )\n",
    "            \n",
    "            print(\"\\nâœ… SECURE PROCESSING RESULTS:\")\n",
    "            print(f\"   Security Level: {results['security_info']['security_level']}\")\n",
    "            print(f\"   Process ID: {results['security_info']['process_id']}\")\n",
    "            print(f\"   Offline Mode: {results['security_info']['offline_mode']}\")\n",
    "            print(f\"   Extractions Found: {results.get('successful_extractions', 0)}\")\n",
    "            \n",
    "            # Show some extractions\n",
    "            if 'extractions' in results:\n",
    "                print(\"\\n SAMPLE EXTRACTIONS:\")\n",
    "                count = 0\n",
    "                for question, result in results['extractions'].items():\n",
    "                    if result.get('answer') and count < 3:\n",
    "                        print(f\"   Q: {question[:50]}...\")\n",
    "                        print(f\"   A: {result['answer']} (conf: {result['confidence']:.3f})\")\n",
    "                        count += 1\n",
    "            \n",
    "            # Export results securely\n",
    "            encrypted_output = secure_system.export_secure_results(\n",
    "                results, \n",
    "                \"secure_extraction_results.json\"\n",
    "            )\n",
    "            \n",
    "            # Show security audit\n",
    "            audit_report = secure_system.get_security_audit_report()\n",
    "            print(f\"\\n SECURITY AUDIT:\")\n",
    "            print(f\"   Total Events: {audit_report['total_events']}\")\n",
    "            print(f\"   Event Types: {audit_report['event_types']}\")\n",
    "            \n",
    "            print(\"\\n âœ… SECURE OFFLINE SYSTEM TEST COMPLETE!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in secure processing: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ Test file not found - update path for testing\")\n",
    "\n",
    "# Run the test\n",
    "test_secure_offline_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selflearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
